/**
 * ProcessCoordinatorV2 - Optimized Pipeline Service with Enhanced Clean Metrics
 * 
 * ENHANCED WITH V2 CLEAN METRICS SYSTEM:
 * - Captures comprehensive pipeline analytics using semantic database tables
 * - Records opportunity processing paths (NEW/UPDATE/SKIP flow analytics)
 * - Tracks optimization impact and token savings in real-time
 * - Provides detailed duplicate detection effectiveness metrics
 * - Generates dashboard-ready performance insights
 * 
 * NEW OPTIMIZED PIPELINE FLOW:
 * 1. SourceOrchestrator - Source analysis and configuration
 * 2. DataExtractionAgent - API data collection and standardization
 * 3. EarlyDuplicateDetector - Categorize opportunities (new/update/skip)
 * 4. Pipeline Branching with Path Analytics:
 *    - NEW opportunities ‚Üí AnalysisAgent ‚Üí Filter ‚Üí StorageAgent
 *    - DUPLICATE with changes ‚Üí DirectUpdateHandler (bypass expensive stages)
 *    - DUPLICATE without changes ‚Üí Skip entirely
 * 
 * Benefits over previous version:
 * - 60-80% token reduction by preventing duplicates from reaching LLM stages
 * - 60-80% faster processing through optimized flow
 * - Comprehensive metrics capture for dashboard analytics
 * - Real-time optimization impact tracking
 * - Opportunity flow path analytics for insights
 * - Direct database updates for critical fields only
 * - Modular architecture with intelligent branching
 * - No timeout constraints for large datasets
 */

import { analyzeSource } from '../agents-v2/core/sourceOrchestrator.js';
import { extractFromSource } from '../agents-v2/core/dataExtractionAgent/index.js';
import { detectDuplicates } from '../agents-v2/optimization/earlyDuplicateDetector.js';
import { updateDuplicateOpportunities } from '../agents-v2/optimization/directUpdateHandler.js';
import { enhanceOpportunities } from '../agents-v2/core/analysisAgent/index.js';
import { filterOpportunities } from '../agents-v2/core/filterFunction.js';
import { storeOpportunities } from '../agents-v2/core/storageAgent/index.js';
import { RunManagerV2 } from './runManagerV2.js';
import { initializeGlobalErrorHandlers, wrapWithErrorHandling, withTimeout } from '../utils/errorHandlers.js';
import { retryStage, CircuitBreaker } from '../utils/retryHandler.js';
import { classifyError, formatErrorForLogging, RetryPolicy } from '../utils/pipelineErrors.js';

/**
 * Helper function to extract execution time from various result formats
 * @param {Object} result - The result object from a stage
 * @param {string} metricsField - Optional metrics field name to check
 * @returns {number} - The execution time in milliseconds
 */
function extractExecutionTime(result, metricsField = 'metrics') {
  // Check direct executionTime field
  if (result?.executionTime) return result.executionTime;
  
  // Check metrics field variations
  if (result?.[metricsField]?.executionTime) return result[metricsField].executionTime;
  if (result?.[metricsField]?.totalExecutionTime) return result[metricsField].totalExecutionTime;
  
  // Check common alternative field names
  if (result?.processingTime) return result.processingTime;
  if (result?.metrics?.processingTime) return result.metrics.processingTime;
  
  // Default to 0 if no time found
  return 0;
}

/**
 * Generate a stable lock ID from a UUID string
 * @param {string} uuid - The UUID string
 * @returns {number} - A stable integer lock ID
 */
function generateLockId(uuid) {
  // Remove hyphens and take first 8 characters of UUID
  const cleanUuid = uuid.replace(/-/g, '').substring(0, 8);
  // Convert hex to integer and ensure it fits in PostgreSQL integer range
  const lockId = parseInt(cleanUuid, 16) % 2147483647;
  return lockId > 0 ? lockId : -lockId; // Ensure positive
}

/**
 * Acquire an advisory lock for a source to prevent concurrent processing
 * @param {string} sourceId - The source ID to lock
 * @param {Object} supabase - Supabase client
 * @returns {Promise<{acquired: boolean, lockId: number}>} - Lock acquisition result
 */
async function acquireSourceLock(sourceId, supabase) {
  try {
    const lockId = generateLockId(sourceId);
    
    // Try to acquire advisory lock (non-blocking)
    const { data, error } = await supabase.rpc('try_advisory_lock', {
      lock_key: lockId
    });
    
    if (error) {
      console.error('[ProcessCoordinatorV2] Error acquiring lock:', error);
      return { acquired: false, lockId };
    }
    
    return { acquired: data === true, lockId };
  } catch (error) {
    console.error('[ProcessCoordinatorV2] Failed to acquire lock:', error);
    return { acquired: false, lockId: null };
  }
}

/**
 * Release an advisory lock for a source
 * @param {number} lockId - The lock ID to release
 * @param {Object} supabase - Supabase client
 */
async function releaseSourceLock(lockId, supabase) {
  if (!lockId) return;
  
  try {
    const { error } = await supabase.rpc('release_advisory_lock', {
      lock_key: lockId
    });
    
    if (error) {
      console.error('[ProcessCoordinatorV2] Error releasing lock:', error);
    }
  } catch (error) {
    console.error('[ProcessCoordinatorV2] Failed to release lock:', error);
  }
}

/**
 * Check if force full reprocessing is enabled for a source
 * Checks both source-specific and global flags
 * @param {string} sourceId - The source ID to check
 * @param {Object} supabase - Supabase client
 * @returns {Promise<boolean>} - True if force processing is enabled
 */
async function shouldForceFullProcessing(sourceId, supabase) {
  try {
    // Call the database function that checks both flags
    const { data, error } = await supabase.rpc('should_force_full_reprocessing', {
      source_id: sourceId
    });
    
    if (error) {
      console.error('[ProcessCoordinatorV2] Error checking force reprocessing flags:', error);
      return false; // Default to normal processing on error
    }
    
    return data === true;
  } catch (error) {
    console.error('[ProcessCoordinatorV2] Failed to check force reprocessing flags:', error);
    return false; // Default to normal processing on error
  }
}


/**
 * ProcessCoordinatorV2 - Main Pipeline Orchestrator with Enhanced Metrics
 * 
 * Features enhanced clean metrics system that captures:
 * - Optimization impact analytics (token savings, time improvements)
 * - Opportunity processing path tracking (NEW/UPDATE/SKIP flows)
 * - Stage-by-stage performance monitoring
 * - Duplicate detection effectiveness measurement
 * - Real-time dashboard-ready metrics
 * 
 * @param {string} sourceId - Source ID to process
 * @param {string} runId - Optional existing run ID
 * @param {Object} supabase - Supabase client instance
 * @param {Object} anthropic - Anthropic client instance
 * @param {Object} options - Optional processing configuration
 * @returns {Promise<Object>} - Complete processing results with enhanced metrics
 */
export async function processApiSourceV2(sourceId, runId = null, supabase, anthropic, options = {}) {
  // Input validation
  if (!sourceId || typeof sourceId !== 'string') {
    throw new Error('Invalid sourceId: must be a non-empty string');
  }
  
  if (runId !== null && typeof runId !== 'string') {
    throw new Error('Invalid runId: must be null or a string');
  }
  
  if (!supabase || typeof supabase !== 'object') {
    throw new Error('Invalid supabase client: must be a valid Supabase instance');
  }
  
  if (!anthropic || typeof anthropic !== 'object') {
    throw new Error('Invalid anthropic client: must be a valid Anthropic instance');
  }
  
  if (options && typeof options !== 'object') {
    throw new Error('Invalid options: must be an object');
  }
  
  // Validate and sanitize sourceId (proper UUID validation)
  const sanitizedSourceId = sourceId.trim();
  const uuidRegex = /^[0-9a-f]{8}-[0-9a-f]{4}-[1-5][0-9a-f]{3}-[89ab][0-9a-f]{3}-[0-9a-f]{12}$/i;
  if (!uuidRegex.test(sanitizedSourceId)) {
    throw new Error('Invalid sourceId format: must be a valid UUID');
  }
  
  const startTime = Date.now()
  let runManager = null
  let sourceLock = { acquired: false, lockId: null }
  
  // Initialize global error handlers to prevent silent failures
  try {
    initializeGlobalErrorHandlers();
  } catch (error) {
    console.warn('[ProcessCoordinatorV2] ‚ö†Ô∏è Failed to initialize global error handlers:', error);
  }
  
  // Configure retry policies for different stage types
  const retryPolicies = {
    sourceOrchestrator: { ...RetryPolicy.CONSERVATIVE, maxAttempts: 2 },
    dataExtraction: { ...RetryPolicy.DEFAULT, maxAttempts: 3 },
    duplicateDetection: { ...RetryPolicy.AGGRESSIVE, maxAttempts: 4 },
    analysis: { ...RetryPolicy.DEFAULT, maxAttempts: 3 },
    storage: { ...RetryPolicy.AGGRESSIVE, maxAttempts: 5 }
  };
  
  // Enhanced metrics tracking
  const metrics = {
    totalTokensUsed: 0,
    totalApiCalls: 0,
    stageMetrics: {},
    optimizationImpact: {
      totalOpportunities: 0,
      bypassedLLM: 0
    },
    opportunityPaths: [],
    duplicateDetectionMetrics: null,
    retryAttempts: 0,
    recoveredErrors: 0,
    forceFullProcessingUsed: false
  }
  
  // Track if force full processing is enabled
  let forceFullProcessing = false;
  
  try {
    console.log(`[ProcessCoordinatorV2] üöÄ Starting V2 processing: ${sourceId}`)
    
    // Step 0: Acquire advisory lock to prevent concurrent processing
    sourceLock = await acquireSourceLock(sourceId, supabase);
    if (!sourceLock.acquired) {
      console.warn(`[ProcessCoordinatorV2] ‚ö†Ô∏è Could not acquire lock for source ${sourceId} - another process may be running`);
      // Continue anyway but log this in metrics
      metrics.concurrentProcessingDetected = true;
    } else {
      console.log(`[ProcessCoordinatorV2] üîí Acquired lock for source ${sourceId} (lock ID: ${sourceLock.lockId})`);
    }
    
    // Step 1: Get source with configurations (same as V1)
    const { data: source, error: sourceError } = await supabase
      .from('api_sources')
      .select('*')
      .eq('id', sanitizedSourceId)
      .single()
    
    if (!source || sourceError) {
      throw new Error(`Source not found: ${sourceId}`)
    }
    
    // Fetch configurations
    const { data: configData, error: configError } = await supabase
      .from('api_source_configurations')
      .select('*')
      .eq('source_id', sanitizedSourceId)
    
    if (configError) throw configError
    
    // Group configurations by type
    const configurations = {}
    configData.forEach(config => {
      configurations[config.config_type] = config.configuration
    })
    source.configurations = configurations
    
    // Step 2: Initialize RunManager V2 with enhanced metrics configuration
    runManager = new RunManagerV2(runId, supabase)
    if (!runId) {
      const runConfiguration = {
        pipeline_version: 'v2.0',
        optimization_enabled: true,
        early_duplicate_detection: true,
        metrics_collection: true,
        ...options
      }
      await runManager.startRun(sourceId, runConfiguration)
    }
    
    // ======================================
    // OPTIMIZED V2 PIPELINE WITH EARLY DUPLICATE DETECTION
    // ======================================
    
    // Step 3: SourceOrchestrator V2 - Source analysis and configuration with enhanced metrics
    console.log(`[ProcessCoordinatorV2] üéØ Stage 1: SourceOrchestrator`)
    await runManager.updateV2SourceOrchestrator('processing', null, null, 0, 0, 0, 0)
    
    let sourceAnalysis;
    try {
      // Execute with retry logic
      const result = await retryStage(
        'SourceOrchestrator',
        async (attempt) => {
          console.log(`[ProcessCoordinatorV2] Executing SourceOrchestrator (attempt ${attempt})`)
          return await analyzeSource(source, anthropic)
        },
        runManager,
        { 
          maxAttempts: retryPolicies.sourceOrchestrator.maxAttempts,
          policy: retryPolicies.sourceOrchestrator
        }
      )
      
      sourceAnalysis = result.result
      metrics.retryAttempts += (result.attempts - 1)
      if (result.attempts > 1) metrics.recoveredErrors++
      
      // Track metrics for this stage
      const sourceTokens = sourceAnalysis.tokenUsage || 0
      const sourceApiCalls = sourceAnalysis.apiCalls || 1
      const sourceExecutionTime = sourceAnalysis.executionTime || 1 // Defensive: ensure minimum 1ms
      
      metrics.totalTokensUsed += sourceTokens
      metrics.totalApiCalls += sourceApiCalls
      metrics.stageMetrics.sourceOrchestrator = {
        executionTime: sourceExecutionTime,
        tokensUsed: sourceTokens,
        apiCalls: sourceApiCalls,
        retryAttempts: result.attempts - 1
      }
      
      // Source Orchestrator has no input (it's analyzing the source), output is the analysis
      await runManager.updateV2SourceOrchestrator('completed', sourceAnalysis, { executionTime: sourceExecutionTime }, sourceTokens, sourceApiCalls, 0, 1)
      console.log(`[ProcessCoordinatorV2] ‚úÖ SourceOrchestrator completed in ${sourceExecutionTime}ms (${sourceTokens} tokens)`)
    } catch (error) {
      const classified = classifyError(error)
      console.error(`[ProcessCoordinatorV2] ‚ùå SourceOrchestrator failed:`, formatErrorForLogging(classified))
      await runManager.updateV2SourceOrchestrator('failed', null, null, 0, 0, 0, 0)
      throw classified
    }
    
    // Step 4: DataExtractionAgent V2 - API data collection + standardization with enhanced metrics
    console.log(`[ProcessCoordinatorV2] üì° Stage 2: DataExtractionAgent`)
    await runManager.updateV2DataExtraction('processing', null, null, 0, 0, 0, 0)
    
    let extractionResult;
    try {
      // Execute with retry logic
      const result = await retryStage(
        'DataExtractionAgent',
        async (attempt) => {
          console.log(`[ProcessCoordinatorV2] Executing DataExtractionAgent (attempt ${attempt})`)
          return await extractFromSource(source, sourceAnalysis, anthropic)
        },
        runManager,
        { 
          maxAttempts: retryPolicies.dataExtraction.maxAttempts,
          policy: retryPolicies.dataExtraction
        }
      )
      
      extractionResult = result.result
      metrics.retryAttempts += (result.attempts - 1)
      if (result.attempts > 1) metrics.recoveredErrors++
      
      // Track metrics for this stage
      const extractionTokens = extractionResult.extractionMetrics?.totalTokens || 0
      const extractionApiCalls = extractionResult.extractionMetrics?.apiCalls || 1
      const totalAvailable = extractionResult.extractionMetrics?.totalFound || 0  // Total available from API
      const apiFetchedResults = extractionResult.extractionMetrics?.totalRetrieved || 
                               extractionResult.opportunities.length  // What we actually fetched
      const extractedOpportunities = extractionResult.opportunities.length  // Successfully extracted
      
      metrics.totalTokensUsed += extractionTokens
      metrics.totalApiCalls += extractionApiCalls
      metrics.optimizationImpact.totalOpportunities = extractedOpportunities
      metrics.stageMetrics.dataExtraction = {
        executionTime: extractExecutionTime(extractionResult, 'extractionMetrics'),
        tokensUsed: extractionTokens,
        apiCalls: extractionApiCalls,
        totalAvailable: totalAvailable,  // Total available according to API
        apiFetchedResults: apiFetchedResults,  // What we actually fetched
        opportunitiesExtracted: extractedOpportunities,  // What we successfully extracted
        extractionRate: apiFetchedResults > 0 ? Math.round((extractedOpportunities / apiFetchedResults) * 100) : 0,
        retryAttempts: result.attempts - 1
      }
      
      // Debug logging with size limits and environment control
      if (process.env.NODE_ENV === 'development') {
        const debugData = {
          extractionResultType: typeof extractionResult,
          opportunitiesLength: extractionResult.opportunities?.length,
          // Limit object keys to prevent memory issues
          extractionMetricsKeys: Object.keys(extractionResult.extractionMetrics || {}).slice(0, 10),
          totalFound: extractionResult.extractionMetrics?.totalFound,
          successfullyExtracted: extractionResult.extractionMetrics?.successfullyExtracted,
          actualOpportunitiesArray: Array.isArray(extractionResult.opportunities)
        };
        if (process.env.NODE_ENV === 'development') {
          console.log(`[ProcessCoordinatorV2] üîç DEBUG - Data Extraction Result:`, debugData);
        }
      }
      
      // Data Extraction: input is source orchestrator output (1), output is number of opportunities extracted
      // Store total available, fetched results, and extracted opportunities in the metrics
      const enhancedExtractionMetrics = {
        ...extractionResult.extractionMetrics,
        totalFound: totalAvailable,  // Keep original field name for backward compatibility
        totalAvailable: totalAvailable,  // Total available according to API
        apiFetchedResults: apiFetchedResults,  // What we actually fetched
        extractedOpportunities: extractedOpportunities,  // What we successfully extracted
        extractionRate: apiFetchedResults > 0 ? Math.round((extractedOpportunities / apiFetchedResults) * 100) : 0
      }
      
      if (process.env.NODE_ENV === 'development') {
        console.log(`[ProcessCoordinatorV2] üìä Enhanced extraction metrics:`, {
          totalAvailable,
          apiFetchedResults,
          extractedOpportunities,
          originalTotalFound: extractionResult.extractionMetrics?.totalFound,
          originalTotalRetrieved: extractionResult.extractionMetrics?.totalRetrieved
        });
      }
      // Pass the enhanced metrics as the stage results so they're stored properly
      await runManager.updateV2DataExtraction('completed', enhancedExtractionMetrics, enhancedExtractionMetrics, extractionTokens, extractionApiCalls, 1, extractedOpportunities)
      console.log(`[ProcessCoordinatorV2] ‚úÖ DataExtraction completed: ${extractedOpportunities} opportunities extracted from ${apiFetchedResults} fetched (${totalAvailable} total available, ${Math.round((extractedOpportunities / apiFetchedResults) * 100)}% extraction rate, ${extractionTokens} tokens)`)
    } catch (error) {
      const classified = classifyError(error)
      console.error(`[ProcessCoordinatorV2] ‚ùå DataExtractionAgent failed:`, formatErrorForLogging(classified))
      await runManager.updateV2DataExtraction('failed', null, null, 0, 0, 1, 0)
      
      // For API errors, check if we should halt processing
      if (classified.category === 'API' && !classified.retryable) {
        console.error(`[ProcessCoordinatorV2] ‚ùå Non-retryable API error, halting pipeline`)
      }
      throw classified
    }
    
    // Step 5: EarlyDuplicateDetector - Categorize opportunities BEFORE expensive processing with enhanced analytics
    console.log(`[ProcessCoordinatorV2] üîç Stage 3: EarlyDuplicateDetector`)
    await runManager.updateV2EarlyDuplicateDetector('processing', null, null, 0, 0)
    
    let duplicateDetection;
    const extractedCount = extractionResult?.opportunities?.length || 0; // Declare here for error handling access
    
    // Check if force full reprocessing is enabled
    forceFullProcessing = await shouldForceFullProcessing(sourceId, supabase);
    
    if (forceFullProcessing) {
      console.log(`[ProcessCoordinatorV2] ‚ö†Ô∏è Force full reprocessing enabled - bypassing duplicate detection`);
      metrics.forceFullProcessingUsed = true;
      
      // Create mock duplicate detection result - treat all as new
      duplicateDetection = {
        newOpportunities: extractionResult.opportunities,
        opportunitiesToUpdate: [],
        opportunitiesToSkip: [],
        metrics: {
          totalProcessed: extractedCount,
          newOpportunities: extractedCount,
          opportunitiesToUpdate: 0,
          opportunitiesToSkip: 0,
          executionTime: 1  // Minimal execution time for bypassed detection
        },
        enhancedMetrics: {
          forceFullProcessingBypassed: true
        }
      };
      
      // Update run manager with bypassed detection
      await runManager.updateV2EarlyDuplicateDetector('completed', duplicateDetection, duplicateDetection.metrics, extractedCount, extractedCount);
      console.log(`[ProcessCoordinatorV2] ‚úÖ Duplicate detection bypassed: All ${extractedCount} opportunities marked as new for full processing`);
    } else {
      // Normal duplicate detection flow
      try {
        duplicateDetection = await detectDuplicates(
          extractionResult.opportunities,
          sourceId,
          supabase,
          extractionResult.rawResponseId // Pass through API response tracking
        )
      
        // Track optimization impact metrics
        const bypassedOpportunities = duplicateDetection.metrics.opportunitiesToUpdate + duplicateDetection.metrics.opportunitiesToSkip
        metrics.optimizationImpact.bypassedLLM = bypassedOpportunities
        metrics.duplicateDetectionMetrics = duplicateDetection.metrics
        
        // Track optimization impact (absolute metrics only)
        
        // Track duplicate detection stage metrics
        metrics.stageMetrics.earlyDuplicateDetector = {
          executionTime: extractExecutionTime(duplicateDetection, 'metrics'),
          totalProcessed: duplicateDetection.metrics.totalProcessed,
          newOpportunities: duplicateDetection.metrics.newOpportunities,
          opportunitiesToUpdate: duplicateDetection.metrics.opportunitiesToUpdate,
          opportunitiesToSkip: duplicateDetection.metrics.opportunitiesToSkip,
          optimizationImpact: bypassedOpportunities
        }
        
        // Early Duplicate Detector: input is all extracted, output is NEW + UPDATE (all opportunities continuing to be processed)
        const dupeInputCount = extractedCount
        const dupeOutputCount = duplicateDetection.metrics.newOpportunities + duplicateDetection.metrics.opportunitiesToUpdate  // NEW + UPDATE (all continuing)
        await runManager.updateV2EarlyDuplicateDetector('completed', duplicateDetection, duplicateDetection.metrics, dupeInputCount, dupeOutputCount)
        console.log(`[ProcessCoordinatorV2] ‚úÖ Duplicate detection completed: ${duplicateDetection.metrics.newOpportunities} new, ${duplicateDetection.metrics.opportunitiesToUpdate} to update, ${duplicateDetection.metrics.opportunitiesToSkip} to skip (${bypassedOpportunities} bypassed LLM)`)
      } catch (error) {
        console.error(`[ProcessCoordinatorV2] ‚ùå EarlyDuplicateDetector failed:`, error)
        await runManager.updateV2EarlyDuplicateDetector('failed', null, null, extractedCount, 0)
        throw new Error(`EarlyDuplicateDetector stage failed: ${error.message}`)
      }
    }
    
    // Record opportunity paths for analytics
    for (const opportunity of duplicateDetection.newOpportunities) {
      metrics.opportunityPaths.push({
        opportunity,
        pathType: 'NEW',
        pathReason: 'no_duplicate_found',
        stagesProcessed: ['data_extraction', 'early_duplicate_detector'],
        analytics: { duplicateDetected: false }
      })
    }
    
    for (const opportunityToUpdate of duplicateDetection.opportunitiesToUpdate) {
      metrics.opportunityPaths.push({
        opportunity: opportunityToUpdate.apiRecord,
        pathType: 'UPDATE',
        pathReason: opportunityToUpdate.reason,
        stagesProcessed: ['data_extraction', 'early_duplicate_detector'],
        analytics: {
          duplicateDetected: true,
          existingOpportunityId: opportunityToUpdate.dbRecord?.id,
          changesDetected: opportunityToUpdate.changesDetected || []
        }
      })
    }
    
    for (const opportunityToSkip of duplicateDetection.opportunitiesToSkip) {
      metrics.opportunityPaths.push({
        opportunity: opportunityToSkip.apiRecord,
        pathType: 'SKIP',
        pathReason: opportunityToSkip.reason,
        stagesProcessed: ['data_extraction', 'early_duplicate_detector'],
        finalOutcome: 'skipped',
        analytics: {
          duplicateDetected: true,
          existingOpportunityId: opportunityToSkip.existingRecord?.id
        }
      })
    }
    
    // ======================================
    // PIPELINE BRANCHING BASED ON DUPLICATE DETECTION
    // ======================================
    
    let analysisResult = { opportunities: [], analysisMetrics: {} }
    let filterResult = { includedOpportunities: [], filterMetrics: {} }
    let storageResult = { metrics: {} }
    let directUpdateResult = { metrics: {} }
    
    // Branch 1: Process NEW opportunities through full pipeline
    let analysisTokens = 0;
    let analysisApiCalls = 0;
    let analysisOutputCount = 0;
    let filterOutputCount = 0;
    
    if (duplicateDetection.newOpportunities.length > 0) {
      console.log(`[ProcessCoordinatorV2] üÜï Processing ${duplicateDetection.newOpportunities.length} NEW opportunities through full pipeline`)
      
      // Step 6A: AnalysisAgent - Only for NEW opportunities with enhanced metrics
      console.log(`[ProcessCoordinatorV2] üß† Stage 4A: AnalysisAgent (NEW opportunities only)`)
      await runManager.updateV2Analysis('processing', null, null, 0, 0, 0, 0)
      
      try {
        analysisResult = await enhanceOpportunities(duplicateDetection.newOpportunities, source, anthropic)
        
        // Track analysis stage metrics
        analysisTokens = analysisResult.analysisMetrics?.totalTokens || 0
        analysisApiCalls = analysisResult.analysisMetrics?.totalApiCalls || 0
        metrics.totalTokensUsed += analysisTokens
        metrics.totalApiCalls += analysisApiCalls
        metrics.stageMetrics.analysis = {
          executionTime: extractExecutionTime(analysisResult, 'analysisMetrics'),
          tokensUsed: analysisTokens,
          apiCalls: analysisApiCalls,
          opportunitiesProcessed: duplicateDetection.newOpportunities.length,
          opportunitiesEnhanced: analysisResult.opportunities.length
        }
        
        // Analysis: input is new opportunities, output is enhanced opportunities
        const analysisInputCount = duplicateDetection.newOpportunities.length
        analysisOutputCount = analysisResult.opportunities.length
        
        // Validation: Analysis input should match Early Duplicate Detector NEW output
        const dupeNewOutputCount = duplicateDetection.newOpportunities.length;
        if (analysisInputCount !== dupeNewOutputCount) {
          console.warn(`[ProcessCoordinatorV2] ‚ö†Ô∏è Stage handoff mismatch [early_duplicate_detector ‚Üí analysis]: expected ${dupeNewOutputCount} opportunities, got ${analysisInputCount}`);
        }
        
        // Pass executionTime as performanceMetrics using standardized extraction
        const analysisPerformanceMetrics = { executionTime: extractExecutionTime(analysisResult, 'analysisMetrics') };
        await runManager.updateV2Analysis('completed', analysisResult, analysisPerformanceMetrics, analysisTokens, analysisApiCalls, analysisInputCount, analysisOutputCount)
        console.log(`[ProcessCoordinatorV2] ‚úÖ Analysis completed: ${analysisOutputCount} NEW opportunities enhanced (${analysisTokens} tokens)`)
      } catch (error) {
        console.error(`[ProcessCoordinatorV2] ‚ùå AnalysisAgent failed:`, error)
        await runManager.updateV2Analysis('failed', null, null, 0, 0, duplicateDetection.newOpportunities.length, 0)
        throw new Error(`AnalysisAgent stage failed: ${error.message}`)
      }
      
      // Update opportunity paths with analysis stage
      metrics.opportunityPaths.forEach(path => {
        if (path.pathType === 'NEW') {
          path.stagesProcessed.push('analysis')
          path.analytics.tokensUsed = (path.analytics.tokensUsed || 0) + (analysisTokens / analysisResult.opportunities.length)
        }
      })
      
      // Step 7A: Filter Function - Only for analyzed NEW opportunities with enhanced metrics
      console.log(`[ProcessCoordinatorV2] üîç Stage 5A: Filter Function (NEW opportunities only)`)
      await runManager.updateV2Filter('processing', null, null, 0, 0)
      
      try {
        filterResult = filterOpportunities(analysisResult.opportunities)
        
        // Track filter stage metrics
        metrics.stageMetrics.filter = {
          executionTime: extractExecutionTime(filterResult, 'filterMetrics'),
          opportunitiesInput: analysisResult.opportunities.length,
          opportunitiesOutput: filterResult.includedOpportunities.length,
          opportunitiesFiltered: analysisResult.opportunities.length - filterResult.includedOpportunities.length,
          filterRate: Math.round((filterResult.includedOpportunities.length / analysisResult.opportunities.length) * 100)
        }
        
        // Filter: input is analyzed opportunities, output is filtered opportunities
        const filterInputCount = analysisResult.opportunities.length
        filterOutputCount = filterResult.includedOpportunities.length
        
        // Validation: Filter input should match Analysis output
        if (filterInputCount !== analysisOutputCount) {
          console.warn(`[ProcessCoordinatorV2] ‚ö†Ô∏è Stage handoff mismatch [analysis ‚Üí filter]: expected ${analysisOutputCount} opportunities, got ${filterInputCount}`);
        }
        
        // Pass processingTime as performanceMetrics.executionTime using standardized extraction
        const filterPerformanceMetrics = { executionTime: extractExecutionTime(filterResult) };
        await runManager.updateV2Filter('completed', filterResult, filterPerformanceMetrics, filterInputCount, filterOutputCount)
        console.log(`[ProcessCoordinatorV2] ‚úÖ Filter completed: ${filterOutputCount} NEW opportunities passed filtering (${metrics.stageMetrics.filter.filterRate}% pass rate)`)
      } catch (error) {
        console.error(`[ProcessCoordinatorV2] ‚ùå Filter Function failed:`, error)
        await runManager.updateV2Filter('failed', null, null, analysisResult.opportunities.length, 0)
        throw new Error(`Filter Function stage failed: ${error.message}`)
      }
      
      // Update opportunity paths with filter stage and outcomes
      metrics.opportunityPaths.forEach(path => {
        if (path.pathType === 'NEW') {
          path.stagesProcessed.push('filter')
          const passedFilter = filterResult.includedOpportunities.some(opp => 
            (opp.api_opportunity_id || opp.id) === (path.opportunity.api_opportunity_id || path.opportunity.id)
          )
          if (!passedFilter) {
            path.finalOutcome = 'filtered_out'
          }
        }
      })
      
      // Step 8A: StorageAgent - Store filtered NEW opportunities with enhanced metrics
      if (filterResult.includedOpportunities.length > 0) {
        console.log(`[ProcessCoordinatorV2] üíæ Stage 6A: StorageAgent (NEW opportunities)`)
        await runManager.updateV2Storage('processing', null, null, 0, 0, 0)
        
        try {
          storageResult = await storeOpportunities(
            filterResult.includedOpportunities,
            source,
            supabase,
            forceFullProcessing // Pass the flag to storage agent
          )
          
          // Track storage stage metrics
          metrics.stageMetrics.storage = {
            executionTime: extractExecutionTime(storageResult, 'metrics'),
            opportunitiesInput: filterResult.includedOpportunities.length,
            opportunitiesStored: storageResult.metrics?.newOpportunities || 0,
            opportunitiesFailed: storageResult.metrics?.failed || 0,
            successRate: filterResult.includedOpportunities.length > 0 
            ? Math.round(((storageResult.metrics?.newOpportunities || 0) / filterResult.includedOpportunities.length) * 100)
            : 0
          }
          
          // Storage: input is filtered opportunities, output is successfully stored
          const storageInputCount = filterResult.includedOpportunities.length
          const storageOutputCount = storageResult.metrics?.newOpportunities || 0
          
          // Validation: Storage input should match Filter output
          if (storageInputCount !== filterOutputCount) {
            console.warn(`[ProcessCoordinatorV2] ‚ö†Ô∏è Stage handoff mismatch [filter ‚Üí storage]: expected ${filterOutputCount} opportunities, got ${storageInputCount}`);
          }
          
          // Pass executionTime as performanceMetrics using standardized extraction
          const storagePerformanceMetrics = { executionTime: extractExecutionTime(storageResult) };
          await runManager.updateV2Storage('completed', storageResult, storagePerformanceMetrics, 0, storageInputCount, storageOutputCount)
          console.log(`[ProcessCoordinatorV2] ‚úÖ Storage completed: ${storageOutputCount} NEW opportunities stored (${metrics.stageMetrics.storage.successRate}% success rate)`)
        } catch (error) {
          console.error(`[ProcessCoordinatorV2] ‚ùå StorageAgent failed:`, error)
          await runManager.updateV2Storage('failed', null, null, 0, filterResult.includedOpportunities.length, 0)
          throw new Error(`StorageAgent stage failed: ${error.message}`)
        }
        
        // Update opportunity paths with storage outcomes
        metrics.opportunityPaths.forEach(path => {
          if (path.pathType === 'NEW' && path.finalOutcome !== 'filtered_out') {
            path.stagesProcessed.push('storage')
            path.finalOutcome = 'stored'
          }
        })
      }
    } else {
      console.log(`[ProcessCoordinatorV2] ‚ÑπÔ∏è No NEW opportunities to process through full pipeline`)
      // Skipped stages still need counts (0 in, 0 out)
      await runManager.updateV2Analysis('skipped', { reason: 'no_new_opportunities' }, null, 0, 0, 0, 0)
      await runManager.updateV2Filter('skipped', { reason: 'no_new_opportunities' }, null, 0, 0)
    }
    
    // Branch 2: Direct update for duplicates with changes (bypass expensive stages) with enhanced metrics
    if (duplicateDetection.opportunitiesToUpdate.length > 0) {
      console.log(`[ProcessCoordinatorV2] üîÑ Processing ${duplicateDetection.opportunitiesToUpdate.length} duplicates with direct updates`)
      await runManager.updateV2DirectUpdate('processing', null, null, 0, 0)
      
      try {
        directUpdateResult = await updateDuplicateOpportunities(
          duplicateDetection.opportunitiesToUpdate,
          supabase
        )
        
        // Track direct update stage metrics
        metrics.stageMetrics.directUpdate = {
          executionTime: extractExecutionTime(directUpdateResult, 'metrics'),
          opportunitiesInput: duplicateDetection.opportunitiesToUpdate.length,
          opportunitiesUpdated: directUpdateResult.metrics.successful,
          opportunitiesFailed: directUpdateResult.metrics.failed,
          successRate: duplicateDetection.opportunitiesToUpdate.length > 0 
            ? Math.round(((directUpdateResult.metrics?.successful || 0) / duplicateDetection.opportunitiesToUpdate.length) * 100)
            : 0
        }
        
        // Direct Update: input is opportunities to update, output is total processed (successful + failed + skipped)
        const directUpdateInputCount = duplicateDetection.opportunitiesToUpdate.length
        const directUpdateOutputCount = directUpdateResult.metrics?.totalProcessed || 0
        
        // Warn if totalProcessed is missing - this could indicate processing failures
        if (!directUpdateResult.metrics?.totalProcessed && directUpdateInputCount > 0) {
          console.warn(`[ProcessCoordinatorV2] ‚ö†Ô∏è Direct update metrics missing totalProcessed field for ${directUpdateInputCount} opportunities`);
        }
        
        // Validation: Direct Update input should match the UPDATE count from Early Duplicate Detector
        const expectedUpdateCount = duplicateDetection.metrics?.opportunitiesToUpdate || 0
        if (directUpdateInputCount !== expectedUpdateCount) {
          console.warn(`[ProcessCoordinatorV2] ‚ö†Ô∏è Stage handoff mismatch [early_duplicate_detector ‚Üí direct_update]: expected ${expectedUpdateCount} UPDATE opportunities, got ${directUpdateInputCount}`);
        }
        
        await runManager.updateV2DirectUpdate('completed', directUpdateResult, directUpdateResult.metrics, directUpdateInputCount, directUpdateOutputCount)
        console.log(`[ProcessCoordinatorV2] ‚úÖ Direct updates completed: ${directUpdateOutputCount} successful, ${directUpdateResult.metrics.failed} failed (${metrics.stageMetrics.directUpdate.successRate}% success rate)`)
      } catch (error) {
        console.error(`[ProcessCoordinatorV2] ‚ùå DirectUpdateHandler failed:`, error)
        await runManager.updateV2DirectUpdate('failed', null, null, duplicateDetection.opportunitiesToUpdate.length, 0)
        throw new Error(`DirectUpdateHandler stage failed: ${error.message}`)
      }
      
      // Update opportunity paths with direct update outcomes
      metrics.opportunityPaths.forEach(path => {
        if (path.pathType === 'UPDATE') {
          path.stagesProcessed.push('direct_update')
          path.finalOutcome = 'updated'
        }
      })
    }
    
    // Branch 3: Skip duplicates without changes (logged for metrics)
    if (duplicateDetection.opportunitiesToSkip.length > 0) {
      console.log(`[ProcessCoordinatorV2] ‚è≠Ô∏è Skipped ${duplicateDetection.opportunitiesToSkip.length} duplicates without changes`)
    }
    
    // ======================================
    // COMPLETE OPTIMIZED V2 PIPELINE WITH ENHANCED METRICS
    // ======================================
    
    // Calculate total execution time and finalize optimization metrics
    const executionTime = Date.now() - startTime
    
    // Calculate absolute performance metrics for database storage
    const totalSuccessfulOpportunities = (storageResult.metrics?.newOpportunities || 0) + (directUpdateResult.metrics?.successful || 0)
    const COST_PER_TOKEN = 0.00001; // Estimated cost per token in USD
    const estimatedCost = metrics.totalTokensUsed * COST_PER_TOKEN
    
    // Remove efficiency score calculation - using absolute metrics only
    
    // Ensure all database operations are complete before calculating final metrics
    await Promise.all([
      // Ensure storage operations are complete
      storageResult?.promise,
      directUpdateResult?.promise
    ].filter(Boolean));
    
    // Update absolute performance metrics in RunManager
    await runManager.updateOptimizationMetrics({
      totalOpportunities: metrics.optimizationImpact.totalOpportunities,
      bypassedLLM: metrics.optimizationImpact.bypassedLLM,
      totalTokens: metrics.totalTokensUsed,
      totalApiCalls: metrics.totalApiCalls,
      estimatedCost,
      successfulOpportunities: totalSuccessfulOpportunities
    })
    
    // Record all opportunity paths
    for (const pathData of metrics.opportunityPaths) {
      await runManager.recordOpportunityPath(
        pathData.opportunity,
        pathData.pathType,
        pathData.pathReason,
        pathData.stagesProcessed,
        pathData.finalOutcome || 'skipped',
        pathData.analytics
      )
    }
    
    console.log(`[ProcessCoordinatorV2] üèÅ Total execution time: ${executionTime}ms`)
    console.log(`[ProcessCoordinatorV2] üìä Performance: ${totalSuccessfulOpportunities} opportunities processed, ${metrics.optimizationImpact.bypassedLLM} bypassed LLM, ${metrics.totalTokensUsed} tokens used`)
    
    // Overall pipeline flow validation
    const totalActuallyProcessed = (storageResult.metrics?.newOpportunities || 0) + (directUpdateResult.metrics?.totalProcessed || 0)
    const totalSkipped = duplicateDetection.metrics?.opportunitiesToSkip || 0
    const totalCategorized = (duplicateDetection.metrics?.newOpportunities || 0) + (duplicateDetection.metrics?.opportunitiesToUpdate || 0) + totalSkipped
    
    // Validate extraction vs categorization (all extracted should be categorized)
    if (extractedCount !== totalCategorized) {
      console.warn(`[ProcessCoordinatorV2] ‚ö†Ô∏è Pipeline categorization mismatch: extracted (${extractedCount}) !== categorized (${totalCategorized})`);
    }
    
    // Validate processing completeness (NEW + UPDATE should be processed, SKIP should be skipped)
    const expectedToProcess = (duplicateDetection.metrics?.newOpportunities || 0) + (duplicateDetection.metrics?.opportunitiesToUpdate || 0)
    if (totalActuallyProcessed > expectedToProcess) {
      console.warn(`[ProcessCoordinatorV2] ‚ö†Ô∏è Processing overflow: processed (${totalActuallyProcessed}) > expected (${expectedToProcess})`);
    }
    
    console.log(`[ProcessCoordinatorV2] üîç Pipeline Flow Summary: ${extractedCount} extracted ‚Üí ${duplicateDetection.metrics.newOpportunities} NEW + ${duplicateDetection.metrics.opportunitiesToUpdate} UPDATE + ${totalSkipped} SKIP`);
    
    // Complete the run with all pipeline branches and enhanced metrics
    const finalResults = {
      pipeline: 'v2-optimized-with-metrics',
      stages: {
        sourceOrchestrator: sourceAnalysis,
        dataExtraction: extractionResult,
        earlyDuplicateDetector: duplicateDetection,
        analysis: analysisResult,
        filter: filterResult,
        storage: storageResult,
        directUpdate: directUpdateResult
      },
      enhancedMetrics: metrics,
      optimizationImpact: {
        totalOpportunities: metrics.optimizationImpact.totalOpportunities,
        bypassedLLM: metrics.optimizationImpact.bypassedLLM,
        successfulOpportunities: totalSuccessfulOpportunities,
        totalTokensUsed: metrics.totalTokensUsed,
        estimatedCost
      }
    }
    
    // Complete the run with proper error handling
    try {
      await runManager.completeRun(executionTime, finalResults)
    } catch (completeError) {
      console.error('[ProcessCoordinatorV2] ‚ùå Failed to complete run:', completeError)
      // Don't throw - we still want to return results even if the completion update fails
    }
    
    // DEPRECATED: api_activity_logs INSERT - redundant with pipeline_runs table
    // Uses invalid enum value 'complete_processing' (valid: api_check, processing, error)
    // All metrics are already captured in pipeline_runs and pipeline_stages tables
    
    // Auto-disable force full reprocessing flag if it was used
    if (forceFullProcessing) {
      try {
        const { error } = await supabase.rpc('disable_force_full_reprocessing', {
          source_id: sourceId
        });
        
        if (error) {
          console.error('[ProcessCoordinatorV2] ‚ö†Ô∏è Failed to auto-disable force reprocessing flag:', error);
        } else {
          console.log('[ProcessCoordinatorV2] ‚úÖ Force full reprocessing flag auto-disabled for source');
        }
      } catch (error) {
        console.error('[ProcessCoordinatorV2] ‚ö†Ô∏è Error disabling force reprocessing flag:', error);
        // Don't throw - this is non-critical
      }
    }
    
    // Release the advisory lock
    await releaseSourceLock(sourceLock.lockId, supabase);
    console.log(`[ProcessCoordinatorV2] üîì Released lock for source ${sourceId}`);
    
    // Return results with enhanced V2 metrics and V1 compatibility
    return {
      status: 'success',
      version: 'v2.0',
      environment: 'service-module',
      pipeline: 'v2-optimized-with-metrics',
      source: {
        id: source.id,
        name: source.name
      },
      stages: {
        sourceOrchestrator: sourceAnalysis,
        dataExtraction: extractionResult,
        earlyDuplicateDetector: duplicateDetection,
        analysis: analysisResult,
        filter: filterResult,
        storage: storageResult,
        ...(duplicateDetection.opportunitiesToUpdate.length > 0 && { directUpdate: directUpdateResult })
      },
      // Enhanced V2 metrics
      enhancedMetrics: metrics,
      optimizationImpact: {
        totalOpportunities: metrics.optimizationImpact.totalOpportunities,
        bypassedLLM: metrics.optimizationImpact.bypassedLLM,
        successfulOpportunities: totalSuccessfulOpportunities,
        totalTokensUsed: metrics.totalTokensUsed,
        estimatedCost,
        forceFullProcessingUsed: metrics.forceFullProcessingUsed
      },
      metrics: {
        // V2 optimized specific metrics
        sourceAnalysis: sourceAnalysis,
        dataExtraction: extractionResult.extractionMetrics,
        earlyDuplicateDetection: duplicateDetection.metrics,
        analysis: analysisResult.analysisMetrics,
        filter: filterResult.filterMetrics,
        storage: storageResult.metrics,
        directUpdate: directUpdateResult.metrics,
        
        // Performance metrics
        pipelineOptimization: {
          totalOpportunities: extractionResult.opportunities.length,
          processedThroughLLM: analysisResult.opportunities.length,
          bypassedLLM: duplicateDetection.metrics.opportunitiesToUpdate + duplicateDetection.metrics.opportunitiesToSkip,
          successfulOpportunities: totalSuccessfulOpportunities,
          totalTokensUsed: metrics.totalTokensUsed
        },
        
        // V1 compatibility metrics
        initialApiMetrics: { totalHitCount: extractionResult.extractionMetrics?.totalFound || 0 },
        firstStageMetrics: null,
        detailApiMetrics: null,
        secondStageMetrics: null,
        storageMetrics: storageResult.metrics,
        
        totalExecutionTime: executionTime
      },
      runId: runManager.runId
    }
    
  } catch (error) {
    console.error(`[ProcessCoordinatorV2] ‚ùå Pipeline failed:`, error)
    
    // Rollback force reprocessing flag if pipeline failed and it was enabled
    if (forceFullProcessing) {
      try {
        console.log('[ProcessCoordinatorV2] üîÑ Rolling back force reprocessing flag due to pipeline failure');
        // Re-enable the flag since processing failed
        const { error: rollbackError } = await supabase
          .from('api_sources')
          .update({ 
            force_full_reprocessing: true,
            updated_at: new Date().toISOString()
          })
          .eq('id', sourceId);
        
        if (rollbackError) {
          console.error('[ProcessCoordinatorV2] ‚ö†Ô∏è Failed to rollback force reprocessing flag:', rollbackError);
        } else {
          console.log('[ProcessCoordinatorV2] ‚úÖ Force reprocessing flag rolled back - will retry on next run');
        }
      } catch (rollbackError) {
        console.error('[ProcessCoordinatorV2] ‚ö†Ô∏è Error during rollback:', rollbackError);
      }
    }
    
    // Release the advisory lock in case of error
    await releaseSourceLock(sourceLock.lockId, supabase);
    console.log(`[ProcessCoordinatorV2] üîì Released lock for source ${sourceId} after error`);
    
    if (runManager) {
      await runManager.updateRunError(error, 'v2_pipeline')
    }
    
    // Log error (same as V1)
    if (supabase) {
      await supabase.from('api_activities').insert({
        source_id: sourceId,
        activity_type: 'complete_processing', 
        status: 'failure',
        details: {
          error: String(error),
          pipeline: 'v2-optimized'
        }
      })
    }
    
    return {
      status: 'error',
      version: 'v2.0',
      environment: 'service-module',
      message: 'V2 pipeline failed',
      error: error.message,
      stack: error.stack
    }
  }
} 