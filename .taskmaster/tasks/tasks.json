{
  "master": {
    "tasks": [
      {
        "id": 1,
        "title": "Implement Data Extraction Agent",
        "description": "Complete the implementation of the Data Extraction Agent, which will handle the core functionality from the v1 apiHandlerAgent.js.",
        "status": "done",
        "dependencies": [],
        "priority": "high",
        "details": "1. Extract core functionality from apiHandlerAgent.js (2,249 lines)\n2. Implement modular structure following v2 patterns\n3. Create separate modules for:\n   - API request handling (use Axios v1.8.3 for HTTP requests)\n   - Pagination management\n   - Field mapping (consider using json-schema for validation)\n   - Error handling\n   - Retry logic (implement exponential backoff)\n4. Ensure compatibility with both single-step and two-step API patterns\n5. Implement proper error handling and retry logic\n6. Maintain field mapping and taxonomy standardization\n7. Process pagination correctly\n8. Return standardized opportunity data structure\n9. Use JavaScript for implementation\n10. Implement unit tests using Vitest",
        "testStrategy": "1. Create unit tests for each module using Vitest\n2. Implement integration tests for the entire agent\n3. Test with mock API responses for various scenarios\n4. Validate correct handling of pagination\n5. Verify error handling and retry logic\n6. Ensure proper field mapping and data standardization\n7. Test performance and optimize as needed",
        "subtasks": [
          {
            "id": 1,
            "title": "Extract Core Functionality from apiHandlerAgent.js",
            "description": "Analyze the existing apiHandlerAgent.js file and extract the core functionality required for the Data Extraction Agent.",
            "dependencies": [],
            "details": "Review the 2,249 lines of code in apiHandlerAgent.js. Identify and isolate the essential functions and logic related to API request handling, pagination management, field mapping, error handling, and retry logic. Document the extracted functionality for use in subsequent tasks.",
            "status": "done",
            "testStrategy": "Create a checklist of core functionalities and verify each extracted component against the original file."
          },
          {
            "id": 2,
            "title": "Design Modular Structure for Data Extraction Agent",
            "description": "Create a modular architecture design for the Data Extraction Agent following v2 patterns.",
            "dependencies": [
              1
            ],
            "details": "Design a modular structure that separates concerns into distinct modules: API request handling, pagination management, field mapping, error handling, and retry logic. Ensure the design supports both single-step and two-step API patterns. Create a detailed architecture diagram and module specifications.",
            "status": "done",
            "testStrategy": "Review the design with team leads to ensure alignment with v2 architecture patterns and project requirements."
          },
          {
            "id": 3,
            "title": "Implement Core Modules of Data Extraction Agent",
            "description": "Develop the core modules of the Data Extraction Agent using JavaScript and following the designed modular structure.",
            "dependencies": [
              2
            ],
            "details": "Implement separate modules for API request handling (using Axios v1.8.3), pagination management, field mapping (consider using json-schema for validation), error handling, and retry logic with exponential backoff. Ensure proper JSDoc documentation and consistent coding patterns for maintainability.",
            "status": "done",
            "testStrategy": "Write unit tests for each module using Vitest, focusing on individual module functionality and edge cases."
          },
          {
            "id": 4,
            "title": "Integrate Modules and Implement Main Agent Logic",
            "description": "Combine the implemented modules and develop the main Data Extraction Agent logic to handle the complete extraction process.",
            "dependencies": [
              3
            ],
            "details": "Integrate the separate modules into a cohesive Data Extraction Agent. Implement the main logic to orchestrate the extraction process, including handling both single-step and two-step API patterns, managing the overall flow, and ensuring proper error handling and retries at the agent level. Implement field mapping and taxonomy standardization logic.",
            "status": "done",
            "testStrategy": "Develop integration tests using Vitest to verify the correct interaction between modules and end-to-end functionality of the agent."
          },
          {
            "id": 5,
            "title": "Optimize and Finalize Data Extraction Agent",
            "description": "Optimize the Data Extraction Agent's performance, ensure compatibility, and prepare for deployment.",
            "dependencies": [
              4
            ],
            "details": "Perform code optimization and refactoring where necessary. Ensure the agent correctly processes pagination and returns a standardized opportunity data structure. Verify compatibility with both single-step and two-step API patterns. Conduct thorough testing, including edge cases and error scenarios. Prepare documentation for deployment and usage.",
            "status": "done",
            "testStrategy": "Conduct performance testing, compatibility testing with various API patterns, and end-to-end testing with real-world scenarios using Vitest. Review and update all unit and integration tests."
          }
        ]
      },
      {
        "id": 2,
        "title": "Develop Storage Agent",
        "description": "Implement the Storage Agent to handle database storage logic, including duplicate detection, state eligibility mapping, and funding source relationships.",
        "details": "1. Refactor dataProcessorAgent.js (1,049 lines) into modular components\n2. Create separate modules for:\n   - Duplicate detection (use fuzzy matching library like fuzzyset.js v1.0.7)\n   - State eligibility processing\n   - Funding source management\n   - Change detection\n   - Data sanitization\n3. Implement database operations using Prisma ORM v4.15.0\n4. Use PostgreSQL as the database (latest stable version)\n5. Implement material change detection algorithm\n6. Create comprehensive metrics tracking system\n7. Optimize for performance, considering batch operations where appropriate\n8. Implement proper error handling and logging (use Winston v3.9.0 for logging)\n9. Use TypeScript for improved code quality and maintainability",
        "testStrategy": "1. Develop unit tests for each module\n2. Create integration tests for the entire Storage Agent\n3. Test duplicate detection with various similar entries\n4. Validate state eligibility mapping accuracy\n5. Verify funding source relationship management\n6. Test material change detection with different scenarios\n7. Ensure proper metrics tracking and accuracy\n8. Perform performance testing with large datasets",
        "priority": "high",
        "dependencies": [
          1
        ],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": 3,
        "title": "Finalize Analysis Agent",
        "description": "Review, validate, and finalize the implementation of the Analysis Agent, ensuring no functionality loss from v1 detailProcessorAgent.js.",
        "details": "1. Review current implementation (435 lines)\n2. Compare functionality with v1 detailProcessorAgent.js (815 lines)\n3. Implement proper scoring algorithms\n4. Add content enhancement capabilities\n5. Ensure compatibility with filtering system\n6. Update prompts and scoring logic as needed\n7. Implement proper error handling\n8. Optimize for performance and token usage\n9. Use GPT-4 API for advanced language processing tasks\n10. Implement caching mechanism to reduce API calls (use Redis v6.0.0)\n11. Use TypeScript for improved code quality\n12. Implement unit and integration tests",
        "testStrategy": "1. Develop comprehensive unit tests for each module\n2. Create integration tests for the entire Analysis Agent\n3. Compare results with v1 agent to ensure no functionality loss\n4. Test scoring algorithms with various input scenarios\n5. Validate content enhancement capabilities\n6. Verify compatibility with the filtering system\n7. Perform performance and token usage optimization tests\n8. Test error handling with various edge cases",
        "priority": "high",
        "dependencies": [
          1,
          2
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Review and Compare Functionality",
            "description": "Thoroughly review the current v2 implementation and compare its functionality with v1 detailProcessorAgent.js",
            "dependencies": [],
            "details": "1. Analyze the current v2 implementation (435 lines)\n2. Compare with v1 detailProcessorAgent.js (815 lines)\n3. Document differences in functionality\n4. Identify any missing features or improvements\n5. Ensure all key responsibilities are addressed",
            "status": "done",
            "testStrategy": "Create a checklist of v1 features and verify each in v2"
          },
          {
            "id": 2,
            "title": "Implement and Validate Scoring Algorithms",
            "description": "Develop and validate systematic scoring algorithms to rate opportunities accurately",
            "dependencies": [
              1
            ],
            "details": "1. Design scoring criteria based on business requirements\n2. Implement scoring algorithms in JavaScript\n3. Test with sample data from Data Extraction Agent\n4. Calibrate and fine-tune algorithms\n5. Document scoring methodology",
            "status": "done",
            "testStrategy": "Unit tests for individual scoring components and integration tests with sample data"
          },
          {
            "id": 3,
            "title": "Enhance Content and Generate Actionable Summaries",
            "description": "Improve opportunity descriptions and create actionable summaries for sales teams",
            "dependencies": [
              1
            ],
            "details": "1. Implement content enhancement logic\n2. Develop summary generation algorithm\n3. Ensure readability and professionalism\n4. Optimize for decision-making\n5. Integrate with GPT-4 API for advanced language processing\n<info added on 2025-06-21T22:20:00.707Z>\nUpdated actionable summary prompt to use natural guidelines-based approach instead of rigid bullet-point structure. New prompt focuses on funding opportunity overview, eligibility criteria, project types, and company relevance assessment for sales teams. This change removes prescriptive formatting constraints while maintaining sales-focused quality requirements. Enhanced flexibility allows AI to write more naturally and contextually appropriate responses. Next step is to apply similar natural approach to enhanced description section and conduct testing of improvements.\n</info added on 2025-06-21T22:20:00.707Z>\n<info added on 2025-06-21T22:38:39.331Z>\nCompleted enhanced description prompt update with strategic, use-case focused approach. Updated enhanced description to strategic, use-case driven format using new prompt: \"Write a detailed, strategic description of this funding opportunity. Summarize what it is, why it's important, who qualifies, and what kinds of projects are eligible. Then provide 2–3 short use case examples showing how our clients—such as cities, school districts, or state facilities—could take advantage of the opportunity. Focus on narrative clarity and practical insight, not boilerplate language.\" Key improvements include strategic focus versus generic rewrite, practical use case examples for actual client types, anti-jargon directive for clarity, client-specific scenarios for cities/school districts/state facilities, and emphasis on narrative clarity and actionable insights. Enhanced descriptions will now include strategic summaries with concrete examples like school district HVAC upgrades, city building efficiency projects, and state facility solar installations, transforming generic rewrites into actionable sales tools. Both actionable summary and enhanced description prompts now use natural, guidelines-based approaches optimized for sales team effectiveness.\n</info added on 2025-06-21T22:38:39.331Z>",
            "status": "done",
            "testStrategy": "A/B testing of enhanced content against original, peer review of generated summaries"
          },
          {
            "id": 6,
            "title": "Clean Up Analysis Agent Prompt",
            "description": "Clean up and refine the analysis agent prompt by getting guidance on what each portion should accomplish",
            "details": "<info added on 2025-06-18T03:42:25.272Z>\nStarted analysis of current prompt structure. Identified 5 main sections: 1) Business context, 2) Data formatting, 3) Analysis instructions, 4) Scoring criteria, 5) JSON schema. Ready to get guidance on each section for cleanup and refinement.\n</info added on 2025-06-18T03:42:25.272Z>\n<info added on 2025-06-18T03:59:47.612Z>\nApplied business context revision to code. Analyzed current systematic scoring criteria with 5 components: projectTypeMatch (0-3), clientTypeMatch (0-3), categoryMatch (0-2), fundingThreshold (0-1), fundingType (0-1) totaling 10 points. Evaluating whether current weights are appropriate and considering additional criteria including competition level, application complexity, timeline feasibility, and geographic alignment to enhance scoring accuracy.\n</info added on 2025-06-18T03:59:47.612Z>\n<info added on 2025-06-18T04:46:05.809Z>\nApplied business context revision to code. Added 3 new subtasks to Task 3: 3.10) Update taxonomy with target client/project types, 3.11) Add eligible_activities field to DB and Data Extraction Agent, 3.12) Replace categoryMatch with activityMatch in scoring. Created Task 20 for client advantage analysis with database field and Data Extraction Agent integration. Ready to continue with systematic scoring refinement after these data structure updates are complete.\n</info added on 2025-06-18T04:46:05.809Z>\n<info added on 2025-06-21T21:42:59.131Z>\nCompleted systematic scoring refinement with new gating system implementation. Replaced 5-component scoring with streamlined 3-component gating system: clientProjectRelevance (0-6) with gating requirement, fundingAttractiveness (0-3) for financial assessment, and fundingType (0-1) grant bonus. Added dynamic taxonomy imports and gating logic requiring ≥2 for viability, ≥5 for auto-qualification. Updated prompt to reference actual field names and fixed metrics calculation. New system prevents irrelevant high-funding opportunities while auto-passing perfect matches, with 60% weighting on business relevance. System ready for testing or proceeding to schema review.\n</info added on 2025-06-21T21:42:59.131Z>",
            "status": "done",
            "dependencies": [
              1
            ],
            "parentTaskId": 3
          },
          {
            "id": 7,
            "title": "Review and Implement Opportunity Analysis Schema",
            "description": "Review the opportunity analysis schema in anthropic client and implement it in the analysis agent once finalized",
            "details": "<info added on 2025-06-21T23:41:18.096Z>\nCOMPLETED: Fixed Analysis Agent schema alignment and added database fields\n\nMAJOR FIXES COMPLETED:\n1. Database Migration Applied: Added enhanced_description (TEXT) and scoring (JSONB) fields to funding_opportunities table\n2. Schema Alignment Fixed: \n   - Updated opportunityAnalysis schema to match database fields exactly\n   - Renamed scoringExplanation → relevanceReasoning (matches DB field)\n   - Kept scoring object in schema (will map to JSONB in storage)\n   - Updated field descriptions for natural language approach\n3. Analysis Agent Updated: \n   - Now uses centralized schemas.opportunityAnalysis instead of inline schema\n   - Updated error handling to use relevanceReasoning field name\n   - Simplified response processing since LLM returns complete objects\n\nSTORAGE MAPPING PLAN (Next Step):\n- scoring.overallScore → relevance_score (for backward compatibility)\n- scoring object → scoring JSONB field (new)\n- relevanceReasoning → relevance_reasoning (direct match)\n- enhancedDescription → enhanced_description (direct match)\n\nThis ensures Analysis Agent output perfectly matches database structure while preserving full scoring object for analysis.\n</info added on 2025-06-21T23:41:18.096Z>",
            "status": "done",
            "dependencies": [
              6
            ],
            "parentTaskId": 3
          },
          {
            "id": 10,
            "title": "Update Taxonomy with Target Client Types and Project Types",
            "description": "Add target client types, target project types, and preferred activities to the taxonomy file to support enhanced opportunity analysis",
            "details": "",
            "status": "done",
            "dependencies": [],
            "parentTaskId": 3
          },
          {
            "id": 11,
            "title": "Add Eligible Activities Field to Database and Data Extraction",
            "description": "Add 'eligible_activities' field to database schema and update Data Extraction Agent to capture this non-standardized data from LLM analysis",
            "details": "<info added on 2025-06-21T23:03:16.256Z>\nCOMPLETED: Successfully created and applied database migration for new funding opportunity fields\n\nMIGRATION FILE: 20250702000001_add_funding_process_fields.sql APPLIED\n\nVERIFIED DATABASE CHANGES:\n- disbursement_type (TEXT, nullable) - How funding is distributed\n- award_process (TEXT, nullable) - Award selection process  \n- eligible_activities (TEXT[], nullable) - Array of fundable activities\n\nIMPLEMENTATION DETAILS:\n- Added proper column comments for documentation\n- Created performance indexes including GIN index for array field\n- Successfully updated funding_opportunities_with_geography view\n- Fixed column ordering to match existing view structure\n- Migration applied successfully to local database\n\nDATABASE VERIFICATION:\n- Confirmed all 3 new columns exist in funding_opportunities table\n- Confirmed all 3 new columns are available in the view\n- Migration executed without errors\n\nSTATUS: COMPLETE - Ready for Data Extraction Agent updates\nNEXT: Update Data Extraction Agent to populate these fields during opportunity processing\n</info added on 2025-06-21T23:03:16.256Z>",
            "status": "done",
            "dependencies": [
              10
            ],
            "parentTaskId": 3
          },
          {
            "id": 12,
            "title": "Replace Category Match with Activity Match in Scoring",
            "description": "Update systematic scoring to replace 'categoryMatch' with 'activityMatch' using the new eligible_activities data for more precise opportunity scoring",
            "details": "",
            "status": "done",
            "dependencies": [
              11
            ],
            "parentTaskId": 3
          },
          {
            "id": 13,
            "title": "Update Anthropic Client Schema with New Database Fields",
            "description": "Add disbursement_type, award_process, and eligible_activities fields to both dataExtraction and opportunityAnalysis schemas in the Anthropic client to support the new database structure",
            "details": "",
            "status": "done",
            "dependencies": [
              10
            ],
            "parentTaskId": 3
          }
        ]
      },
      {
        "id": 4,
        "title": "Conduct Individual Agent Testing",
        "description": "Perform thorough testing of each v2 agent using real data, ensuring performance metrics meet or exceed v1 benchmarks.",
        "details": "1. Update existing test scripts in scripts/test/\n2. Add comprehensive error scenario testing\n3. Implement performance benchmarking (use Benchmark.js v2.1.4)\n4. Create data quality validation tests\n5. Test with multiple API source types\n6. Use real API responses for testing\n7. Implement mock servers for API testing (use Mock Service Worker v1.2.1)\n8. Create detailed test reports\n9. Use Jest v29.5.0 for test runner and assertion library\n10. Implement code coverage reporting (use Istanbul v0.4.5)\n11. Automate testing process using GitHub Actions",
        "testStrategy": "1. Run individual tests for each agent\n2. Perform integration tests with real API data\n3. Conduct performance benchmarking against v1 agents\n4. Validate error handling with various scenarios\n5. Verify data quality and accuracy\n6. Test with different API source types\n7. Ensure test coverage is above 80%\n8. Review and analyze test reports",
        "priority": "high",
        "dependencies": [
          1,
          2,
          3
        ],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": 5,
        "title": "Rewrite Unit Tests",
        "description": "Update all unit tests to match v2 implementations, maintaining 80%+ coverage and properly covering edge cases.",
        "details": "1. Update existing unit tests in app/lib/agents-v2/tests/\n2. Add new tests for completed agents\n3. Implement proper mocking for external dependencies (use Jest mocking capabilities)\n4. Create test data fixtures that match real API responses\n5. Ensure test coverage maintains 80%+ (use Istanbul for coverage reporting)\n6. Implement snapshot testing for complex objects\n7. Use TypeScript for writing tests\n8. Implement test helpers and utilities for common testing scenarios\n9. Use faker.js v8.0.2 for generating test data\n10. Implement property-based testing for edge cases (use fast-check v3.10.0)",
        "testStrategy": "1. Review and update each test suite\n2. Verify that all tests pass after updates\n3. Check test coverage and add tests where needed\n4. Validate mocking of external dependencies\n5. Ensure edge cases are properly covered\n6. Perform code review of test updates\n7. Run the entire test suite and verify results",
        "priority": "medium",
        "dependencies": [
          1,
          2,
          3,
          4
        ],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": 6,
        "title": "Implement End-to-End Pipeline Testing",
        "description": "Develop and execute comprehensive end-to-end tests for the complete v2 pipeline, ensuring proper orchestration and error handling.",
        "details": "1. Implement end-to-end tests covering the entire pipeline: Source → Data Extraction → Analysis → Filter → Storage\n2. Validate process coordinator orchestration\n3. Test Supabase Edge Function integration\n4. Verify run tracking and metrics collection\n5. Implement comprehensive error handling tests\n6. Use Cypress v12.14.0 for end-to-end testing\n7. Implement test scenarios with various data sources and configurations\n8. Create detailed logging for each stage of the pipeline\n9. Implement performance monitoring for the entire pipeline\n10. Use Docker for creating isolated test environments\n11. Implement parallel test execution for faster results",
        "testStrategy": "1. Design test scenarios covering various use cases\n2. Execute end-to-end tests in a staging environment\n3. Validate correct data flow through the entire pipeline\n4. Test error handling and recovery mechanisms\n5. Verify metrics collection and accuracy\n6. Perform load testing to ensure scalability\n7. Validate Supabase Edge Function integration\n8. Review logs and performance metrics",
        "priority": "high",
        "dependencies": [
          1,
          2,
          3,
          4,
          5
        ],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": 7,
        "title": "Conduct Performance Validation",
        "description": "Perform thorough performance testing to ensure v2 meets or exceeds performance targets, including processing time, memory usage, and token consumption.",
        "details": "1. Implement performance monitoring tools (use Prometheus v2.40.0 and Grafana v9.5.0)\n2. Compare v1 vs v2 processing times\n3. Monitor memory and resource usage (use node-memwatch v2.0.0)\n4. Validate timeout resolution in Supabase Edge Functions\n5. Document performance improvements\n6. Use Apache JMeter v5.5 for load testing\n7. Implement custom performance benchmarks\n8. Use Datadog v0.36.0 for comprehensive system monitoring\n9. Optimize database queries and indexing\n10. Implement caching strategies where appropriate (use Redis v6.0.0)\n11. Profile code to identify and resolve performance bottlenecks (use Node.js built-in profiler)",
        "testStrategy": "1. Execute performance tests in a production-like environment\n2. Compare v1 and v2 performance metrics\n3. Validate that processing time is 60-80% faster than v1\n4. Verify memory usage reduction by 70%\n5. Confirm token consumption reduction by 15-25%\n6. Test for timeout issues in Supabase Edge Functions\n7. Perform load testing to ensure scalability\n8. Analyze and document performance results",
        "priority": "high",
        "dependencies": [
          6
        ],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": 8,
        "title": "Implement Feature Flag System",
        "description": "Develop a feature flag system to toggle between v1 and v2 processing, with granular control and traffic percentage routing.",
        "details": "1. Implement feature flag configuration (use LaunchDarkly v2.25.1)\n2. Create routing service for v1/v2 selection\n3. Add environment variable controls\n4. Build admin interface for toggle management (use React v18.2.0 and Material-UI v5.13.0)\n5. Implement A/B testing infrastructure\n6. Use Redis for distributed caching of feature flags\n7. Implement logging for feature flag changes\n8. Create dashboard for monitoring feature flag usage\n9. Implement gradual rollout capabilities\n10. Ensure proper error handling for flag-related issues\n11. Use TypeScript for type-safe feature flag implementation",
        "testStrategy": "1. Test feature flag functionality in various scenarios\n2. Verify correct routing between v1 and v2\n3. Validate admin controls for toggle management\n4. Test gradual traffic routing capabilities\n5. Ensure proper handling of edge cases\n6. Verify logging and monitoring of feature flag usage\n7. Perform security testing on admin interface",
        "priority": "high",
        "dependencies": [
          6,
          7
        ],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": 9,
        "title": "Develop Migration Strategy",
        "description": "Implement a migration strategy for gradual transition from v1 to v2, including comparison tracking and rollback capability.",
        "details": "1. Implement Strangler Fig pattern as outlined in the architecture guide\n2. Start with 5% traffic routing to v2\n3. Implement comparison logging between v1 and v2 results\n4. Monitor performance and accuracy metrics\n5. Implement automated rollback triggers\n6. Create dashboard for migration progress monitoring\n7. Implement database schema migrations (use Prisma Migrate)\n8. Develop data consistency checks between v1 and v2\n9. Create detailed rollback procedures\n10. Implement blue-green deployment strategy\n11. Use Terraform v1.5.0 for infrastructure as code",
        "testStrategy": "1. Test gradual traffic routing mechanism\n2. Validate comparison logging accuracy\n3. Verify automated rollback functionality\n4. Test data consistency between v1 and v2\n5. Perform mock migration scenarios\n6. Validate monitoring and alerting systems\n7. Conduct full rollback test\n8. Review migration dashboard effectiveness",
        "priority": "high",
        "dependencies": [
          8
        ],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": 10,
        "title": "Implement Funding-Only Dashboard",
        "description": "Develop a new dashboard focused solely on funding opportunities, removing legislation-related features and preparing for user management integration.",
        "details": "1. Audit current dashboard for legislation references\n2. Remove legislation-specific components\n3. Update navigation and UI elements\n4. Implement enhanced funding opportunity views\n5. Prepare user context integration points\n6. Use React v18.2.0 for frontend development\n7. Implement state management with Redux Toolkit v1.9.5\n8. Use Material-UI v5.13.0 for consistent UI components\n9. Implement responsive design for mobile compatibility\n10. Use React Query v3.39.3 for efficient data fetching\n11. Implement code-splitting for improved performance\n12. Use Storybook v7.0.20 for component development and documentation",
        "testStrategy": "1. Perform UI/UX testing for the new dashboard\n2. Validate removal of all legislation-related features\n3. Test responsiveness on various devices\n4. Conduct user acceptance testing\n5. Verify data accuracy in funding opportunity displays\n6. Test search and filtering capabilities\n7. Ensure accessibility compliance (WCAG 2.1)\n8. Perform cross-browser testing",
        "priority": "medium",
        "dependencies": [
          9
        ],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": 11,
        "title": "Enhance Dashboard Performance and UX",
        "description": "Implement performance improvements and UX enhancements for the funding-only dashboard, including faster load times and improved mobile responsiveness.",
        "details": "1. Implement lazy loading for opportunity lists (use react-window v1.8.9)\n2. Optimize database queries (use database indexing and query optimization)\n3. Add proper caching strategies (implement Redis caching)\n4. Enhance mobile interface (use responsive design principles)\n5. Implement accessibility standards (follow WCAG 2.1 guidelines)\n6. Use Next.js v13.4.0 for server-side rendering and improved performance\n7. Implement service workers for offline capabilities\n8. Use Web Vitals library to measure and optimize Core Web Vitals\n9. Implement skeleton screens for perceived performance improvement\n10. Use React Suspense for code-splitting and lazy loading\n11. Optimize images using next/image component\n12. Implement virtualized lists for large datasets",
        "testStrategy": "1. Conduct performance testing using Lighthouse\n2. Measure and validate improvements in page load times\n3. Test mobile responsiveness on various devices\n4. Perform accessibility audit using axe-core\n5. Validate search and filtering performance improvements\n6. Test offline capabilities\n7. Conduct user testing for UX improvements\n8. Verify Core Web Vitals metrics",
        "priority": "medium",
        "dependencies": [
          10
        ],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": 12,
        "title": "Develop V2 Admin Run Processing Interface",
        "description": "Create an admin interface for managing v2 processing runs, including run triggering, monitoring, and error handling.",
        "details": "1. Build admin interface for v2 run management (use React v18.2.0 and Material-UI v5.13.0)\n2. Implement run scheduling capabilities (use node-cron v3.0.2)\n3. Add comprehensive monitoring dashboard (use Grafana v9.5.0)\n4. Create error investigation tools\n5. Build source configuration interface\n6. Implement real-time updates using WebSockets (socket.io v4.6.0)\n7. Use Redux Toolkit v1.9.5 for state management\n8. Implement role-based access control\n9. Create detailed logging system (use Winston v3.9.0)\n10. Implement audit trail for admin actions\n11. Use Formik v2.4.0 for form handling and validation\n12. Implement dark mode for better UX",
        "testStrategy": "1. Perform unit and integration tests for admin interface\n2. Test run scheduling functionality\n3. Validate monitoring dashboard accuracy\n4. Test error handling and investigation tools\n5. Verify source configuration management\n6. Conduct user acceptance testing with admin users\n7. Test real-time update functionality\n8. Verify role-based access control",
        "priority": "high",
        "dependencies": [
          9,
          11
        ],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": 13,
        "title": "Implement Source Management V2",
        "description": "Develop an enhanced source management system with CRUD operations, API configuration management, and performance monitoring.",
        "details": "1. Build source management interface (use React v18.2.0 and Material-UI v5.13.0)\n2. Implement configuration validation\n3. Add API testing capabilities (use Postman API v10.14.0 for testing)\n4. Create performance monitoring tools (use Prometheus v2.40.0)\n5. Build bulk operation workflows\n6. Implement version control for source configurations (use Git-like versioning)\n7. Create visual API response mapper\n8. Implement OAuth 2.0 for secure API connections\n9. Use TypeScript for type-safe development\n10. Implement database migrations for schema changes (use Prisma Migrate)\n11. Create comprehensive documentation for each source type\n12. Implement alerting system for source issues (use Alertmanager v0.25.0)",
        "testStrategy": "1. Perform CRUD operation testing for sources\n2. Validate API configuration management\n3. Test performance monitoring accuracy\n4. Verify bulk operation functionality\n5. Test version control for configurations\n6. Validate OAuth 2.0 implementation\n7. Conduct user acceptance testing\n8. Test alerting system for various scenarios",
        "priority": "high",
        "dependencies": [
          12
        ],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": 14,
        "title": "Develop Lightweight User Identification System",
        "description": "Implement a simple user identification system with pre-populated user database, client-side token storage, and admin user management.",
        "details": "1. Implement user identification system as per specifications\n2. Create user selection interface (use React v18.2.0 and Material-UI v5.13.0)\n3. Build token management system (use JWT for token generation)\n4. Add admin user management panel\n5. Implement role-based permissions\n6. Use localStorage for client-side token storage\n7. Implement server-side session management (use express-session v1.17.3)\n8. Create user database schema and migrations\n9. Implement user activity logging\n10. Use bcrypt v5.1.0 for password hashing (for admin users)\n11. Implement GDPR compliance measures\n12. Create user onboarding workflow",
        "testStrategy": "1. Test user selection and identification process\n2. Validate token management and storage\n3. Verify admin user management functionality\n4. Test role-based permissions\n5. Conduct security testing for token handling\n6. Verify user activity logging\n7. Test GDPR compliance measures\n8. Perform user acceptance testing",
        "priority": "high",
        "dependencies": [
          11,
          13
        ],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": 15,
        "title": "Implement User Activity Tracking",
        "description": "Develop a system to track user interactions with opportunities, including favorite actions and status changes.",
        "details": "1. Create user actions tracking system\n2. Build activity logging infrastructure (use Winston v3.9.0 for logging)\n3. Implement user-specific data views\n4. Add analytics and reporting capabilities (use Chart.js v4.3.0 for visualizations)\n5. Create user activity dashboards\n6. Implement real-time activity updates (use WebSockets with socket.io v4.6.0)\n7. Use Redis v6.0.0 for caching frequently accessed user data\n8. Implement data aggregation for analytics (use Apache Spark v3.4.0)\n9. Create export functionality for activity reports\n10. Implement privacy controls for user data\n11. Use Elasticsearch v8.8.0 for efficient activity search\n12. Implement activity notifications system",
        "testStrategy": "1. Test user interaction tracking accuracy\n2. Validate activity logging system\n3. Verify user-specific data view functionality\n4. Test analytics and reporting features\n5. Validate real-time update functionality\n6. Perform load testing on activity tracking system\n7. Test data privacy controls\n8. Verify activity search functionality",
        "priority": "medium",
        "dependencies": [
          14
        ],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": 16,
        "title": "Develop Three-Status System",
        "description": "Implement a three-status system (None/Hot/Of Interest) for opportunity management with admin-only Hot status designation.",
        "details": "1. Implement database schema for status management\n2. Create status change logic and validation\n3. Build status tracking and logging system\n4. Add UI components for status display (use React v18.2.0 and Material-UI v5.13.0)\n5. Implement status-based filtering\n6. Create admin interface for Hot status designation\n7. Implement user favoriting system for Of Interest status\n8. Use Redux Toolkit v1.9.5 for state management\n9. Implement WebSocket (socket.io v4.6.0) for real-time status updates\n10. Create database indexes for efficient status-based queries\n11. Implement status change notifications\n12. Develop status analytics dashboard",
        "testStrategy": "1. Test status change functionality for all user types\n2. Validate admin-only Hot status designation\n3. Verify user favoriting system\n4. Test status-based filtering accuracy\n5. Validate real-time status updates\n6. Perform database query optimization tests\n7. Test notification system for status changes\n8. Verify status analytics accuracy",
        "priority": "high",
        "dependencies": [
          14,
          15
        ],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": 17,
        "title": "Create Hot Board and Of Interest Board",
        "description": "Develop Hot Board as the primary sales interface and Of Interest Board, combining Hot and Of Interest opportunities with visual distinctions.",
        "details": "1. Build Hot Board interface (use React v18.2.0 and Material-UI v5.13.0)\n2. Create Of Interest Board\n3. Implement status-based filtering\n4. Add action buttons for status changes\n5. Build team collaboration features\n6. Implement drag-and-drop functionality (use react-beautiful-dnd v13.1.1)\n7. Create visual distinctions between status types\n8. Implement real-time updates (use WebSockets with socket.io v4.6.0)\n9. Add sorting and advanced filtering options\n10. Implement board customization features\n11. Create board sharing functionality\n12. Develop board analytics and reporting",
        "testStrategy": "1. Test Hot Board and Of Interest Board functionality\n2. Validate status-based filtering and sorting\n3. Verify status change actions\n4. Test team collaboration features\n5. Validate drag-and-drop functionality\n6. Verify real-time update system\n7. Test board customization features\n8. Conduct user acceptance testing with sales team",
        "priority": "high",
        "dependencies": [
          16
        ],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": 18,
        "title": "Implement Notification System",
        "description": "Develop an email notification system for Hot opportunities with territory-based routing, deadline reminders, and configurable preferences.",
        "details": "1. Implement email notification system (use Nodemailer v6.9.0)\n2. Create notification templates (use Handlebars v4.7.0 for templating)\n3. Build territory-based routing logic\n4. Add deadline reminder functionality\n5. Implement status change notifications\n6. Create notification preference management\n7. Implement notification scheduling (use node-cron v3.0.2)\n8. Use Redis v6.0.0 for notification queue management\n9. Implement rate limiting for notifications\n10. Create notification analytics dashboard\n11. Implement multi-channel notifications (email, in-app, SMS)\n12. Develop A/B testing for notification content",
        "testStrategy": "1. Test email notification delivery and content\n2. Validate territory-based routing accuracy\n3. Verify deadline reminder functionality\n4. Test status change notification system\n5. Validate user preference management\n6. Perform load testing on notification system\n7. Test rate limiting functionality\n8. Verify multi-channel notification delivery",
        "priority": "medium",
        "dependencies": [
          16,
          17
        ],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": 19,
        "title": "Create Real Data Test Script for Analysis Agent",
        "description": "Update the existing test script to validate the Analysis Agent with real funding source data, focusing on compatibility with the new implementation, scoring system updates, and pipeline integration verification.",
        "status": "done",
        "dependencies": [
          3
        ],
        "priority": "high",
        "details": "1. Update the existing 03-test-analysis-agent.js script to work with the new Analysis Agent implementation\n2. Modify validation checks to match the new scoring system and schema changes\n3. Update test data handling to accommodate new scoring fields and output format\n4. Fix any compatibility issues with the updated Analysis Agent interface\n5. Run comprehensive tests using real funding source data samples\n6. Verify opportunity enhancement functionality works correctly with new implementation\n7. Test scoring accuracy and consistency with the updated scoring system\n8. Validate analysis metrics generation produces expected outputs\n9. Verify pipeline compatibility and seamless integration with existing workflow\n10. Document any issues found during testing and their resolutions\n11. Create test result summary report highlighting key findings\n12. Validate data anonymization still works effectively with new data structures\n13. Test edge cases specific to the new scoring system implementation\n14. Verify error handling works properly with updated Analysis Agent\n15. Confirm test script runs successfully end-to-end with real data",
        "testStrategy": "1. Execute updated test script against Analysis Agent with real funding source data samples\n2. Validate opportunity enhancement accuracy with new implementation\n3. Verify scoring system updates work correctly and produce consistent results\n4. Test analysis metrics generation matches expected new output format\n5. Validate pipeline compatibility and workflow integration\n6. Verify all validation checks pass with new scoring fields and schema\n7. Test edge cases and error handling with updated Analysis Agent\n8. Document test results and any compatibility issues discovered\n9. Confirm data anonymization effectiveness with new data structures\n10. Validate end-to-end functionality from data input to final output\n11. Compare results against expected behavior with new scoring system\n12. Verify test script stability and reliability with real data processing",
        "subtasks": [
          {
            "id": 1,
            "title": "Update Test Script Schema and Validation",
            "description": "Update the existing 03-test-analysis-agent.js script to match the new Analysis Agent implementation - fix scoring field names, update validation checks, and modify display functions to work with the new schema",
            "details": "1. Update scoring field references from old system (projectTypeMatch, clientTypeMatch, categoryMatch, scoringExplanation) to new system (clientProjectRelevance, fundingAttractiveness, fundingType, relevanceReasoning)\n2. Fix validation checks to match new 3-component scoring system (0-6, 0-3, 0-1) \n3. Update displayEnhancedOpportunity function to show new scoring breakdown\n4. Modify validation logic to check for new required fields\n5. Update score distribution calculations to match new ranges\n6. Fix any imports or function calls that changed in the new Analysis Agent\n<info added on 2025-06-22T00:51:29.363Z>\nPROGRESS UPDATE - Schema and Validation Updates Complete:\n\nSuccessfully updated all scoring field references and validation logic to match new Analysis Agent system. Key changes implemented:\n\n- Replaced old 4-field scoring system (projectTypeMatch, clientTypeMatch, categoryMatch, scoringExplanation) with new 3-component system (clientProjectRelevance 0-6, fundingAttractiveness 0-3, fundingType 0-1, relevanceReasoning)\n- Updated displayEnhancedOpportunity function to show new scoring breakdown format\n- Modified all validation checks to enforce new field requirements and score ranges\n- Adjusted score distribution display ranges: High 7-10, Medium 4-6, Low 0-3\n- Fixed Analysis Agent import path for schemas\n- Test script now loads and runs successfully with updated schema\n\nStatus: Schema updates complete and validated. Test script ready for execution pending environment variable configuration (ANTHROPIC_API_KEY, Supabase credentials) needed for subtask 19.2.\n</info added on 2025-06-22T00:51:29.363Z>",
            "status": "done",
            "dependencies": [],
            "parentTaskId": 19
          },
          {
            "id": 2,
            "title": "Run and Debug Real Data Tests",
            "description": "Execute the updated test script with real funding data, identify and fix any issues, and verify that the Analysis Agent works correctly with the new implementation",
            "details": "1. Run the updated test script against real funding opportunities\n2. Debug any errors or compatibility issues that arise\n3. Verify that enhanced descriptions and actionable summaries are generated correctly\n4. Check that scoring calculations work as expected with the new system\n5. Validate that analysis metrics are calculated properly\n6. Ensure all validation checks pass with real data\n7. Test edge cases and error handling scenarios\n8. Verify performance is acceptable with real data volumes\n<info added on 2025-06-22T04:36:44.821Z>\nCOMPLETED: Successfully ran and debugged real data tests\n\nFIXED ENVIRONMENT ISSUE: \n- Root cause: Test script was using wrong relative path for .env.local file\n- Solution: Changed from '.env.local' to '../../.env.local' in dotenv.config()\n- Result: ANTHROPIC_API_KEY now loads correctly (108 characters)\n\nSUCCESSFUL TEST EXECUTION:\n- Both California and Grants.gov sources analyzed successfully  \n- All validation checks PASSED (12/12)\n- Enhanced descriptions and actionable summaries generated correctly\n- New scoring system working: clientProjectRelevance (0-6), fundingAttractiveness (0-3), fundingType (0-1)\n- Analysis metrics calculated properly\n- Performance within acceptable range (~20 seconds per batch)\n\nVALIDATED NEW ANALYSIS AGENT:\n- Schema updates working correctly\n- Field names all aligned (relevanceReasoning vs scoringExplanation)\n- Score distributions calculated with new ranges (High: 7-10, Medium: 4-6, Low: 0-3)\n- Pipeline integration confirmed - ready for Stage 4\n\nSTATUS: Real data testing complete and successful. Analysis Agent v2 fully validated with production-ready performance.\n</info added on 2025-06-22T04:36:44.821Z>",
            "status": "done",
            "dependencies": [],
            "parentTaskId": 19
          }
        ]
      },
      {
        "id": 20,
        "title": "Implement Client Advantage Analysis for Opportunities",
        "description": "Develop a comprehensive client advantage analysis system that evaluates opportunities based on client-specific criteria, competitive positioning, and success probability scoring. This system will integrate with the Data Extraction Agent to populate a 'client_advantage_analysis' database field with specific advantage insights for preferred clients.",
        "status": "pending",
        "dependencies": [
          1
        ],
        "priority": "high",
        "details": "1. Integrate with Data Extraction Agent (Task 1) to add 'client_advantage_analysis' field to opportunities database\n2. Design optimal data storage structure for client advantage analysis that supports:\n   - Structured JSON format for flexible advantage categories\n   - Efficient querying for opportunity detail page display\n   - Scalable storage for multiple client advantage scenarios\n3. Implement data processing logic within Data Extraction Agent to:\n   - Analyze opportunity requirements against preferred client capabilities\n   - Generate specific use case examples for client advantages\n   - Extract competitive positioning insights from opportunity data\n4. Create client advantage scoring algorithm that evaluates opportunities based on:\n   - Client's historical success rates with similar opportunities\n   - Geographic proximity and regional presence\n   - Industry expertise alignment\n   - Resource capacity and capability matching\n   - Competitive landscape analysis\n5. Implement opportunity scoring service (use TypeScript for type safety)\n6. Build client profile management system to store:\n   - Company capabilities and expertise areas\n   - Geographic coverage and regional strengths\n   - Past performance metrics and success rates\n   - Resource availability and capacity\n7. Create competitive analysis module that:\n   - Identifies potential competitors for each opportunity\n   - Analyzes market positioning and competitive advantages\n   - Calculates win probability based on competitive factors\n8. Implement machine learning model for predictive scoring (use TensorFlow.js v4.8.0)\n9. Build advantage visualization dashboard (use D3.js v7.8.0 for data visualization)\n10. Create recommendation engine that suggests:\n    - High-advantage opportunities for specific clients\n    - Strategic partnerships or teaming arrangements\n    - Capability gaps to address for better positioning\n11. Implement real-time scoring updates when opportunity details change\n12. Use Redis v6.0.0 for caching calculated scores and analysis results\n13. Create API endpoints for advantage analysis integration with existing dashboard\n14. Implement batch processing for large-scale opportunity analysis\n15. Add configurable scoring weights and criteria customization\n16. Create detailed reporting and analytics for advantage trends\n17. Implement data export capabilities for client presentations\n18. Optimize database queries for opportunity detail page performance\n19. Create specific use case generation logic for client advantage examples",
        "testStrategy": "1. Test integration with Data Extraction Agent for 'client_advantage_analysis' field population\n2. Validate database structure performance for opportunity detail page queries\n3. Test advantage scoring algorithm accuracy with historical opportunity data\n4. Validate client profile management CRUD operations\n5. Test competitive analysis accuracy against known market data\n6. Verify machine learning model predictions with test datasets\n7. Test real-time scoring updates when opportunity data changes\n8. Validate API endpoint functionality and response formats\n9. Perform load testing on batch processing capabilities\n10. Test dashboard visualization accuracy and performance\n11. Verify recommendation engine suggestions against business logic\n12. Test data export functionality and format accuracy\n13. Conduct user acceptance testing with sample client profiles\n14. Validate caching performance and data consistency\n15. Test scoring customization and weight adjustment features\n16. Perform integration testing with existing opportunity management system\n17. Test opportunity detail page load times with client advantage data\n18. Validate specific use case generation accuracy and relevance",
        "subtasks": []
      },
      {
        "id": 21,
        "title": "Update Filter Function to use New Gating System Scoring",
        "description": "Refactor the filter function to use the new gating system scoring structure from the Analysis Agent, replacing the old scoring system with clientProjectRelevance, fundingAttractiveness, and fundingType metrics.",
        "details": "1. Update filter function to use new scoring structure:\n   - Replace projectTypeMatch/clientTypeMatch with clientProjectRelevance (0-6 scale)\n   - Replace overallScore with fundingAttractiveness (0-3 scale)\n   - Update fundingType to use 0-1 binary scoring instead of string matching\n   - Remove fundingThreshold parameter as it's replaced by gating logic\n\n2. Implement new gating logic:\n   - Primary gate: clientProjectRelevance ≥ 2 (minimum to pass filter)\n   - Auto-qualification gate: clientProjectRelevance ≥ 5 (bypasses other checks)\n   - Secondary filtering based on fundingAttractiveness and fundingType scores\n\n3. Update filter function signature and parameters:\n   - Modify function to accept new scoring object structure\n   - Update TypeScript interfaces for new scoring system\n   - Ensure backward compatibility during transition period\n\n4. Implement scoring validation:\n   - Add input validation for score ranges (0-6, 0-3, 0-1)\n   - Handle edge cases where scores are missing or invalid\n   - Add logging for filtering decisions and score thresholds\n\n5. Update filtering logic flow:\n   - First check: clientProjectRelevance ≥ 2 (fail if below)\n   - Auto-qualify if clientProjectRelevance ≥ 5\n   - For scores 2-4, apply additional filtering based on fundingAttractiveness and fundingType\n   - Implement weighted scoring for edge cases\n\n6. Performance considerations:\n   - Optimize filtering algorithm for large datasets\n   - Implement early exit conditions for failed gates\n   - Add caching for frequently used filter configurations\n\n7. Integration updates:\n   - Update all calling functions to pass new scoring structure\n   - Modify database queries to work with new scoring fields\n   - Update API responses to reflect new filtering results\n<info added on 2025-06-22T05:20:49.589Z>\n8. COMPLETED IMPLEMENTATION:\n   - Filter function successfully updated to use new gating system scoring structure\n   - Simplified logic removes unnecessary complexity (strict/lenient filtering, grant preference, quality requirements)\n   - Implements clean 3-stage filtering process: Primary Gate (clientProjectRelevance ≥ 2) → Auto-Qualification (clientProjectRelevance ≥ 5) → Secondary Filtering (status check + fundingAttractiveness ≥ 1)\n   - Enhanced logging and metrics tracking added for filtering decisions\n\n9. Test suite updates completed:\n   - Updated existing test suite to align with new scoring system\n   - Removed tests for deprecated functionality\n   - Added comprehensive validation for new gating logic\n   - Created new real data integration test script (scripts/test/04-test-filter-function.js) that uses actual Analysis Agent output from Stage 3\n   - Integration test validates filtering with multiple configurations using real analyzed opportunities\n\n10. Ready for validation:\n    - All changes implemented and tested\n    - Filter function now fully integrated with new gating system\n    - Can be tested using: node scripts/test/04-test-filter-function.js\n</info added on 2025-06-22T05:20:49.589Z>",
        "testStrategy": "1. Unit Testing:\n   - Test filter function with various score combinations across all ranges\n   - Verify gating logic: scores below 2 are rejected, scores ≥5 auto-qualify\n   - Test edge cases: missing scores, invalid ranges, null values\n   - Validate backward compatibility with old scoring system during transition\n\n2. Integration Testing:\n   - Test filter function integration with Analysis Agent output\n   - Verify correct filtering results with real opportunity data\n   - Test performance with large datasets (1000+ opportunities)\n   - Validate filtering accuracy against expected business rules\n\n3. Scoring Validation Tests:\n   - Test clientProjectRelevance scoring: 0-6 range validation\n   - Test fundingAttractiveness scoring: 0-3 range validation  \n   - Test fundingType binary scoring: 0-1 validation\n   - Verify proper error handling for out-of-range scores\n\n4. Gating Logic Tests:\n   - Test primary gate: opportunities with clientProjectRelevance < 2 are filtered out\n   - Test auto-qualification: opportunities with clientProjectRelevance ≥ 5 pass regardless of other scores\n   - Test intermediate scoring: opportunities with scores 2-4 are evaluated based on additional criteria\n   - Verify weighted scoring calculations for edge cases\n\n5. Performance Testing:\n   - Benchmark filtering speed with new vs old scoring system\n   - Test memory usage with large opportunity datasets\n   - Validate early exit conditions reduce processing time\n   - Test caching effectiveness for repeated filter operations\n\n6. End-to-End Testing:\n   - Test complete pipeline: Analysis Agent → Filter Function → Results\n   - Verify filtered results match expected business outcomes\n   - Test with various opportunity types and funding sources\n   - Validate logging and metrics collection for filtering decisions",
        "status": "done",
        "dependencies": [
          3
        ],
        "priority": "high",
        "subtasks": []
      },
      {
        "id": 22,
        "title": "Create Comprehensive Test Script for Updated Filter Function with New Scoring System",
        "description": "Develop a comprehensive test script that validates the updated Filter Function using real data and the new dual scoring system (clientRelevance + projectRelevance), building on the existing test script in scripts/test/03-test-analysis-agent.js to ensure proper filtering based on the new scoring thresholds, updated funding attractiveness criteria, and grant preferences.",
        "status": "deferred",
        "dependencies": [
          19,
          21
        ],
        "priority": "high",
        "details": "1. Extend existing test script at scripts/test/03-test-analysis-agent.js to include comprehensive filter function testing with new dual scoring system\n2. Create test data sets covering all new scoring scenarios:\n   - clientRelevance scores: 0-3 range based on eligible applicants\n   - projectRelevance scores: 0-3 range based on eligible activities\n   - Combined scoring scenarios with maximum 10 points total (6 from relevance + up to 4 from other factors)\n   - Updated fundingAttractiveness thresholds: 50M+/5M+ exceptional, 25M+/2M+ strong, 10M+/1M+ moderate\n   - fundingType binary scores: 0 and 1 values\n   - Grant preference matching scenarios with preferred activities weighting\n\n3. Implement new gating system validation tests:\n   - Test filtering logic with new scoring ranges (0-10 total vs previous system)\n   - Validate research opportunity filtering with low projectRelevance scores\n   - Test preferred activities weighting in filtering decisions\n   - Verify proper handling of clientRelevance vs projectRelevance balance\n\n4. Create edge case test scenarios:\n   - Missing or null scoring values for new dual scoring fields\n   - Invalid score ranges for clientRelevance and projectRelevance (negative numbers, values exceeding 3)\n   - Transition scenarios from old clientProjectRelevance to new dual scoring\n   - Malformed opportunity data structures with new scoring fields\n   - Empty result sets and boundary conditions with new thresholds\n\n5. Implement performance validation:\n   - Test with large datasets (1000+ opportunities) to validate filtering performance with new scoring calculations\n   - Memory usage monitoring during dual scoring filtering operations\n   - Execution time benchmarking comparing old vs new scoring system performance\n\n6. Build real data integration:\n   - Use anonymized real funding opportunity data from Task 19's data collection system\n   - Validate filtering accuracy against known opportunities using new dual scoring criteria\n   - Test research opportunity filtering effectiveness with projectRelevance scores\n   - Test with diverse funding types, agencies, and opportunity structures under new scoring\n\n7. Create comprehensive assertion framework:\n   - Validate filtered result counts match expected outcomes with new scoring thresholds\n   - Verify dual score-based sorting and ranking (clientRelevance + projectRelevance)\n   - Confirm research opportunities are properly filtered with low projectRelevance\n   - Test preferred activities weighting impact on filtering results\n   - Validate new funding attractiveness threshold application\n\n8. Implement regression testing:\n   - Compare results between old clientProjectRelevance and new dual scoring system\n   - Ensure smooth transition from previous filter function behavior\n   - Validate that new scoring improvements maintain filtering quality\n   - Test backward compatibility during scoring system transition\n\n9. Use Jest v29.5.0 testing framework with custom matchers for dual scoring validation\n10. Implement test data factories using faker.js v8.0.2 for generating varied dual scoring test scenarios",
        "testStrategy": "1. Execute comprehensive test suite covering all dual scoring combinations (clientRelevance 0-3, projectRelevance 0-3) and new gating scenarios\n2. Validate new filtering logic with updated scoring ranges (maximum 10 points total)\n3. Test research opportunity filtering: confirm opportunities with low projectRelevance scores are appropriately filtered\n4. Verify preferred activities weighting functionality in filtering decisions\n5. Test new funding attractiveness thresholds (50M+/5M+ exceptional, 25M+/2M+ strong, 10M+/1M+ moderate)\n6. Run edge case tests with malformed dual scoring data, missing clientRelevance/projectRelevance values, and boundary conditions\n7. Execute performance tests comparing old vs new scoring system with datasets of varying sizes (100, 500, 1000+ opportunities)\n8. Validate filtering accuracy using real anonymized funding data from Task 19 with new dual scoring criteria\n9. Conduct regression testing to ensure smooth transition from clientProjectRelevance to dual scoring system\n10. Verify test coverage reaches 95%+ for all new dual scoring filter function code paths\n11. Run load testing to ensure filter performance under concurrent usage with new scoring calculations\n12. Validate memory usage remains within acceptable limits during dual scoring processing of large datasets\n13. Execute integration tests with the complete Analysis Agent pipeline using new dual scoring system\n14. Test filtering consistency between old and new scoring systems during transition period",
        "subtasks": []
      },
      {
        "id": 23,
        "title": "Review Analysis Metrics Implementation",
        "description": "Examine the analysis metrics calculation in the Analysis Agent to determine if it's an alternate implementation of what the run manager was doing in v1 and should be doing in v2. Compare functionality and determine if consolidation or coordination is needed.",
        "details": "1. Conduct comprehensive code review of Analysis Agent metrics implementation (examine current 435-line implementation)\n2. Analyze v1 run manager metrics functionality and identify overlapping responsibilities\n3. Document current metrics being calculated by Analysis Agent:\n   - Opportunity scoring metrics\n   - Content enhancement statistics\n   - Processing performance metrics\n   - Token usage tracking\n   - API call frequency and success rates\n4. Compare with v1 run manager metrics to identify:\n   - Duplicate functionality\n   - Missing capabilities\n   - Inconsistent calculations\n   - Performance implications\n5. Create detailed comparison matrix showing:\n   - Metric types calculated by each system\n   - Data sources used\n   - Calculation methodologies\n   - Storage and retrieval patterns\n6. Evaluate architectural implications:\n   - Single responsibility principle adherence\n   - Data flow efficiency\n   - Maintenance complexity\n   - Testing coverage gaps\n7. Develop recommendations for:\n   - Consolidation opportunities (merge duplicate functionality)\n   - Coordination mechanisms (if separate systems needed)\n   - Migration strategy for existing metrics\n   - Performance optimization opportunities\n8. Design unified metrics architecture if consolidation is recommended:\n   - Central metrics collection service\n   - Standardized metric definitions\n   - Consistent data formats and storage\n   - Real-time vs batch processing decisions\n9. Create implementation plan for chosen approach:\n   - Code refactoring requirements\n   - Database schema changes\n   - API modifications\n   - Migration scripts for historical data\n10. Document impact on dependent systems and update integration points",
        "testStrategy": "1. Create comprehensive test suite comparing metrics output between Analysis Agent and v1 run manager using identical input data\n2. Validate metric calculation accuracy by cross-referencing results with manual calculations\n3. Test performance impact of current vs proposed metrics implementation using benchmark scenarios\n4. Verify data consistency across all metric collection points and storage systems\n5. Conduct integration testing to ensure metrics consolidation doesn't break dependent dashboard and reporting systems\n6. Test migration scripts with historical data to ensure no data loss or corruption\n7. Validate real-time metrics updates and batch processing scenarios\n8. Perform load testing on unified metrics system to ensure scalability\n9. Test rollback procedures in case consolidation needs to be reverted\n10. Conduct user acceptance testing with stakeholders who rely on metrics data for decision making",
        "status": "pending",
        "dependencies": [
          3
        ],
        "priority": "medium",
        "subtasks": []
      },
      {
        "id": 24,
        "title": "Comprehensive V1 vs V2 Functionality Audit",
        "description": "Conduct a thorough comparison between v1 and v2 agent implementations to ensure no functionality has been lost in the transition, documenting any differences and ensuring feature parity where intended.",
        "details": "1. Create comprehensive functionality mapping between v1 and v2 systems:\n   - Map v1 apiHandlerAgent.js (2,249 lines) to Data Extraction Agent functionality\n   - Compare v1 detailProcessorAgent.js (815 lines) with Analysis Agent implementation (435 lines)\n   - Analyze v1 dataProcessorAgent.js (1,049 lines) against Storage Agent functionality\n   - Document v1 run manager metrics vs Analysis Agent metrics implementation\n\n2. Implement automated comparison framework:\n   - Create side-by-side execution environment for v1 and v2 agents\n   - Use identical input datasets for both versions\n   - Implement result comparison utilities with configurable tolerance levels\n   - Build automated reporting system for functionality gaps\n\n3. Conduct detailed feature analysis:\n   - API handling capabilities: pagination, retry logic, error handling patterns\n   - Data processing workflows: field mapping, sanitization, validation\n   - Scoring algorithms: compare old vs new scoring systems (projectTypeMatch/clientTypeMatch vs clientProjectRelevance)\n   - Content enhancement: analyze prompt effectiveness and output quality\n   - Database operations: duplicate detection, state eligibility, change tracking\n\n4. Performance and efficiency comparison:\n   - Token usage analysis between v1 and v2 implementations\n   - Processing time benchmarks for equivalent operations\n   - Memory usage patterns and optimization improvements\n   - API call frequency and batching efficiency\n\n5. Create comprehensive audit documentation:\n   - Functionality parity matrix with pass/fail status\n   - Performance improvement metrics and regression analysis\n   - Missing feature identification and impact assessment\n   - Recommendation report for addressing gaps or confirming intentional changes\n\n6. Implement gap resolution tracking:\n   - Prioritize missing functionality by business impact\n   - Create implementation roadmap for critical gaps\n   - Document intentional feature removals and their justifications\n   - Establish ongoing monitoring for feature parity maintenance",
        "testStrategy": "1. Execute parallel testing framework with identical datasets on both v1 and v2 systems, comparing outputs across all major functionality areas including API handling, data processing, and scoring algorithms\n\n2. Validate functionality parity by running comprehensive test suites covering edge cases, error scenarios, and performance benchmarks, ensuring v2 meets or exceeds v1 capabilities in all critical areas\n\n3. Conduct regression testing using historical production data to identify any functionality gaps or performance degradations, with automated reporting of discrepancies exceeding defined tolerance thresholds\n\n4. Perform user acceptance testing with stakeholders to validate that business requirements are met by v2 implementation and that no critical workflows have been disrupted\n\n5. Execute load testing scenarios comparing v1 and v2 performance under various conditions, validating that efficiency improvements don't come at the cost of functionality loss\n\n6. Review audit documentation with development team and stakeholders to confirm gap resolution plans and obtain sign-off on intentional feature changes or removals",
        "status": "pending",
        "dependencies": [
          1,
          2,
          3,
          4,
          7,
          23
        ],
        "priority": "medium",
        "subtasks": []
      },
      {
        "id": 25,
        "title": "Agent Performance Optimization and Caching",
        "description": "Implement comprehensive performance optimization across all agents including Redis caching, token usage optimization, and performance profiling to enhance system efficiency and reduce operational costs.",
        "details": "1. Implement Redis caching layer (use Redis v7.0.0) for:\n   - API response caching with configurable TTL\n   - Database query result caching\n   - Session and authentication token caching\n   - Frequently accessed opportunity data caching\n2. Optimize token usage across all agents:\n   - Implement token pooling and rate limiting\n   - Add intelligent prompt optimization to reduce token consumption\n   - Create token usage analytics and monitoring dashboard\n   - Implement context window optimization for large documents\n3. Performance profiling implementation:\n   - Integrate APM tools (use New Relic v9.0.0 or Datadog v5.0.0)\n   - Add custom performance metrics collection\n   - Implement memory leak detection and monitoring\n   - Create performance bottleneck identification system\n4. Agent-specific optimizations:\n   - Data Extraction Agent: Implement connection pooling and batch processing\n   - Storage Agent: Optimize database queries with proper indexing\n   - Analysis agents: Implement result caching and parallel processing\n5. Infrastructure optimizations:\n   - Implement CDN for static assets (use CloudFlare)\n   - Add database connection pooling (use PgBouncer for PostgreSQL)\n   - Implement horizontal scaling capabilities\n   - Add load balancing for agent distribution\n6. Monitoring and alerting:\n   - Set up performance threshold alerts\n   - Create performance degradation detection\n   - Implement automated scaling triggers\n   - Add comprehensive logging for performance events\n7. Code-level optimizations:\n   - Implement lazy loading patterns\n   - Add memoization for expensive computations\n   - Optimize async/await patterns\n   - Implement efficient data structures and algorithms\n8. Cache invalidation strategies:\n   - Implement smart cache invalidation based on data changes\n   - Add cache warming strategies for critical data\n   - Create cache hit/miss ratio monitoring\n   - Implement distributed cache consistency",
        "testStrategy": "1. Performance baseline establishment:\n   - Measure current system performance metrics before optimization\n   - Document processing times, memory usage, and token consumption\n   - Establish performance benchmarks for each agent\n2. Cache effectiveness testing:\n   - Validate cache hit/miss ratios meet target thresholds (>80% hit rate)\n   - Test cache invalidation accuracy and timing\n   - Verify data consistency between cached and source data\n   - Load test cache performance under high concurrent access\n3. Token optimization validation:\n   - Measure token usage reduction (target: 25-40% reduction)\n   - Validate prompt optimization maintains output quality\n   - Test token pooling and rate limiting functionality\n   - Monitor token cost reduction in production environment\n4. Performance profiling verification:\n   - Validate APM tool integration and data accuracy\n   - Test performance alert triggers and thresholds\n   - Verify memory leak detection capabilities\n   - Validate bottleneck identification accuracy\n5. Agent performance testing:\n   - Conduct load testing on each optimized agent\n   - Measure processing time improvements (target: 40-60% improvement)\n   - Test memory usage optimization (target: 30-50% reduction)\n   - Validate concurrent processing capabilities\n6. Infrastructure optimization testing:\n   - Test CDN performance improvements for static assets\n   - Validate database connection pooling effectiveness\n   - Test horizontal scaling triggers and performance\n   - Verify load balancing distribution accuracy\n7. End-to-end performance validation:\n   - Execute comprehensive system performance tests\n   - Compare optimized vs. baseline performance metrics\n   - Validate system stability under optimized configuration\n   - Test performance monitoring and alerting systems",
        "status": "pending",
        "dependencies": [
          1,
          2,
          6,
          7,
          8
        ],
        "priority": "low",
        "subtasks": []
      },
      {
        "id": 26,
        "title": "Review and Refine Analysis Agent Output Quality",
        "description": "Conduct comprehensive review of Analysis Agent test results to improve scoring accuracy and sales team relevance, focusing on complete sample testing, research opportunity filtering, and sales-oriented output refinement. Includes critical scoring precision improvements to decouple applicant and project scoring, and integrate eligible activities analysis for more granular relevance assessment.",
        "status": "pending",
        "dependencies": [
          3,
          19,
          "30"
        ],
        "priority": "high",
        "details": "1. Execute comprehensive testing with complete dataset:\n   - Run Analysis Agent tests with ALL opportunities (not limited samples) to get statistically significant results\n   - Update test scripts to process full batches without artificial limitations\n   - Document performance metrics and processing times for complete dataset analysis\n   - Identify any bottlenecks or timeout issues when processing large volumes\n\n2. Fix scoring logic for research-focused opportunities:\n   - Review current scoring criteria and identify research-heavy opportunity patterns\n   - Implement scoring penalties for opportunities primarily focused on research activities\n   - Add keyword detection for research indicators (grants, studies, academic partnerships)\n   - Create scoring rules that downgrade opportunities outside core service capabilities\n   - Test scoring adjustments with known research opportunities to validate filtering\n\n3. Implement scoring precision improvements:\n   - Decouple applicant scoring from project scoring in clientProjectRelevance component\n   - Create separate scoring components for eligible applicants vs project types for granular control\n   - Design weighted scoring system that prioritizes eligible activities over broad project categories\n   - Implement activity-specific scoring boosts for service-aligned activities (equipment purchase, installation, HVAC upgrades)\n   - Add scoring penalties for non-service activities (research, studies, planning-only)\n   - Test decoupled scoring system to validate improved precision\n\n4. Integrate eligible activities data into scoring logic:\n   - Analyze current eligible activities data structure and availability\n   - Design integration points between eligible activities and opportunity scoring\n   - Implement scoring boosts for opportunities matching high-value eligible activities\n   - Create weighted scoring system that considers activity alignment with company capabilities\n   - Weight eligible activities more heavily than broad project type categories in scoring calculations\n   - Validate that eligible activities integration improves scoring relevance and precision\n\n5. Refine use case examples for client execution context:\n   - Review all current use case examples in prompts and scoring logic\n   - Rewrite examples to focus on company executing projects FOR clients rather than client self-execution\n   - Update prompt templates to emphasize service delivery perspective\n   - Create context-appropriate examples that align with company's service model\n   - Ensure all generated descriptions reflect proper client-vendor relationship\n\n6. Enhance sales team output relevance:\n   - Review all enhanced descriptions and actionable summaries for sales context\n   - Refine output templates to include sales-relevant information (decision makers, timelines, budget indicators)\n   - Update summary generation to highlight key selling points and engagement strategies\n   - Ensure technical details are translated into business value propositions\n   - Add sales-specific metadata fields to opportunity records\n\n7. Validate service capability filtering:\n   - Test scoring system with opportunities clearly outside service scope\n   - Verify that low-relevance opportunities receive appropriately low scores\n   - Implement capability matching logic that aligns opportunities with service offerings\n   - Create negative scoring factors for incompatible opportunity types\n   - Test edge cases where project types seem irrelevant but activities are aligned (e.g., security projects with retrofit activities)\n   - Document and test edge cases where opportunities might be borderline relevant\n\n8. Update scoring criteria and prompts based on findings:\n   - Analyze test results to identify scoring inconsistencies or gaps\n   - Refine scoring weights and thresholds based on sales team feedback requirements\n   - Update prompt engineering to improve output quality and consistency\n   - Implement iterative improvements based on comprehensive testing results\n   - Document all changes and create validation tests for new criteria",
        "testStrategy": "1. Comprehensive dataset testing validation:\n   - Execute Analysis Agent with complete opportunity dataset and measure processing success rate\n   - Compare results between limited sample testing and full dataset testing to identify differences\n   - Validate that all opportunities are processed without data loss or timeout issues\n   - Measure and document performance metrics for full dataset processing\n\n2. Research opportunity filtering verification:\n   - Create test dataset with known research-focused opportunities\n   - Validate that research opportunities receive appropriately low scores (below defined threshold)\n   - Test edge cases where opportunities have mixed research and service components\n   - Verify that legitimate service opportunities are not incorrectly penalized\n\n3. Scoring precision improvements validation:\n   - Test decoupled applicant vs project scoring to validate improved granular control\n   - Compare scoring results before and after decoupling to measure precision improvements\n   - Validate that eligible activities weighting produces more accurate relevance scores\n   - Test edge cases where project types and activities have conflicting relevance signals\n   - Verify that activity-specific scoring adjustments correctly identify service-aligned opportunities\n\n4. Eligible activities integration testing:\n   - Test scoring improvements with opportunities that have matching eligible activities\n   - Validate that eligible activities data is correctly integrated into scoring calculations\n   - Compare scoring results before and after eligible activities integration\n   - Verify that activities alignment produces measurable scoring improvements\n   - Test scenarios where activities override project type scoring (security projects with retrofit activities)\n\n5. Use case example validation:\n   - Review generated opportunity descriptions to ensure client-vendor context is maintained\n   - Test prompt updates with various opportunity types to validate consistent framing\n   - Verify that all examples reflect company executing work FOR clients\n   - Validate that technical capabilities are properly positioned as services offered\n\n6. Sales team output quality assessment:\n   - Test enhanced descriptions and summaries for sales relevance and actionability\n   - Validate that output includes key sales information (timelines, decision makers, budget signals)\n   - Verify that technical details are appropriately translated for sales consumption\n   - Test output consistency across different opportunity types and complexity levels\n\n7. Service capability filtering verification:\n   - Test with opportunities clearly outside company service scope to validate low scoring\n   - Verify that capability matching logic correctly identifies relevant vs irrelevant opportunities\n   - Test borderline opportunities to ensure appropriate scoring nuance\n   - Validate that filtering doesn't exclude legitimate opportunities\n   - Test mixed-signal opportunities (irrelevant project type but relevant activities)\n\n8. Scoring criteria validation and regression testing:\n   - Test updated scoring criteria with historical opportunity data to validate improvements\n   - Perform regression testing to ensure changes don't negatively impact previously working functionality\n   - Validate scoring consistency and reproducibility across multiple test runs\n   - Compare scoring results with sales team expectations and feedback requirements\n   - Test precision improvements against known edge cases and challenging opportunity types",
        "subtasks": [
          {
            "id": 1,
            "title": "Run Full Analysis Agent Test with All Opportunities",
            "description": "Modify the test script to process ALL opportunities in the test data (not just 1 per batch) and analyze the complete results to get a proper sample size for evaluation",
            "status": "pending",
            "dependencies": [],
            "details": "1. Update test script to process all 6 opportunities (3 California + 3 Grants.gov) instead of stopping at 1 per batch\n2. Run comprehensive analysis and capture all enhanced descriptions, actionable summaries, and scoring\n3. Document results in a structured format for review\n4. Identify patterns in scoring across different opportunity types\n5. Flag any opportunities that may have been scored incorrectly\n<info added on 2025-07-02T05:43:24.812Z>\n1. Update test script to use new dual scoring system (clientRelevance and projectRelevance instead of clientProjectRelevance)\n2. Validate that new scoring criteria work correctly:\n   - clientRelevance (0-3): Based on eligible applicants matching target client types\n   - projectRelevance (0-3): Based on eligible activities matching preferred activities \n   - Updated funding attractiveness thresholds (50M+/5M+ for exceptional, 25M+/2M+ for strong, etc.)\n3. Test that research opportunities get appropriately low scores with new system\n4. Verify use case examples reflect \"we help clients\" perspective \n5. Document results showing improved scoring precision vs old system\n</info added on 2025-07-02T05:43:24.812Z>",
            "testStrategy": ""
          },
          {
            "id": 2,
            "title": "Fix Research Opportunity Scoring",
            "description": "Address the issue where research-focused opportunities (like NSF grants) are getting high scores when they should be low since research isn't in our wheelhouse",
            "status": "done",
            "dependencies": [],
            "details": "1. Review the Electrochemical Systems grant scoring (scored 8/10 but is pure research)\n2. Analyze why clientProjectRelevance scored 4/6 for a research opportunity\n3. Update scoring criteria or prompts to properly identify and down-score research-only opportunities\n4. Consider adding research vs. implementation distinction to scoring logic\n5. Test revised scoring against research opportunities to ensure they get appropriate low scores",
            "testStrategy": ""
          },
          {
            "id": 3,
            "title": "Refine Use Cases to Focus on Our Service Delivery",
            "description": "Update use case examples to properly reflect our company executing projects FOR clients, not clients doing projects themselves",
            "status": "done",
            "dependencies": [],
            "details": "1. Review current use cases that say things like 'A school district could develop...' \n2. Reframe to 'We could help a school district by...' or 'Our energy services team could...'\n3. Ensure use cases focus on our capabilities: HVAC systems, lighting upgrades, solar installation, building envelope improvements\n4. Update prompt instructions to emphasize our role as the service provider/contractor\n5. Test revised prompts to ensure use cases are properly framed",
            "testStrategy": ""
          },
          {
            "id": 4,
            "title": "Review and Improve Sales Team Relevance",
            "description": "Comprehensively review enhanced descriptions and actionable summaries to ensure they're optimally framed for sales team needs",
            "status": "done",
            "dependencies": [],
            "details": "1. Evaluate whether enhanced descriptions provide the right level of strategic insight for sales conversations\n2. Check if actionable summaries give sales reps clear next steps and client targeting guidance\n3. Ensure language is sales-appropriate (not too technical, properly business-focused)\n4. Verify that competitive landscape and client fit assessments are accurate\n5. Update prompts based on findings to better serve sales team workflow",
            "testStrategy": ""
          },
          {
            "id": 5,
            "title": "Implement Decoupled Applicant and Project Scoring",
            "description": "Separate the current clientProjectRelevance scoring into distinct components for eligible applicants and project types to achieve more granular scoring control",
            "status": "done",
            "dependencies": [],
            "details": "1. Analyze current clientProjectRelevance scoring logic to understand how applicant and project factors are combined\n2. Design separate scoring components: applicantRelevance and projectTypeRelevance\n3. Define weighting strategy for combining the decoupled scores into final relevance assessment\n4. Update scoring prompts and logic to handle separate evaluation of applicants vs project types\n5. Test decoupled scoring against current dataset to validate improved precision\n6. Document scoring component definitions and weighting rationale",
            "testStrategy": ""
          },
          {
            "id": 6,
            "title": "Integrate Eligible Activities Analysis into Scoring",
            "description": "Implement eligible activities analysis as a primary scoring factor, weighted more heavily than broad project type categories for improved relevance assessment",
            "status": "done",
            "dependencies": [],
            "details": "1. Analyze eligible activities data structure and extract activity-specific keywords and patterns\n2. Create activity scoring matrix with high-value activities (equipment purchase, installation, HVAC upgrades) and low-value activities (research, studies, planning-only)\n3. Implement eligible activities scoring component that evaluates activity alignment with company capabilities\n4. Weight eligible activities more heavily than project type categories in final scoring calculations\n5. Test activity-based scoring with edge cases (security projects with retrofit activities, energy projects with research activities)\n6. Validate that activities analysis correctly identifies service-aligned opportunities regardless of project type category",
            "testStrategy": ""
          }
        ]
      },
      {
        "id": 27,
        "title": "Refine Filter Function Logic and Thresholds",
        "description": "Refactor the filter function to remove temporal availability checks and implement specific funding thresholds, focusing on opportunity quality rather than data completeness.",
        "details": "1. Remove open/closed opportunity filtering from filter function:\n   - Eliminate temporal availability checks (open/closed status) from the core filter logic\n   - Move open/closed filtering to the initial API call phase in the Data Extraction Agent\n   - Update filter function to focus solely on relevance and quality scoring metrics\n   - Ensure filter function only processes clientProjectRelevance, fundingAttractiveness, and fundingType scores\n\n2. Implement specific funding threshold logic:\n   - Remove blanket exclusion of opportunities with unknown funding amounts\n   - Define minimum funding thresholds: $50K total funding or $25K per award minimum\n   - Create logic to handle missing funding data gracefully\n   - Implement funding amount parsing and validation for threshold comparison\n   - Add configuration options for adjustable funding thresholds\n\n3. Update filter function parameters and logic:\n   - Remove temporal filtering parameters from function signature\n   - Add funding threshold parameters with default values ($50K total, $25K per award)\n   - Update gating logic to incorporate funding thresholds alongside existing scoring gates\n   - Ensure backward compatibility with existing scoring system integration\n\n4. Code implementation considerations:\n   - Update filter function in the Analysis Agent module\n   - Modify any calling code that passes temporal filtering parameters\n   - Add proper error handling for funding amount parsing\n   - Implement logging for funding threshold decisions\n   - Update documentation and inline comments to reflect new logic\n\n5. Configuration and flexibility:\n   - Make funding thresholds configurable via environment variables\n   - Add admin interface controls for threshold adjustment\n   - Implement A/B testing capability for different threshold values\n   - Create metrics tracking for threshold impact analysis",
        "testStrategy": "1. Unit Testing:\n   - Test filter function with opportunities containing unknown funding amounts to verify they are not automatically excluded\n   - Validate funding threshold logic with various funding scenarios: below $25K per award, below $50K total, above thresholds, and missing data\n   - Test that temporal availability checks are completely removed from filter function\n   - Verify existing scoring gates (clientProjectRelevance ≥ 2, auto-qualification ≥ 5) still function correctly\n\n2. Integration Testing:\n   - Verify temporal filtering has been moved to Data Extraction Agent API call phase\n   - Test end-to-end pipeline to ensure no opportunities are lost due to missing funding data\n   - Validate that quality scoring remains the primary filter criteria\n   - Test configuration changes for funding thresholds through admin interface\n\n3. Data Validation Testing:\n   - Run filter function against historical data set to compare before/after results\n   - Identify opportunities previously excluded due to missing funding data that now pass through\n   - Verify funding threshold calculations are accurate for various funding structures\n   - Test edge cases: zero funding, negative values, non-numeric funding data\n\n4. Performance Testing:\n   - Measure filter function performance impact of new funding threshold logic\n   - Validate that removal of temporal checks improves processing speed\n   - Test memory usage with larger datasets containing mixed funding data quality\n\n5. User Acceptance Testing:\n   - Validate that more relevant opportunities appear in results due to reduced exclusions\n   - Confirm funding threshold settings produce appropriate opportunity quality\n   - Test admin controls for threshold adjustment and verify immediate impact",
        "status": "pending",
        "dependencies": [
          21,
          3
        ],
        "priority": "high",
        "subtasks": []
      },
      {
        "id": 28,
        "title": "Consolidate V1 Runs and V2 Agent Execution Metrics Tracking",
        "description": "Analyze and consolidate the dual metrics tracking systems (V2 agent_executions table and V1 runs tables) to create a unified approach that maintains historical capabilities while improving current performance tracking.",
        "details": "1. Conduct comprehensive analysis of existing metrics systems:\n   - Document V2 agent_executions table structure: input/output tracking, timing metrics, individual agent performance data\n   - Document V1 runs table structure: pipeline execution tracking, run stages, broader system metrics\n   - Identify data overlaps, gaps, and redundancies between the two systems\n   - Map relationships between individual agent operations and pipeline-level execution\n\n2. Compare metrics capabilities and coverage:\n   - Analyze granularity differences: agent-level vs pipeline-level tracking\n   - Evaluate historical data preservation requirements\n   - Assess current reporting and dashboard dependencies on each system\n   - Document performance implications of maintaining dual systems\n\n3. Design unified metrics architecture:\n   - Create consolidated database schema that combines strengths of both approaches\n   - Design hierarchical structure linking individual agent executions to pipeline runs\n   - Implement data migration strategy for historical V1 runs data\n   - Ensure backward compatibility for existing reporting systems\n   - Design efficient indexing strategy for performance optimization\n\n4. Implement unified metrics dashboard:\n   - Build comprehensive dashboard using React v18.2.0 and Material-UI v5.13.0\n   - Create drill-down capability from pipeline-level to agent-level metrics\n   - Implement real-time monitoring using WebSockets (socket.io v4.6.0)\n   - Add comparative analysis tools for V1 vs V2 performance\n   - Use Chart.js v4.3.0 for data visualizations\n   - Implement filtering and time-range selection capabilities\n\n5. Create metrics consolidation service:\n   - Build service to aggregate agent-level metrics into pipeline-level summaries\n   - Implement data retention policies for different metric granularities\n   - Create automated reporting system for performance trends\n   - Add alerting capabilities for performance degradation detection\n   - Use Redis v6.0.0 for caching frequently accessed metrics\n\n6. Establish metrics governance:\n   - Define standard metrics collection patterns for future agents\n   - Create documentation for metrics schema and usage guidelines\n   - Implement data quality validation for metrics integrity\n   - Design archival strategy for long-term historical data",
        "testStrategy": "1. Data Analysis and Validation:\n   - Compare metrics data between V1 and V2 systems using identical time periods\n   - Validate data integrity and completeness in both systems\n   - Test data migration accuracy from V1 to consolidated system\n   - Verify no data loss during consolidation process\n\n2. Performance Testing:\n   - Benchmark query performance on consolidated metrics schema vs separate systems\n   - Test dashboard load times with large datasets spanning multiple time periods\n   - Validate real-time metrics updates under high load conditions\n   - Measure storage efficiency improvements from consolidation\n\n3. Functional Testing:\n   - Test drill-down functionality from pipeline metrics to individual agent performance\n   - Validate filtering and time-range selection accuracy\n   - Test comparative analysis tools with historical V1 and current V2 data\n   - Verify alerting system triggers correctly for performance thresholds\n\n4. Integration Testing:\n   - Test compatibility with existing reporting systems and dashboards\n   - Validate metrics collection from all V2 agents feeds into consolidated system\n   - Test backward compatibility for any systems still dependent on V1 runs structure\n   - Verify metrics consolidation service accurately aggregates agent-level data\n\n5. User Acceptance Testing:\n   - Conduct testing with stakeholders who rely on current metrics systems\n   - Validate that unified dashboard meets all current reporting requirements\n   - Test usability of new consolidated metrics interface\n   - Verify historical data accessibility and accuracy in new system",
        "status": "pending",
        "dependencies": [
          4,
          23
        ],
        "priority": "medium",
        "subtasks": []
      },
      {
        "id": 29,
        "title": "Enhance Description Extraction in Data Extraction Agent",
        "description": "Update the Data Extraction Agent to comprehensively extract all descriptive content from API responses, including primary descriptions, nested synopsis objects, and supplementary narrative fields to provide richer context for analysis.",
        "details": "1. Expand description field extraction logic in the Data Extraction Agent:\n   - Identify and map ALL descriptive content fields from API responses including primary fields (description, summary, abstract, overview, synopsis)\n   - Extract nested synopsis objects and their sub-properties (detailedDescription, programSummary, backgroundInfo)\n   - Capture supplementary narrative fields (notes, additionalInfo, programDescription, details, remarks, commentary)\n   - Implement field detection logic that dynamically identifies descriptive content based on field names and data types\n\n2. Implement intelligent content combination strategy:\n   - Create structured output format with clear source markers: \"Primary Description: [content]\", \"Program Summary: [content]\", \"Additional Details: [content]\"\n   - Preserve verbatim content without modification, enhancement, or interpretation\n   - Implement deduplication logic to avoid repeating identical content from multiple fields\n   - Maintain hierarchical structure when combining nested descriptive objects\n   - Prioritize completeness over brevity - include all available descriptive content\n\n3. Update buildExtractionPrompt() method:\n   - Add comprehensive description extraction instructions to the prompt template\n   - Include specific field mapping guidance for common API response structures\n   - Add examples of proper description combination formatting\n   - Implement fallback extraction patterns for non-standard API response formats\n   - Update prompt to instruct preservation of original wording and context\n\n4. Enhance field mapping configuration:\n   - Extend existing field mapping to include comprehensive description field arrays\n   - Add support for nested object traversal to extract deep descriptive content\n   - Implement regex patterns for identifying descriptive fields in unknown API structures\n   - Create configurable extraction rules per funding source type\n\n5. Update data standardization process:\n   - Modify output schema to accommodate enhanced description structure\n   - Ensure backward compatibility with existing Analysis Agent expectations\n   - Implement validation to ensure all extracted descriptions are properly formatted\n   - Add logging for description extraction success rates and field coverage\n<info added on 2025-07-02T17:35:01.253Z>\n**TASK COMPLETED** ✅\n\n**Summary of Implementation:**\n\n1. **Enhanced buildExtractionPrompt() Function:**\n   - Added comprehensive description extraction strategy targeting primary description sources (description, summary, abstract, overview, programSummary, synopsis, details)\n   - Implemented nested descriptive content extraction (synopsis.description, details.overview, data.synopsis.*)\n   - Added supplementary narrative field capture (notes, additionalInfo, remarks, applicationProcess)\n   - Established content combination rules with clear source markers\n   - Ensured verbatim content preservation without modification or enhancement\n\n2. **Updated dataExtraction Schema:**\n   - Enhanced description field definition to reflect comprehensive extraction approach\n   - Schema now accurately documents the multi-source description combination strategy\n\n3. **Updated opportunityAnalysis Schema:**\n   - Made description field definition consistent with Data Extraction Agent output\n   - Ensures proper data flow between extraction and analysis stages\n\n**Implementation Impact:**\n- Data Extraction Agent now captures ALL available descriptive content from API responses across multiple field types\n- Descriptions are significantly more comprehensive with multiple sources properly combined\n- Analysis Agent receives much richer contextual information for improved scoring and use case generation\n- Both schemas accurately document and support the enhanced extraction behavior\n\n**Status:** Ready for Task 30 (Re-run Stage 2 Tests) to generate fresh test data utilizing the enhanced description extraction capabilities.\n</info added on 2025-07-02T17:35:01.253Z>",
        "testStrategy": "1. Description extraction validation:\n   - Test with sample API responses containing multiple description fields to verify all content is captured\n   - Validate proper source labeling and formatting of combined descriptions\n   - Verify verbatim content preservation without modification or enhancement\n   - Test deduplication logic with responses containing duplicate descriptive content\n\n2. Field mapping accuracy testing:\n   - Test extraction with various API response structures including nested objects and arrays\n   - Validate dynamic field detection with unknown API response formats\n   - Test fallback extraction patterns with non-standard response structures\n   - Verify comprehensive coverage of all descriptive field types\n\n3. Integration testing with Analysis Agent:\n   - Test enhanced descriptions improve Analysis Agent scoring accuracy\n   - Validate backward compatibility with existing Analysis Agent processing\n   - Measure improvement in use case generation quality with richer context\n   - Test performance impact of processing larger description content\n\n4. Prompt effectiveness validation:\n   - Test buildExtractionPrompt() updates produce consistent extraction results\n   - Validate extraction instructions work across different funding source types\n   - Test prompt performance with edge cases and malformed API responses\n   - Measure token usage impact of enhanced extraction prompts\n\n5. Data quality assurance:\n   - Validate output schema compliance with enhanced description structure\n   - Test extraction logging and monitoring for field coverage metrics\n   - Verify proper handling of missing or null description fields\n   - Test extraction performance with large API response datasets",
        "status": "done",
        "dependencies": [
          1
        ],
        "priority": "high",
        "subtasks": []
      },
      {
        "id": 30,
        "title": "Re-run Stage 2 Tests with Enhanced Descriptions and New Fields",
        "description": "Execute comprehensive stage 2 testing for both California and Grants.gov API sources using the enhanced Data Extraction Agent to generate fresh test results with improved descriptions and newly added fields like eligibleActivities.",
        "details": "1. Execute fresh stage 2 tests for California API source:\n   - Run data extraction using enhanced description logic from Task 29\n   - Verify all descriptive content fields are captured (primary descriptions, nested synopsis objects, supplementary narrative fields)\n   - Validate new field extraction including eligibleActivities, enhanced applicant criteria, and expanded program details\n   - Generate complete test dataset with comprehensive field coverage\n\n2. Execute fresh stage 2 tests for Grants.gov API source:\n   - Apply enhanced extraction logic to Grants.gov responses\n   - Ensure proper handling of Grants.gov-specific field structures and nested data\n   - Validate extraction of all new schema additions and field mappings\n   - Generate updated test results with full field population\n\n3. Comprehensive field validation:\n   - Verify eligibleActivities field is properly populated from API responses\n   - Confirm enhanced descriptions are significantly more comprehensive than previous versions\n   - Validate all recent schema additions are captured in test data\n   - Ensure proper source labeling and formatting of combined descriptions\n\n4. Test result preparation:\n   - Save updated stage 2 test results in standardized format for Analysis Agent consumption\n   - Document field coverage improvements and description enhancements\n   - Create comparison reports showing before/after data quality improvements\n   - Prepare comprehensive test datasets for downstream Analysis Agent testing\n\n5. Quality assurance checks:\n   - Verify data integrity and completeness across both sources\n   - Confirm no regression in existing field extraction\n   - Validate proper handling of edge cases and missing data scenarios\n   - Ensure test results meet requirements for Analysis Agent full testing (Task 26.1)",
        "testStrategy": "1. Pre-execution validation:\n   - Confirm enhanced Data Extraction Agent is properly deployed with Task 29 improvements\n   - Verify API connectivity and authentication for both California and Grants.gov sources\n   - Validate test environment configuration and data storage capabilities\n\n2. Stage 2 test execution validation:\n   - Monitor test execution for both sources to ensure successful completion\n   - Verify no errors or timeouts during enhanced extraction processing\n   - Confirm all API responses are processed with new extraction logic\n   - Validate proper handling of pagination and bulk data processing\n\n3. Enhanced description verification:\n   - Compare new descriptions against previous versions to confirm significant improvement\n   - Verify all descriptive content sources are captured and properly combined\n   - Validate source labeling and formatting consistency\n   - Confirm verbatim content preservation without unwanted modifications\n\n4. New field population testing:\n   - Specifically verify eligibleActivities field is populated across test opportunities\n   - Validate all new schema fields are properly extracted and formatted\n   - Confirm field mapping accuracy for both API sources\n   - Test handling of missing or null values in new fields\n\n5. Test result quality assurance:\n   - Validate saved test results contain complete field coverage\n   - Verify data format compatibility with Analysis Agent requirements\n   - Confirm test datasets include sufficient variety for comprehensive Analysis Agent testing\n   - Generate coverage reports showing field population rates and description quality metrics\n\n6. Integration readiness verification:\n   - Confirm test results are properly formatted for Task 26.1 (Analysis Agent full testing)\n   - Validate data accessibility and proper file/database storage\n   - Test data loading and parsing by Analysis Agent test scripts\n   - Verify no data corruption or formatting issues that would impact downstream testing",
        "status": "pending",
        "dependencies": [
          29
        ],
        "priority": "high",
        "subtasks": []
      }
    ],
    "metadata": {
      "created": "2025-06-17T23:54:58.582Z",
      "updated": "2025-07-02T17:35:05.399Z",
      "description": "Tasks for master context"
    }
  }
}