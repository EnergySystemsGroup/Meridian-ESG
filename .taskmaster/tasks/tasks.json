{
  "master": {
    "tasks": [
      {
        "id": 1,
        "title": "Implement Data Extraction Agent",
        "description": "Complete the implementation of the Data Extraction Agent, which will handle the core functionality from the v1 apiHandlerAgent.js.",
        "status": "done",
        "dependencies": [],
        "priority": "high",
        "details": "1. Extract core functionality from apiHandlerAgent.js (2,249 lines)\n2. Implement modular structure following v2 patterns\n3. Create separate modules for:\n   - API request handling (use Axios v1.8.3 for HTTP requests)\n   - Pagination management\n   - Field mapping (consider using json-schema for validation)\n   - Error handling\n   - Retry logic (implement exponential backoff)\n4. Ensure compatibility with both single-step and two-step API patterns\n5. Implement proper error handling and retry logic\n6. Maintain field mapping and taxonomy standardization\n7. Process pagination correctly\n8. Return standardized opportunity data structure\n9. Use JavaScript for implementation\n10. Implement unit tests using Vitest",
        "testStrategy": "1. Create unit tests for each module using Vitest\n2. Implement integration tests for the entire agent\n3. Test with mock API responses for various scenarios\n4. Validate correct handling of pagination\n5. Verify error handling and retry logic\n6. Ensure proper field mapping and data standardization\n7. Test performance and optimize as needed",
        "subtasks": [
          {
            "id": 1,
            "title": "Extract Core Functionality from apiHandlerAgent.js",
            "description": "Analyze the existing apiHandlerAgent.js file and extract the core functionality required for the Data Extraction Agent.",
            "dependencies": [],
            "details": "Review the 2,249 lines of code in apiHandlerAgent.js. Identify and isolate the essential functions and logic related to API request handling, pagination management, field mapping, error handling, and retry logic. Document the extracted functionality for use in subsequent tasks.",
            "status": "done",
            "testStrategy": "Create a checklist of core functionalities and verify each extracted component against the original file."
          },
          {
            "id": 2,
            "title": "Design Modular Structure for Data Extraction Agent",
            "description": "Create a modular architecture design for the Data Extraction Agent following v2 patterns.",
            "dependencies": [
              1
            ],
            "details": "Design a modular structure that separates concerns into distinct modules: API request handling, pagination management, field mapping, error handling, and retry logic. Ensure the design supports both single-step and two-step API patterns. Create a detailed architecture diagram and module specifications.",
            "status": "done",
            "testStrategy": "Review the design with team leads to ensure alignment with v2 architecture patterns and project requirements."
          },
          {
            "id": 3,
            "title": "Implement Core Modules of Data Extraction Agent",
            "description": "Develop the core modules of the Data Extraction Agent using JavaScript and following the designed modular structure.",
            "dependencies": [
              2
            ],
            "details": "Implement separate modules for API request handling (using Axios v1.8.3), pagination management, field mapping (consider using json-schema for validation), error handling, and retry logic with exponential backoff. Ensure proper JSDoc documentation and consistent coding patterns for maintainability.",
            "status": "done",
            "testStrategy": "Write unit tests for each module using Vitest, focusing on individual module functionality and edge cases."
          },
          {
            "id": 4,
            "title": "Integrate Modules and Implement Main Agent Logic",
            "description": "Combine the implemented modules and develop the main Data Extraction Agent logic to handle the complete extraction process.",
            "dependencies": [
              3
            ],
            "details": "Integrate the separate modules into a cohesive Data Extraction Agent. Implement the main logic to orchestrate the extraction process, including handling both single-step and two-step API patterns, managing the overall flow, and ensuring proper error handling and retries at the agent level. Implement field mapping and taxonomy standardization logic.",
            "status": "done",
            "testStrategy": "Develop integration tests using Vitest to verify the correct interaction between modules and end-to-end functionality of the agent."
          },
          {
            "id": 5,
            "title": "Optimize and Finalize Data Extraction Agent",
            "description": "Optimize the Data Extraction Agent's performance, ensure compatibility, and prepare for deployment.",
            "dependencies": [
              4
            ],
            "details": "Perform code optimization and refactoring where necessary. Ensure the agent correctly processes pagination and returns a standardized opportunity data structure. Verify compatibility with both single-step and two-step API patterns. Conduct thorough testing, including edge cases and error scenarios. Prepare documentation for deployment and usage.",
            "status": "done",
            "testStrategy": "Conduct performance testing, compatibility testing with various API patterns, and end-to-end testing with real-world scenarios using Vitest. Review and update all unit and integration tests."
          }
        ]
      },
      {
        "id": 2,
        "title": "Develop Storage Agent",
        "description": "Optimize the Storage Agent pipeline by implementing early duplicate detection with ID + Title validation to reduce LLM token usage and improve performance. The Storage Agent V2 already exists and is functionally complete, but needs pipeline flow optimization and duplicate detection repositioning with critical field protection.",
        "status": "done",
        "dependencies": [
          1
        ],
        "priority": "high",
        "details": "PIPELINE OPTIMIZATION STRATEGY:\nCurrent inefficient flow: DataExtraction → Analysis → Filter → Storage (with duplicate detection)\nNew optimized flow: DataExtraction → EarlyDuplicateDetector → [New opportunities only] → Analysis → Filter → Storage\n\nDUPLICATE PROCESSING FLOW (CLARIFIED):\nWhen EarlyDuplicateDetector finds a duplicate:\n1. Check the 6 critical fields for changes (comparing API data vs DB data)\n2. If changes found: Update fields directly in database (with null protection)\n3. If no changes found: Skip entirely (no processing needed)\n4. NEVER send duplicates through Analysis or Filter stages\n\nPIPELINE BRANCHING LOGIC:\n- New opportunities → Continue to Analysis → Filter → Storage\n- Duplicates with changes → Direct database update (bypass Analysis/Filter)\n- Duplicates without changes → Skip entirely (bypass Analysis/Filter/Storage)\n- Stale review (90-day fallback) → Direct field comparison and update (bypass Analysis/Filter)\n\nThis maximizes token savings by ensuring duplicates never hit expensive LLM processing stages.\n\nKEY IMPLEMENTATION AREAS:\n1. Create EarlyDuplicateDetector module with ID + Title validation approach\n2. Extend database schema with api_updated_at timestamps\n3. Modify DataExtractionAgent to capture API timestamps\n4. Update ProcessCoordinatorV2 to implement pipeline branching logic\n5. Implement Critical Fields + Null Protection strategy with direct database updates\n6. Implement API-based freshness checks using updated_at timestamps with 90-day stale review logic\n7. Use existing technologies: Prisma ORM v4.15.0, PostgreSQL, fuzzyset.js v1.0.7 for fuzzy matching\n8. Maintain comprehensive metrics tracking and error handling with Winston v3.9.0\n9. Continue using TypeScript for code quality\n\nCRITICAL FIELDS + NULL PROTECTION STRATEGY:\nOnly update these 6 critical fields when processing duplicates:\n- title (prevents duplicate detection failures when APIs change titles)\n- minimumAward, maximumAward, totalFundingAvailable (business-critical amounts)\n- closeDate, openDate (timing-critical dates)\n\nRule: Never overwrite existing data with null values. Only update when API provides non-null data.\n\nID + TITLE VALIDATION APPROACH:\n1. Check for ID matches within same source (treat ID as hint, not guarantee)\n2. Validate ID matches with title similarity check (confirming same opportunity)\n3. Fall back to title-only matching if no valid ID match found\n\nSTALE REVIEW LOGIC (CLARIFIED):\n- Trigger Condition: Only when API provides NO api_updated_at timestamp\n- Threshold: 90 days since our last updated_at timestamp\n- Action: Direct field comparison and update (bypass Analysis/Filter)\n- Post-Review: Update updated_at to current timestamp to reset the 90-day clock\n- Purpose: Fallback mechanism for APIs that don't provide update timestamps\n\nFRESHNESS CHECK OUTCOMES (UPDATED):\n1. API has timestamp + newer → Direct field comparison and update (bypass pipeline)\n2. API has timestamp + same/older → Skip entirely\n3. API has NO timestamp + 90+ days old → Direct field comparison and update (bypass pipeline)\n4. API has NO timestamp + <90 days → Skip entirely\n\nOnly truly new opportunities go through the full Analysis and Filter pipeline.\n\nBUSINESS DECISIONS RESOLVED:\n- Manual admin edit protection: Critical Fields + Null Protection (simple, effective)\n- Stale review threshold: 90 days (prevents endless re-processing while ensuring periodic verification)\n- ID reliability: Use ID + Title validation instead of ID-first matching",
        "testStrategy": "1. Test new pipeline branching logic ensures duplicates bypass Analysis/Filter stages\n2. Validate ID + Title validation approach prevents catastrophic updates\n3. Test Critical Fields + Null Protection strategy preserves admin edits with direct database updates\n4. Measure token savings from duplicates never hitting LLM processing stages\n5. Test API timestamp capture and freshness checking logic with 4-scenario decision matrix\n6. Verify 90-day stale review threshold works correctly and bypasses pipeline stages\n7. Test ProcessCoordinatorV2 branching modifications\n8. Ensure direct database updates for duplicates don't break existing functionality\n9. Performance testing with large datasets to validate maximum optimization gains\n10. Integration tests for complete optimized pipeline with proper branching logic\n11. Regression testing to ensure existing Storage Agent V2 functionality is preserved\n12. Test stale review logic triggers direct updates and resets 90-day clock correctly\n13. Validate only new opportunities go through full Analysis and Filter pipeline",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement EarlyDuplicateDetector with ID + Title validation",
            "description": "Create a new module that performs duplicate detection immediately after data extraction using two-step ID + Title validation approach",
            "status": "done",
            "dependencies": [],
            "details": "- Create EarlyDuplicateDetector class in /app/lib/agents-v2/core/storageAgent/\n- Implement ID + Title validation: ID match within source + title similarity confirmation\n- Fall back to title-only matching using fuzzyset.js v1.0.7 if no valid ID match\n- Design API for integration with ProcessCoordinatorV2 branching logic\n- Include timestamp-based freshness checking with 90-day stale threshold\n- Implement 4-scenario freshness check decision matrix\n- Return duplicate status, confidence scores, validation method used, and processing recommendation (bypass/continue)\n- Handle edge cases: ID reuse, missing IDs, scraped data\n- Include critical field comparison logic for duplicates\n<info added on 2025-07-06T18:10:44.971Z>\n✅ IMPLEMENTATION COMPLETED\n\nSuccessfully implemented EarlyDuplicateDetector V2 with all requested features:\n\n🏗️ ARCHITECTURE IMPLEMENTED:\n- Efficient batch processing: Single batch query for all IDs and titles (2 DB calls vs N calls)\n- Action-oriented output: Categorizes opportunities into newOpportunities, opportunitiesToUpdate, opportunitiesToSkip\n- Reuses existing duplicateDetector.titlesAreSimilar() and changeDetector.hasFieldChanged() logic\n\n🔍 ID + TITLE VALIDATION APPROACH:\n- Step 1: Check ID matches within same source (treats ID as hint)  \n- Step 2: Validate ID match with title similarity (prevents catastrophic updates from ID reuse)\n- Step 3: Fall back to title-only matching if ID validation fails\n- Comprehensive logging for ID reuse detection\n\n📊 4-SCENARIO FRESHNESS CHECK MATRIX:\n1. API has timestamp + newer → Process for update\n2. API has timestamp + same/older → Skip (no changes)\n3. API has NO timestamp + 90+ days old → Process for stale review  \n4. API has NO timestamp + <90 days → Skip (recently reviewed)\n\n⚡ CRITICAL FIELD COMPARISON:\n- Only checks 6 critical fields: title, minimumAward, maximumAward, totalFundingAvailable, closeDate, openDate\n- Uses existing changeDetector logic for consistency\n- Returns boolean for direct update decisions\n\n📁 FILES CREATED:\n- app/lib/agents-v2/core/storageAgent/earlyDuplicateDetector.js (main module)\n- app/lib/agents-v2/core/storageAgent/directUpdateHandler.js (companion utility)\n- app/lib/agents-v2/tests/earlyDuplicateDetector.test.js (comprehensive tests)\n\n🧪 TEST COVERAGE:\n- Main function integration tests\n- Batch optimization validation\n- ID + Title validation scenarios (including ID reuse protection)\n- All 4 freshness check scenarios\n- Critical field change detection\n- Error handling and edge cases\n- Performance characteristics (100 opportunities in <1 second)\n\n✅ READY FOR REVIEW (Task 2.7)\n</info added on 2025-07-06T18:10:44.971Z>",
            "testStrategy": "Test ID + Title validation prevents catastrophic updates when APIs reuse IDs, validate fallback to title-only matching works correctly, test all 4 freshness check scenarios, verify processing recommendations for pipeline branching"
          },
          {
            "id": 2,
            "title": "Extend database schema for optimization features",
            "description": "Add necessary database fields to support API timestamp tracking for freshness checks",
            "status": "done",
            "dependencies": [
              "2.7"
            ],
            "details": "- Add api_updated_at timestamp fields to relevant tables\n- Create Prisma migrations for schema changes\n- Update TypeScript types to reflect schema changes\n- Ensure schema supports 90-day stale review threshold tracking\n- Support both api_updated_at (from API) and updated_at (our timestamp) fields",
            "testStrategy": "Validate timestamp fields capture API update times correctly and support freshness calculations with 90-day stale review logic"
          },
          {
            "id": 3,
            "title": "Modify DataExtractionAgent for timestamp capture",
            "description": "Update DataExtractionAgent to capture and pass through API updated_at timestamps",
            "status": "done",
            "dependencies": [],
            "details": "- Modify data extraction logic to capture API timestamps when available\n- Ensure timestamp data flows through to EarlyDuplicateDetector\n- Update data structures and interfaces as needed\n- Maintain backward compatibility\n- Handle cases where APIs don't provide timestamps (for stale review logic)\n- Distinguish between API-provided timestamps and missing timestamp scenarios",
            "testStrategy": "Test timestamp capture from various API sources, validate data flow to duplicate detector, test handling of APIs without timestamps"
          },
          {
            "id": 4,
            "title": "Update ProcessCoordinatorV2 pipeline branching logic",
            "description": "Implement pipeline branching to ensure duplicates bypass Analysis/Filter stages and go directly to database updates",
            "status": "done",
            "dependencies": [],
            "details": "- Insert EarlyDuplicateDetector step after DataExtraction\n- Implement branching logic: New opportunities → Analysis/Filter/Storage, Duplicates → Direct database update\n- Implement 4-scenario freshness check decision matrix logic\n- Handle duplicates with changes: direct critical field updates with null protection\n- Handle duplicates without changes: skip entirely\n- Handle stale review scenarios: direct field comparison and update\n- Update error handling and logging for new branching flow\n- Ensure metrics tracking captures optimization benefits from bypassed stages\n- Implement direct database update functionality for duplicate processing\n<info added on 2025-07-08T00:07:29.793Z>\nSCOPE CLARIFICATION: This subtask prepares EarlyDuplicateDetector and DirectUpdateHandler modules for pipeline integration (ProcessCoordinatorV2 scope). These modules are complete and tested, but will be integrated into the pipeline in a separate task. The Storage Agent itself will be simplified to only handle new opportunities when the pipeline integration is complete.\n</info added on 2025-07-08T00:07:29.793Z>",
            "testStrategy": "Test complete pipeline branching with various duplicate scenarios, validate duplicates never reach Analysis/Filter stages, test token savings maximization, test all 4 freshness check scenarios including stale review logic"
          },
          {
            "id": 5,
            "title": "Implement Critical Fields + Null Protection with direct database updates",
            "description": "Create direct database update functionality for duplicates that only updates 6 critical fields and never overwrites existing data with null values",
            "status": "done",
            "dependencies": [],
            "details": "- Implement direct database update logic for duplicates (bypassing StorageAgent)\n- Update only: title, minimumAward, maximumAward, totalFundingAvailable, closeDate, openDate\n- Add null protection: never overwrite existing non-null data with null values\n- Create field comparison logic to detect changes in critical fields\n- Handle stale review updates: update critical fields + reset updated_at timestamp to current time\n- Ensure proper error handling and logging for direct updates\n- Maintain transaction integrity for database operations",
            "testStrategy": "Test critical field updates work correctly with direct database access, validate null protection prevents data loss, ensure admin edits are preserved, test stale review timestamp reset logic, verify transaction integrity"
          },
          {
            "id": 6,
            "title": "Performance testing and optimization validation",
            "description": "Measure and validate the performance improvements from pipeline optimization and architectural decisions",
            "status": "done",
            "dependencies": [],
            "details": "- Create benchmarks comparing old vs new pipeline performance\n- Measure token usage reduction from duplicates bypassing Analysis/Filter stages\n- Test end-to-end latency improvements from pipeline branching\n- Validate accuracy is maintained with ID + Title validation approach\n- Test Critical Fields + Null Protection effectiveness with direct updates\n- Document performance gains and admin edit protection benefits achieved\n- Validate 90-day stale review threshold prevents endless re-processing\n- Test all 4 freshness check scenarios for performance impact\n- Measure maximum reduction in unnecessary LLM processing\n- Validate only new opportunities consume Analysis/Filter resources",
            "testStrategy": "Comprehensive performance testing with real-world data scenarios including edge cases, validate 90-day stale review logic efficiency, measure maximum token savings from pipeline branching"
          },
          {
            "id": 7,
            "title": "Review and Validate EarlyDuplicateDetector Implementation",
            "description": "Comprehensive review and validation of the EarlyDuplicateDetector implementation to ensure it meets all architectural requirements before proceeding with database schema changes",
            "details": "- Review EarlyDuplicateDetector code for architectural compliance with ID + Title validation approach\n- Validate batch processing efficiency and database query optimization\n- Confirm output structure provides clear action-oriented categorization\n- Test ID + Title validation logic prevents catastrophic updates\n- Verify freshness check decision matrix implementation (4 scenarios)\n- Validate integration points with ProcessCoordinatorV2\n- Ensure proper error handling and edge case coverage\n- Confirm critical field comparison logic works correctly\n- Review code quality, TypeScript compliance, and documentation\n- Validate unit tests cover all scenarios and edge cases\n- Check performance characteristics with sample data\n- Ensure no regression with existing Storage Agent functionality",
            "status": "done",
            "dependencies": [
              1
            ],
            "parentTaskId": 2
          }
        ]
      },
      {
        "id": 3,
        "title": "Finalize Analysis Agent",
        "description": "Review, validate, and finalize the implementation of the Analysis Agent, ensuring no functionality loss from v1 detailProcessorAgent.js.",
        "details": "1. Review current implementation (435 lines)\n2. Compare functionality with v1 detailProcessorAgent.js (815 lines)\n3. Implement proper scoring algorithms\n4. Add content enhancement capabilities\n5. Ensure compatibility with filtering system\n6. Update prompts and scoring logic as needed\n7. Implement proper error handling\n8. Optimize for performance and token usage\n9. Use GPT-4 API for advanced language processing tasks\n10. Implement caching mechanism to reduce API calls (use Redis v6.0.0)\n11. Use TypeScript for improved code quality\n12. Implement unit and integration tests",
        "testStrategy": "1. Develop comprehensive unit tests for each module\n2. Create integration tests for the entire Analysis Agent\n3. Compare results with v1 agent to ensure no functionality loss\n4. Test scoring algorithms with various input scenarios\n5. Validate content enhancement capabilities\n6. Verify compatibility with the filtering system\n7. Perform performance and token usage optimization tests\n8. Test error handling with various edge cases",
        "priority": "high",
        "dependencies": [
          1,
          2
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Review and Compare Functionality",
            "description": "Thoroughly review the current v2 implementation and compare its functionality with v1 detailProcessorAgent.js",
            "dependencies": [],
            "details": "1. Analyze the current v2 implementation (435 lines)\n2. Compare with v1 detailProcessorAgent.js (815 lines)\n3. Document differences in functionality\n4. Identify any missing features or improvements\n5. Ensure all key responsibilities are addressed",
            "status": "done",
            "testStrategy": "Create a checklist of v1 features and verify each in v2"
          },
          {
            "id": 2,
            "title": "Implement and Validate Scoring Algorithms",
            "description": "Develop and validate systematic scoring algorithms to rate opportunities accurately",
            "dependencies": [
              1
            ],
            "details": "1. Design scoring criteria based on business requirements\n2. Implement scoring algorithms in JavaScript\n3. Test with sample data from Data Extraction Agent\n4. Calibrate and fine-tune algorithms\n5. Document scoring methodology",
            "status": "done",
            "testStrategy": "Unit tests for individual scoring components and integration tests with sample data"
          },
          {
            "id": 3,
            "title": "Enhance Content and Generate Actionable Summaries",
            "description": "Improve opportunity descriptions and create actionable summaries for sales teams",
            "dependencies": [
              1
            ],
            "details": "1. Implement content enhancement logic\n2. Develop summary generation algorithm\n3. Ensure readability and professionalism\n4. Optimize for decision-making\n5. Integrate with GPT-4 API for advanced language processing\n<info added on 2025-06-21T22:20:00.707Z>\nUpdated actionable summary prompt to use natural guidelines-based approach instead of rigid bullet-point structure. New prompt focuses on funding opportunity overview, eligibility criteria, project types, and company relevance assessment for sales teams. This change removes prescriptive formatting constraints while maintaining sales-focused quality requirements. Enhanced flexibility allows AI to write more naturally and contextually appropriate responses. Next step is to apply similar natural approach to enhanced description section and conduct testing of improvements.\n</info added on 2025-06-21T22:20:00.707Z>\n<info added on 2025-06-21T22:38:39.331Z>\nCompleted enhanced description prompt update with strategic, use-case focused approach. Updated enhanced description to strategic, use-case driven format using new prompt: \"Write a detailed, strategic description of this funding opportunity. Summarize what it is, why it's important, who qualifies, and what kinds of projects are eligible. Then provide 2–3 short use case examples showing how our clients—such as cities, school districts, or state facilities—could take advantage of the opportunity. Focus on narrative clarity and practical insight, not boilerplate language.\" Key improvements include strategic focus versus generic rewrite, practical use case examples for actual client types, anti-jargon directive for clarity, client-specific scenarios for cities/school districts/state facilities, and emphasis on narrative clarity and actionable insights. Enhanced descriptions will now include strategic summaries with concrete examples like school district HVAC upgrades, city building efficiency projects, and state facility solar installations, transforming generic rewrites into actionable sales tools. Both actionable summary and enhanced description prompts now use natural, guidelines-based approaches optimized for sales team effectiveness.\n</info added on 2025-06-21T22:38:39.331Z>",
            "status": "done",
            "testStrategy": "A/B testing of enhanced content against original, peer review of generated summaries"
          },
          {
            "id": 6,
            "title": "Clean Up Analysis Agent Prompt",
            "description": "Clean up and refine the analysis agent prompt by getting guidance on what each portion should accomplish",
            "details": "<info added on 2025-06-18T03:42:25.272Z>\nStarted analysis of current prompt structure. Identified 5 main sections: 1) Business context, 2) Data formatting, 3) Analysis instructions, 4) Scoring criteria, 5) JSON schema. Ready to get guidance on each section for cleanup and refinement.\n</info added on 2025-06-18T03:42:25.272Z>\n<info added on 2025-06-18T03:59:47.612Z>\nApplied business context revision to code. Analyzed current systematic scoring criteria with 5 components: projectTypeMatch (0-3), clientTypeMatch (0-3), categoryMatch (0-2), fundingThreshold (0-1), fundingType (0-1) totaling 10 points. Evaluating whether current weights are appropriate and considering additional criteria including competition level, application complexity, timeline feasibility, and geographic alignment to enhance scoring accuracy.\n</info added on 2025-06-18T03:59:47.612Z>\n<info added on 2025-06-18T04:46:05.809Z>\nApplied business context revision to code. Added 3 new subtasks to Task 3: 3.10) Update taxonomy with target client/project types, 3.11) Add eligible_activities field to DB and Data Extraction Agent, 3.12) Replace categoryMatch with activityMatch in scoring. Created Task 20 for client advantage analysis with database field and Data Extraction Agent integration. Ready to continue with systematic scoring refinement after these data structure updates are complete.\n</info added on 2025-06-18T04:46:05.809Z>\n<info added on 2025-06-21T21:42:59.131Z>\nCompleted systematic scoring refinement with new gating system implementation. Replaced 5-component scoring with streamlined 3-component gating system: clientProjectRelevance (0-6) with gating requirement, fundingAttractiveness (0-3) for financial assessment, and fundingType (0-1) grant bonus. Added dynamic taxonomy imports and gating logic requiring ≥2 for viability, ≥5 for auto-qualification. Updated prompt to reference actual field names and fixed metrics calculation. New system prevents irrelevant high-funding opportunities while auto-passing perfect matches, with 60% weighting on business relevance. System ready for testing or proceeding to schema review.\n</info added on 2025-06-21T21:42:59.131Z>",
            "status": "done",
            "dependencies": [
              1
            ],
            "parentTaskId": 3
          },
          {
            "id": 7,
            "title": "Review and Implement Opportunity Analysis Schema",
            "description": "Review the opportunity analysis schema in anthropic client and implement it in the analysis agent once finalized",
            "details": "<info added on 2025-06-21T23:41:18.096Z>\nCOMPLETED: Fixed Analysis Agent schema alignment and added database fields\n\nMAJOR FIXES COMPLETED:\n1. Database Migration Applied: Added enhanced_description (TEXT) and scoring (JSONB) fields to funding_opportunities table\n2. Schema Alignment Fixed: \n   - Updated opportunityAnalysis schema to match database fields exactly\n   - Renamed scoringExplanation → relevanceReasoning (matches DB field)\n   - Kept scoring object in schema (will map to JSONB in storage)\n   - Updated field descriptions for natural language approach\n3. Analysis Agent Updated: \n   - Now uses centralized schemas.opportunityAnalysis instead of inline schema\n   - Updated error handling to use relevanceReasoning field name\n   - Simplified response processing since LLM returns complete objects\n\nSTORAGE MAPPING PLAN (Next Step):\n- scoring.overallScore → relevance_score (for backward compatibility)\n- scoring object → scoring JSONB field (new)\n- relevanceReasoning → relevance_reasoning (direct match)\n- enhancedDescription → enhanced_description (direct match)\n\nThis ensures Analysis Agent output perfectly matches database structure while preserving full scoring object for analysis.\n</info added on 2025-06-21T23:41:18.096Z>",
            "status": "done",
            "dependencies": [
              6
            ],
            "parentTaskId": 3
          },
          {
            "id": 10,
            "title": "Update Taxonomy with Target Client Types and Project Types",
            "description": "Add target client types, target project types, and preferred activities to the taxonomy file to support enhanced opportunity analysis",
            "details": "",
            "status": "done",
            "dependencies": [],
            "parentTaskId": 3
          },
          {
            "id": 11,
            "title": "Add Eligible Activities Field to Database and Data Extraction",
            "description": "Add 'eligible_activities' field to database schema and update Data Extraction Agent to capture this non-standardized data from LLM analysis",
            "details": "<info added on 2025-06-21T23:03:16.256Z>\nCOMPLETED: Successfully created and applied database migration for new funding opportunity fields\n\nMIGRATION FILE: 20250702000001_add_funding_process_fields.sql APPLIED\n\nVERIFIED DATABASE CHANGES:\n- disbursement_type (TEXT, nullable) - How funding is distributed\n- award_process (TEXT, nullable) - Award selection process  \n- eligible_activities (TEXT[], nullable) - Array of fundable activities\n\nIMPLEMENTATION DETAILS:\n- Added proper column comments for documentation\n- Created performance indexes including GIN index for array field\n- Successfully updated funding_opportunities_with_geography view\n- Fixed column ordering to match existing view structure\n- Migration applied successfully to local database\n\nDATABASE VERIFICATION:\n- Confirmed all 3 new columns exist in funding_opportunities table\n- Confirmed all 3 new columns are available in the view\n- Migration executed without errors\n\nSTATUS: COMPLETE - Ready for Data Extraction Agent updates\nNEXT: Update Data Extraction Agent to populate these fields during opportunity processing\n</info added on 2025-06-21T23:03:16.256Z>",
            "status": "done",
            "dependencies": [
              10
            ],
            "parentTaskId": 3
          },
          {
            "id": 12,
            "title": "Replace Category Match with Activity Match in Scoring",
            "description": "Update systematic scoring to replace 'categoryMatch' with 'activityMatch' using the new eligible_activities data for more precise opportunity scoring",
            "details": "",
            "status": "done",
            "dependencies": [
              11
            ],
            "parentTaskId": 3
          },
          {
            "id": 13,
            "title": "Update Anthropic Client Schema with New Database Fields",
            "description": "Add disbursement_type, award_process, and eligible_activities fields to both dataExtraction and opportunityAnalysis schemas in the Anthropic client to support the new database structure",
            "details": "",
            "status": "done",
            "dependencies": [
              10
            ],
            "parentTaskId": 3
          }
        ]
      },
      {
        "id": 4,
        "title": "Conduct Individual Agent Testing",
        "description": "Perform thorough testing of each v2 agent using the new optimized pipeline flow (DataExtraction → EarlyDuplicateDetector → [New opportunities only] → Analysis → Filter → Storage), ensuring performance metrics meet or exceed v1 benchmarks with 60-80% token reduction.",
        "status": "in-progress",
        "dependencies": [
          1,
          2,
          3
        ],
        "priority": "high",
        "details": "Test the new optimized pipeline where duplicate opportunities are detected early and handled separately, allowing only NEW opportunities to go through expensive LLM Analysis and Filter stages:\n\n1. Update existing test scripts in scripts/test/ for new pipeline flow\n2. Add comprehensive testing for EarlyDuplicateDetector integration\n3. Test duplicate handling scenarios (update vs skip)\n4. Implement performance benchmarking focusing on token usage reduction (use Benchmark.js v2.1.4)\n5. Validate that only new opportunities reach Analysis and Filter stages\n6. Create data quality validation tests for optimized flow\n7. Test with multiple API source types in new pipeline\n8. Use real API responses for testing duplicate detection accuracy\n9. Implement mock servers for API testing (use Mock Service Worker v1.2.1)\n10. Create detailed test reports showing token usage improvements\n11. Use Jest v29.5.0 for test runner and assertion library\n12. Implement code coverage reporting (use Istanbul v0.4.5)\n13. Automate testing process using GitHub Actions\n14. Measure and validate 60-80% token reduction claims",
        "testStrategy": "1. Run individual tests for each agent in the new pipeline order\n2. Test EarlyDuplicateDetector with various duplicate scenarios\n3. Verify that duplicates bypass Analysis and Filter stages\n4. Perform integration tests with real API data using optimized flow\n5. Conduct performance benchmarking against v1 agents, focusing on token usage\n6. Validate error handling with various scenarios in new pipeline\n7. Verify data quality and accuracy of duplicate detection\n8. Test with different API source types in optimized pipeline\n9. Ensure test coverage is above 80%\n10. Review and analyze test reports, particularly token usage metrics\n11. Validate that new opportunities still receive full processing\n12. Test edge cases where duplicate detection might fail",
        "subtasks": [
          {
            "id": 1,
            "title": "Update Test Scripts for New Pipeline Flow",
            "description": "Modify existing test scripts in scripts/test/ to reflect the new optimized pipeline flow: DataExtraction → EarlyDuplicateDetector → [New opportunities only] → Analysis → Filter → Storage",
            "dependencies": [],
            "details": "Update test scripts to use the new pipeline sequence, ensuring EarlyDuplicateDetector is properly integrated after DataExtraction and before Analysis/Filter stages. Modify test configurations to handle the conditional flow where only new opportunities proceed to expensive LLM stages.",
            "status": "done",
            "testStrategy": "Unit tests for pipeline flow validation using Jest v29.5.0"
          },
          {
            "id": 2,
            "title": "Implement EarlyDuplicateDetector Integration Tests",
            "description": "Create comprehensive tests for EarlyDuplicateDetector integration within the new pipeline, focusing on duplicate detection accuracy and performance",
            "dependencies": [
              1
            ],
            "details": "Develop test cases that validate EarlyDuplicateDetector's ability to identify duplicates early in the pipeline. Test various duplicate scenarios including exact matches, near-duplicates, and edge cases. Ensure proper integration with DataExtraction agent output.",
            "status": "pending",
            "testStrategy": "Integration tests with real API response data and mock duplicate scenarios"
          },
          {
            "id": 3,
            "title": "Test Duplicate Handling Scenarios",
            "description": "Validate duplicate handling logic for update vs skip scenarios, ensuring the system correctly processes existing opportunities without unnecessary LLM calls",
            "dependencies": [
              2
            ],
            "details": "Create test cases for different duplicate handling scenarios: updating existing opportunities with new information, skipping unchanged duplicates, and handling partial duplicates. Verify that duplicate opportunities bypass Analysis and Filter stages appropriately.",
            "status": "pending",
            "testStrategy": "End-to-end tests with controlled duplicate data sets"
          },
          {
            "id": 4,
            "title": "Setup Performance Benchmarking Framework",
            "description": "Implement performance benchmarking using Benchmark.js v2.1.4 to measure token usage reduction and overall pipeline performance improvements",
            "dependencies": [
              1
            ],
            "details": "Setup benchmarking infrastructure to measure token usage, processing time, and throughput for both v1 and v2 pipelines. Create baseline measurements and establish metrics for the 60-80% token reduction target. Implement automated benchmark reporting.",
            "status": "pending",
            "testStrategy": "Performance benchmarks comparing v1 vs v2 pipeline efficiency"
          },
          {
            "id": 5,
            "title": "Validate New Opportunity Flow Control",
            "description": "Test that only new opportunities reach Analysis and Filter stages, ensuring the optimization logic works correctly and duplicates are properly filtered out",
            "dependencies": [
              2,
              3
            ],
            "details": "Create test scenarios that verify the conditional flow where only new opportunities proceed to expensive LLM stages. Validate that duplicate opportunities are handled efficiently without unnecessary processing. Monitor and log the flow control decisions.",
            "status": "pending",
            "testStrategy": "Flow control validation tests with mixed new and duplicate opportunity datasets"
          },
          {
            "id": 6,
            "title": "Implement Data Quality Validation Tests",
            "description": "Create comprehensive data quality validation tests for the optimized pipeline flow, ensuring data integrity throughout the new process",
            "dependencies": [
              3,
              5
            ],
            "details": "Develop tests that validate data quality at each stage of the optimized pipeline. Ensure that duplicate detection doesn't compromise data integrity and that new opportunities maintain full data quality through all processing stages.",
            "status": "pending",
            "testStrategy": "Data quality assertion tests with schema validation"
          },
          {
            "id": 7,
            "title": "Test Multiple API Source Types",
            "description": "Validate the new pipeline with multiple API source types using Mock Service Worker v1.2.1 to ensure compatibility across different funding source formats",
            "dependencies": [
              4,
              6
            ],
            "details": "Setup mock servers using Mock Service Worker v1.2.1 to simulate different API source types. Test the pipeline with various funding source formats to ensure the EarlyDuplicateDetector and overall flow work correctly across all supported sources.",
            "status": "pending",
            "testStrategy": "Multi-source integration tests with mocked API responses"
          },
          {
            "id": 8,
            "title": "Generate Performance Reports and Automate Testing",
            "description": "Create detailed test reports showing token usage improvements and setup automated testing process using GitHub Actions with code coverage reporting via Istanbul v0.4.5",
            "dependencies": [
              4,
              5,
              6,
              7
            ],
            "details": "Generate comprehensive performance reports documenting the 60-80% token reduction achievement. Setup GitHub Actions workflow for automated testing and implement code coverage reporting using Istanbul v0.4.5. Create dashboard for monitoring ongoing performance metrics.",
            "status": "pending",
            "testStrategy": "Automated test reporting with performance metrics tracking and code coverage analysis"
          }
        ]
      },
      {
        "id": 5,
        "title": "Rewrite Unit Tests",
        "description": "Update all unit tests to match v2 implementations, maintaining 80%+ coverage and properly covering edge cases.",
        "details": "1. Update existing unit tests in app/lib/agents-v2/tests/\n2. Add new tests for completed agents\n3. Implement proper mocking for external dependencies (use Jest mocking capabilities)\n4. Create test data fixtures that match real API responses\n5. Ensure test coverage maintains 80%+ (use Istanbul for coverage reporting)\n6. Implement snapshot testing for complex objects\n7. Use TypeScript for writing tests\n8. Implement test helpers and utilities for common testing scenarios\n9. Use faker.js v8.0.2 for generating test data\n10. Implement property-based testing for edge cases (use fast-check v3.10.0)",
        "testStrategy": "1. Review and update each test suite\n2. Verify that all tests pass after updates\n3. Check test coverage and add tests where needed\n4. Validate mocking of external dependencies\n5. Ensure edge cases are properly covered\n6. Perform code review of test updates\n7. Run the entire test suite and verify results",
        "priority": "medium",
        "dependencies": [
          1,
          2,
          3,
          4
        ],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": 6,
        "title": "Implement End-to-End Pipeline Testing",
        "description": "Develop and execute comprehensive end-to-end tests for the complete optimized v2 pipeline, ensuring proper orchestration, early duplicate detection, and significant performance improvements.",
        "status": "pending",
        "dependencies": [
          1,
          2,
          3,
          4,
          5
        ],
        "priority": "high",
        "details": "1. Implement end-to-end tests covering the NEW OPTIMIZED pipeline flow: DataExtraction → EarlyDuplicateDetector → [New opportunities only] → Analysis → Filter → Storage\n2. Test EarlyDuplicateDetector functionality with ID + Title validation logic\n3. Validate that duplicates are handled with direct database updates (Critical Fields + Null Protection)\n4. Verify that only NEW opportunities proceed to expensive LLM Analysis and Filter stages\n5. Measure and validate 60-80% reduction in token usage\n6. Confirm 60-80% performance improvement in processing speed\n7. Test process coordinator orchestration with the new optimized flow\n8. Validate Supabase Edge Function integration with optimized pipeline\n9. Verify run tracking and metrics collection for the new architecture\n10. Implement comprehensive error handling tests for each stage\n11. Use Cypress v12.14.0 for end-to-end testing with optimized pipeline scenarios\n12. Create test scenarios validating duplicate detection accuracy\n13. Implement performance benchmarking comparing old vs new pipeline\n14. Test various data sources and configurations with the optimized flow\n15. Create detailed logging for each stage of the optimized pipeline\n16. Use Docker for creating isolated test environments\n17. Implement parallel test execution for faster results",
        "testStrategy": "1. Design test scenarios covering the optimized pipeline flow with various duplicate detection cases\n2. Execute end-to-end tests in a staging environment focusing on EarlyDuplicateDetector accuracy\n3. Validate correct data flow through the optimized pipeline (DataExtraction → EarlyDuplicateDetector → Analysis/Filter → Storage)\n4. Test error handling and recovery mechanisms at each stage\n5. Verify metrics collection including token usage reduction and performance improvements\n6. Perform load testing to ensure scalability with the optimized architecture\n7. Validate that duplicate opportunities bypass expensive LLM stages\n8. Test Critical Fields updates and Null Protection for duplicate handling\n9. Benchmark performance improvements (target: 60-80% faster processing)\n10. Validate token usage reduction (target: 60-80% reduction)\n11. Review logs and performance metrics for the optimized pipeline\n12. Compare performance metrics between old and new pipeline architectures",
        "subtasks": []
      },
      {
        "id": 7,
        "title": "Conduct Performance Validation",
        "description": "Perform thorough performance testing to ensure v2 with optimized pipeline meets or exceeds performance targets, focusing on EarlyDuplicateDetector benefits and LLM token reduction.",
        "status": "pending",
        "dependencies": [
          6
        ],
        "priority": "high",
        "details": "1. Implement performance monitoring tools (use Prometheus v2.40.0 and Grafana v9.5.0)\n2. Compare v1 vs v2 processing times with optimized pipeline\n3. Monitor memory and resource usage with new pipeline architecture (use node-memwatch v2.0.0)\n4. Validate timeout resolution in Supabase Edge Functions\n5. Document performance improvements from optimized pipeline\n6. Use Apache JMeter v5.5 for load testing\n7. Implement custom performance benchmarks for EarlyDuplicateDetector\n8. Use Datadog v0.36.0 for comprehensive system monitoring\n9. Optimize database queries and indexing for Critical Fields updates\n10. Implement caching strategies where appropriate (use Redis v6.0.0)\n11. Profile code to identify and resolve performance bottlenecks (use Node.js built-in profiler)\n12. Measure EarlyDuplicateDetector accuracy and performance impact\n13. Validate 60-80% reduction in LLM token usage from duplicate prevention\n14. Test direct Critical Fields database updates performance\n15. Compare memory usage with new pipeline architecture",
        "testStrategy": "1. Execute performance tests in a production-like environment\n2. Compare v1 and v2 performance metrics with optimized pipeline\n3. Validate that processing time is 60-80% faster than v1\n4. Verify 60-80% reduction in LLM token usage due to EarlyDuplicateDetector\n5. Confirm memory usage reduction with new pipeline architecture\n6. Test EarlyDuplicateDetector accuracy (>95% duplicate detection rate)\n7. Measure database performance for direct Critical Fields updates\n8. Test for timeout issues in Supabase Edge Functions\n9. Perform load testing to ensure scalability with optimized pipeline\n10. Validate that duplicates are prevented from reaching Analysis/Filter stages\n11. Analyze and document performance results from optimized architecture",
        "subtasks": []
      },
      {
        "id": 8,
        "title": "Implement Feature Flag System",
        "description": "Develop a feature flag system to toggle between v1 and v2 processing, with granular control and traffic percentage routing.",
        "details": "1. Implement feature flag configuration (use LaunchDarkly v2.25.1)\n2. Create routing service for v1/v2 selection\n3. Add environment variable controls\n4. Build admin interface for toggle management (use React v18.2.0 and Material-UI v5.13.0)\n5. Implement A/B testing infrastructure\n6. Use Redis for distributed caching of feature flags\n7. Implement logging for feature flag changes\n8. Create dashboard for monitoring feature flag usage\n9. Implement gradual rollout capabilities\n10. Ensure proper error handling for flag-related issues\n11. Use TypeScript for type-safe feature flag implementation",
        "testStrategy": "1. Test feature flag functionality in various scenarios\n2. Verify correct routing between v1 and v2\n3. Validate admin controls for toggle management\n4. Test gradual traffic routing capabilities\n5. Ensure proper handling of edge cases\n6. Verify logging and monitoring of feature flag usage\n7. Perform security testing on admin interface",
        "priority": "high",
        "dependencies": [
          6,
          7
        ],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": 9,
        "title": "Develop Migration Strategy",
        "description": "Implement a migration strategy for gradual transition from v1 to v2, including comparison tracking and rollback capability.",
        "details": "1. Implement Strangler Fig pattern as outlined in the architecture guide\n2. Start with 5% traffic routing to v2\n3. Implement comparison logging between v1 and v2 results\n4. Monitor performance and accuracy metrics\n5. Implement automated rollback triggers\n6. Create dashboard for migration progress monitoring\n7. Implement database schema migrations (use Prisma Migrate)\n8. Develop data consistency checks between v1 and v2\n9. Create detailed rollback procedures\n10. Implement blue-green deployment strategy\n11. Use Terraform v1.5.0 for infrastructure as code",
        "testStrategy": "1. Test gradual traffic routing mechanism\n2. Validate comparison logging accuracy\n3. Verify automated rollback functionality\n4. Test data consistency between v1 and v2\n5. Perform mock migration scenarios\n6. Validate monitoring and alerting systems\n7. Conduct full rollback test\n8. Review migration dashboard effectiveness",
        "priority": "high",
        "dependencies": [
          8
        ],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": 10,
        "title": "Implement Funding-Only Dashboard",
        "description": "Develop a new dashboard focused solely on funding opportunities, removing legislation-related features and preparing for user management integration.",
        "details": "1. Audit current dashboard for legislation references\n2. Remove legislation-specific components\n3. Update navigation and UI elements\n4. Implement enhanced funding opportunity views\n5. Prepare user context integration points\n6. Use React v18.2.0 for frontend development\n7. Implement state management with Redux Toolkit v1.9.5\n8. Use Material-UI v5.13.0 for consistent UI components\n9. Implement responsive design for mobile compatibility\n10. Use React Query v3.39.3 for efficient data fetching\n11. Implement code-splitting for improved performance\n12. Use Storybook v7.0.20 for component development and documentation",
        "testStrategy": "1. Perform UI/UX testing for the new dashboard\n2. Validate removal of all legislation-related features\n3. Test responsiveness on various devices\n4. Conduct user acceptance testing\n5. Verify data accuracy in funding opportunity displays\n6. Test search and filtering capabilities\n7. Ensure accessibility compliance (WCAG 2.1)\n8. Perform cross-browser testing",
        "priority": "medium",
        "dependencies": [
          9
        ],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": 11,
        "title": "Enhance Dashboard Performance and UX",
        "description": "Implement performance improvements and UX enhancements for the funding-only dashboard, including faster load times and improved mobile responsiveness.",
        "details": "1. Implement lazy loading for opportunity lists (use react-window v1.8.9)\n2. Optimize database queries (use database indexing and query optimization)\n3. Add proper caching strategies (implement Redis caching)\n4. Enhance mobile interface (use responsive design principles)\n5. Implement accessibility standards (follow WCAG 2.1 guidelines)\n6. Use Next.js v13.4.0 for server-side rendering and improved performance\n7. Implement service workers for offline capabilities\n8. Use Web Vitals library to measure and optimize Core Web Vitals\n9. Implement skeleton screens for perceived performance improvement\n10. Use React Suspense for code-splitting and lazy loading\n11. Optimize images using next/image component\n12. Implement virtualized lists for large datasets",
        "testStrategy": "1. Conduct performance testing using Lighthouse\n2. Measure and validate improvements in page load times\n3. Test mobile responsiveness on various devices\n4. Perform accessibility audit using axe-core\n5. Validate search and filtering performance improvements\n6. Test offline capabilities\n7. Conduct user testing for UX improvements\n8. Verify Core Web Vitals metrics",
        "priority": "medium",
        "dependencies": [
          10
        ],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": 12,
        "title": "Develop V2 Admin Run Processing Interface",
        "description": "Create an admin interface for managing v2 processing runs, including run triggering, monitoring, and error handling with focus on comprehensive run details pages.",
        "status": "deferred",
        "dependencies": [
          9,
          11
        ],
        "priority": "high",
        "details": "1. Build admin interface for v2 run management using Next.js 15 App Router architecture\n2. Implement comprehensive run details pages with real-time status updates\n3. Add monitoring dashboard with Supabase real-time subscriptions\n4. Create error investigation tools with detailed processing information display\n5. Build source configuration interface\n6. Implement real-time updates using Supabase realtime subscriptions\n7. Use Next.js 15 server components and client components appropriately\n8. Implement role-based access control\n9. Create detailed logging system integrated with Supabase\n10. Implement audit trail for admin actions\n11. Use modern form handling with React Hook Form\n12. Implement responsive design with TailwindCSS",
        "testStrategy": "1. Perform unit and integration tests for admin interface components\n2. Test run details page functionality and real-time updates\n3. Validate monitoring dashboard accuracy with Supabase integration\n4. Test error handling and investigation tools\n5. Verify source configuration management\n6. Conduct user acceptance testing with admin users\n7. Test Supabase real-time subscription functionality\n8. Verify role-based access control with Next.js middleware",
        "subtasks": [
          {
            "id": 1,
            "title": "Set up Admin Interface Foundation with Next.js 15 App Router",
            "description": "Create the core admin interface structure using Next.js 15 App Router architecture with Supabase authentication and routing foundation.",
            "status": "pending",
            "dependencies": [],
            "details": "Set up Next.js 15 App Router admin interface with TailwindCSS and shadcn/ui components. Implement Supabase authentication with role-based access control. Create protected admin routes using Next.js middleware. Set up base admin layout with navigation, theme toggle, and responsive design. Initialize Supabase client utilities for admin operations.",
            "testStrategy": "Unit tests for authentication components, integration tests for protected routes, and accessibility testing for the interface"
          },
          {
            "id": 2,
            "title": "Build Comprehensive Run Details Page with Real-time Updates",
            "description": "Create detailed individual run view pages with real-time status monitoring, processing information display, and error handling capabilities.",
            "status": "done",
            "dependencies": [
              1
            ],
            "details": "Develop run details page using Next.js 15 App Router with dynamic routing (/admin/runs/[id]). Implement Supabase real-time subscriptions for live run status updates. Create comprehensive run information display including processing stages, agent logs, and performance metrics. Build error investigation interface with detailed error logs and context. Add run control capabilities (cancel, retry, pause/resume). Implement processing timeline visualization and progress indicators.\n<info added on 2025-08-01T22:22:29.442Z>\nCOMPLETED: Successfully implemented comprehensive V2 runs details page at /app/admin/funding-sources/runs/[id]/pageV2.jsx featuring V2 optimization metrics dashboard with token/time savings visualization and efficiency scoring, 7-stage pipeline visualization with real-time progress indicators and stage-by-stage status tracking, Supabase real-time subscriptions for live run and stage updates, enhanced tabbed interface with Pipeline Details, Optimization metrics, Stage Details, and Raw Data views, backwards compatibility with automatic V1 fallback when V2 data unavailable, and detailed stage performance tracking including token usage, API calls, execution times, and expandable JSON result views.\n</info added on 2025-08-01T22:22:29.442Z>",
            "testStrategy": "Component testing for run details interface, Supabase real-time subscription testing, and end-to-end tests for run monitoring functionality"
          },
          {
            "id": 3,
            "title": "Create Run Management Dashboard with Supabase Integration",
            "description": "Build the main dashboard for viewing, triggering, and monitoring v2 processing runs with Supabase data persistence.",
            "status": "pending",
            "dependencies": [
              1
            ],
            "details": "Develop dashboard using Next.js 15 server and client components with Supabase queries. Create run listing with filtering, sorting, and pagination using Supabase database functions. Implement run triggering interface with source selection and parameter configuration. Build run status overview with real-time updates via Supabase subscriptions. Add bulk run operations and batch processing capabilities.",
            "testStrategy": "Integration testing with Supabase queries, real-time subscription testing, and dashboard performance testing"
          },
          {
            "id": 4,
            "title": "Implement Source Configuration Interface",
            "description": "Build comprehensive interface for managing funding source configurations with Supabase CRUD operations and validation.",
            "status": "pending",
            "dependencies": [
              1
            ],
            "details": "Create source management interface using React Hook Form for form handling. Build forms for adding, editing, and deleting funding sources with Supabase integration. Implement source configuration validation with Zod schemas. Add source testing capabilities to verify API connectivity. Create source status monitoring and health checks. Implement bulk source operations with optimistic updates.",
            "testStrategy": "Form validation testing, Supabase CRUD operation tests, and source configuration validation testing"
          },
          {
            "id": 5,
            "title": "Develop Monitoring and Analytics Dashboard",
            "description": "Create comprehensive monitoring interface for analyzing run performance, system health, and processing analytics.",
            "status": "pending",
            "dependencies": [
              2,
              3
            ],
            "details": "Build monitoring dashboard using Next.js 15 with Supabase analytics queries. Create performance metrics visualization with charts and graphs. Implement system health monitoring with real-time status indicators. Add processing analytics including success rates, processing times, and error patterns. Create alerting interface for run failures and performance issues. Build audit trail interface for tracking admin actions.",
            "testStrategy": "Analytics query testing, real-time monitoring validation, and performance metrics accuracy testing"
          }
        ]
      },
      {
        "id": 13,
        "title": "Implement Source Management V2",
        "description": "Develop an enhanced source management system with CRUD operations, API configuration management, and performance monitoring.",
        "details": "1. Build source management interface (use React v18.2.0 and Material-UI v5.13.0)\n2. Implement configuration validation\n3. Add API testing capabilities (use Postman API v10.14.0 for testing)\n4. Create performance monitoring tools (use Prometheus v2.40.0)\n5. Build bulk operation workflows\n6. Implement version control for source configurations (use Git-like versioning)\n7. Create visual API response mapper\n8. Implement OAuth 2.0 for secure API connections\n9. Use TypeScript for type-safe development\n10. Implement database migrations for schema changes (use Prisma Migrate)\n11. Create comprehensive documentation for each source type\n12. Implement alerting system for source issues (use Alertmanager v0.25.0)",
        "testStrategy": "1. Perform CRUD operation testing for sources\n2. Validate API configuration management\n3. Test performance monitoring accuracy\n4. Verify bulk operation functionality\n5. Test version control for configurations\n6. Validate OAuth 2.0 implementation\n7. Conduct user acceptance testing\n8. Test alerting system for various scenarios",
        "priority": "high",
        "dependencies": [
          12
        ],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": 14,
        "title": "Develop Lightweight User Identification System",
        "description": "Implement a simple user identification system with pre-populated user database, client-side token storage, and admin user management.",
        "details": "1. Implement user identification system as per specifications\n2. Create user selection interface (use React v18.2.0 and Material-UI v5.13.0)\n3. Build token management system (use JWT for token generation)\n4. Add admin user management panel\n5. Implement role-based permissions\n6. Use localStorage for client-side token storage\n7. Implement server-side session management (use express-session v1.17.3)\n8. Create user database schema and migrations\n9. Implement user activity logging\n10. Use bcrypt v5.1.0 for password hashing (for admin users)\n11. Implement GDPR compliance measures\n12. Create user onboarding workflow",
        "testStrategy": "1. Test user selection and identification process\n2. Validate token management and storage\n3. Verify admin user management functionality\n4. Test role-based permissions\n5. Conduct security testing for token handling\n6. Verify user activity logging\n7. Test GDPR compliance measures\n8. Perform user acceptance testing",
        "priority": "high",
        "dependencies": [
          11,
          13
        ],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": 15,
        "title": "Implement User Activity Tracking",
        "description": "Develop a system to track user interactions with opportunities, including favorite actions and status changes.",
        "details": "1. Create user actions tracking system\n2. Build activity logging infrastructure (use Winston v3.9.0 for logging)\n3. Implement user-specific data views\n4. Add analytics and reporting capabilities (use Chart.js v4.3.0 for visualizations)\n5. Create user activity dashboards\n6. Implement real-time activity updates (use WebSockets with socket.io v4.6.0)\n7. Use Redis v6.0.0 for caching frequently accessed user data\n8. Implement data aggregation for analytics (use Apache Spark v3.4.0)\n9. Create export functionality for activity reports\n10. Implement privacy controls for user data\n11. Use Elasticsearch v8.8.0 for efficient activity search\n12. Implement activity notifications system",
        "testStrategy": "1. Test user interaction tracking accuracy\n2. Validate activity logging system\n3. Verify user-specific data view functionality\n4. Test analytics and reporting features\n5. Validate real-time update functionality\n6. Perform load testing on activity tracking system\n7. Test data privacy controls\n8. Verify activity search functionality",
        "priority": "medium",
        "dependencies": [
          14
        ],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": 16,
        "title": "Develop Three-Status System",
        "description": "Implement a three-status system (None/Hot/Of Interest) for opportunity management with admin-only Hot status designation.",
        "details": "1. Implement database schema for status management\n2. Create status change logic and validation\n3. Build status tracking and logging system\n4. Add UI components for status display (use React v18.2.0 and Material-UI v5.13.0)\n5. Implement status-based filtering\n6. Create admin interface for Hot status designation\n7. Implement user favoriting system for Of Interest status\n8. Use Redux Toolkit v1.9.5 for state management\n9. Implement WebSocket (socket.io v4.6.0) for real-time status updates\n10. Create database indexes for efficient status-based queries\n11. Implement status change notifications\n12. Develop status analytics dashboard",
        "testStrategy": "1. Test status change functionality for all user types\n2. Validate admin-only Hot status designation\n3. Verify user favoriting system\n4. Test status-based filtering accuracy\n5. Validate real-time status updates\n6. Perform database query optimization tests\n7. Test notification system for status changes\n8. Verify status analytics accuracy",
        "priority": "high",
        "dependencies": [
          14,
          15
        ],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": 17,
        "title": "Create Hot Board and Of Interest Board",
        "description": "Develop Hot Board as the primary sales interface and Of Interest Board, combining Hot and Of Interest opportunities with visual distinctions.",
        "details": "1. Build Hot Board interface (use React v18.2.0 and Material-UI v5.13.0)\n2. Create Of Interest Board\n3. Implement status-based filtering\n4. Add action buttons for status changes\n5. Build team collaboration features\n6. Implement drag-and-drop functionality (use react-beautiful-dnd v13.1.1)\n7. Create visual distinctions between status types\n8. Implement real-time updates (use WebSockets with socket.io v4.6.0)\n9. Add sorting and advanced filtering options\n10. Implement board customization features\n11. Create board sharing functionality\n12. Develop board analytics and reporting",
        "testStrategy": "1. Test Hot Board and Of Interest Board functionality\n2. Validate status-based filtering and sorting\n3. Verify status change actions\n4. Test team collaboration features\n5. Validate drag-and-drop functionality\n6. Verify real-time update system\n7. Test board customization features\n8. Conduct user acceptance testing with sales team",
        "priority": "high",
        "dependencies": [
          16
        ],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": 18,
        "title": "Implement Notification System",
        "description": "Develop an email notification system for Hot opportunities with territory-based routing, deadline reminders, and configurable preferences.",
        "details": "1. Implement email notification system (use Nodemailer v6.9.0)\n2. Create notification templates (use Handlebars v4.7.0 for templating)\n3. Build territory-based routing logic\n4. Add deadline reminder functionality\n5. Implement status change notifications\n6. Create notification preference management\n7. Implement notification scheduling (use node-cron v3.0.2)\n8. Use Redis v6.0.0 for notification queue management\n9. Implement rate limiting for notifications\n10. Create notification analytics dashboard\n11. Implement multi-channel notifications (email, in-app, SMS)\n12. Develop A/B testing for notification content",
        "testStrategy": "1. Test email notification delivery and content\n2. Validate territory-based routing accuracy\n3. Verify deadline reminder functionality\n4. Test status change notification system\n5. Validate user preference management\n6. Perform load testing on notification system\n7. Test rate limiting functionality\n8. Verify multi-channel notification delivery",
        "priority": "medium",
        "dependencies": [
          16,
          17
        ],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": 19,
        "title": "Create Real Data Test Script for Analysis Agent",
        "description": "Update the existing test script to validate the Analysis Agent with real funding source data, focusing on compatibility with the new implementation, scoring system updates, and pipeline integration verification.",
        "status": "done",
        "dependencies": [
          3
        ],
        "priority": "high",
        "details": "1. Update the existing 03-test-analysis-agent.js script to work with the new Analysis Agent implementation\n2. Modify validation checks to match the new scoring system and schema changes\n3. Update test data handling to accommodate new scoring fields and output format\n4. Fix any compatibility issues with the updated Analysis Agent interface\n5. Run comprehensive tests using real funding source data samples\n6. Verify opportunity enhancement functionality works correctly with new implementation\n7. Test scoring accuracy and consistency with the updated scoring system\n8. Validate analysis metrics generation produces expected outputs\n9. Verify pipeline compatibility and seamless integration with existing workflow\n10. Document any issues found during testing and their resolutions\n11. Create test result summary report highlighting key findings\n12. Validate data anonymization still works effectively with new data structures\n13. Test edge cases specific to the new scoring system implementation\n14. Verify error handling works properly with updated Analysis Agent\n15. Confirm test script runs successfully end-to-end with real data",
        "testStrategy": "1. Execute updated test script against Analysis Agent with real funding source data samples\n2. Validate opportunity enhancement accuracy with new implementation\n3. Verify scoring system updates work correctly and produce consistent results\n4. Test analysis metrics generation matches expected new output format\n5. Validate pipeline compatibility and workflow integration\n6. Verify all validation checks pass with new scoring fields and schema\n7. Test edge cases and error handling with updated Analysis Agent\n8. Document test results and any compatibility issues discovered\n9. Confirm data anonymization effectiveness with new data structures\n10. Validate end-to-end functionality from data input to final output\n11. Compare results against expected behavior with new scoring system\n12. Verify test script stability and reliability with real data processing",
        "subtasks": [
          {
            "id": 1,
            "title": "Update Test Script Schema and Validation",
            "description": "Update the existing 03-test-analysis-agent.js script to match the new Analysis Agent implementation - fix scoring field names, update validation checks, and modify display functions to work with the new schema",
            "details": "1. Update scoring field references from old system (projectTypeMatch, clientTypeMatch, categoryMatch, scoringExplanation) to new system (clientProjectRelevance, fundingAttractiveness, fundingType, relevanceReasoning)\n2. Fix validation checks to match new 3-component scoring system (0-6, 0-3, 0-1) \n3. Update displayEnhancedOpportunity function to show new scoring breakdown\n4. Modify validation logic to check for new required fields\n5. Update score distribution calculations to match new ranges\n6. Fix any imports or function calls that changed in the new Analysis Agent\n<info added on 2025-06-22T00:51:29.363Z>\nPROGRESS UPDATE - Schema and Validation Updates Complete:\n\nSuccessfully updated all scoring field references and validation logic to match new Analysis Agent system. Key changes implemented:\n\n- Replaced old 4-field scoring system (projectTypeMatch, clientTypeMatch, categoryMatch, scoringExplanation) with new 3-component system (clientProjectRelevance 0-6, fundingAttractiveness 0-3, fundingType 0-1, relevanceReasoning)\n- Updated displayEnhancedOpportunity function to show new scoring breakdown format\n- Modified all validation checks to enforce new field requirements and score ranges\n- Adjusted score distribution display ranges: High 7-10, Medium 4-6, Low 0-3\n- Fixed Analysis Agent import path for schemas\n- Test script now loads and runs successfully with updated schema\n\nStatus: Schema updates complete and validated. Test script ready for execution pending environment variable configuration (ANTHROPIC_API_KEY, Supabase credentials) needed for subtask 19.2.\n</info added on 2025-06-22T00:51:29.363Z>",
            "status": "done",
            "dependencies": [],
            "parentTaskId": 19
          },
          {
            "id": 2,
            "title": "Run and Debug Real Data Tests",
            "description": "Execute the updated test script with real funding data, identify and fix any issues, and verify that the Analysis Agent works correctly with the new implementation",
            "details": "1. Run the updated test script against real funding opportunities\n2. Debug any errors or compatibility issues that arise\n3. Verify that enhanced descriptions and actionable summaries are generated correctly\n4. Check that scoring calculations work as expected with the new system\n5. Validate that analysis metrics are calculated properly\n6. Ensure all validation checks pass with real data\n7. Test edge cases and error handling scenarios\n8. Verify performance is acceptable with real data volumes\n<info added on 2025-06-22T04:36:44.821Z>\nCOMPLETED: Successfully ran and debugged real data tests\n\nFIXED ENVIRONMENT ISSUE: \n- Root cause: Test script was using wrong relative path for .env.local file\n- Solution: Changed from '.env.local' to '../../.env.local' in dotenv.config()\n- Result: ANTHROPIC_API_KEY now loads correctly (108 characters)\n\nSUCCESSFUL TEST EXECUTION:\n- Both California and Grants.gov sources analyzed successfully  \n- All validation checks PASSED (12/12)\n- Enhanced descriptions and actionable summaries generated correctly\n- New scoring system working: clientProjectRelevance (0-6), fundingAttractiveness (0-3), fundingType (0-1)\n- Analysis metrics calculated properly\n- Performance within acceptable range (~20 seconds per batch)\n\nVALIDATED NEW ANALYSIS AGENT:\n- Schema updates working correctly\n- Field names all aligned (relevanceReasoning vs scoringExplanation)\n- Score distributions calculated with new ranges (High: 7-10, Medium: 4-6, Low: 0-3)\n- Pipeline integration confirmed - ready for Stage 4\n\nSTATUS: Real data testing complete and successful. Analysis Agent v2 fully validated with production-ready performance.\n</info added on 2025-06-22T04:36:44.821Z>",
            "status": "done",
            "dependencies": [],
            "parentTaskId": 19
          }
        ]
      },
      {
        "id": 20,
        "title": "Implement Client Advantage Analysis for Opportunities",
        "description": "Develop a comprehensive client advantage analysis system that evaluates opportunities based on client-specific criteria, competitive positioning, and success probability scoring. This system will integrate with the Data Extraction Agent to populate a 'client_advantage_analysis' database field with specific advantage insights for preferred clients.",
        "status": "pending",
        "dependencies": [
          1
        ],
        "priority": "high",
        "details": "1. Integrate with Data Extraction Agent (Task 1) to add 'client_advantage_analysis' field to opportunities database\n2. Design optimal data storage structure for client advantage analysis that supports:\n   - Structured JSON format for flexible advantage categories\n   - Efficient querying for opportunity detail page display\n   - Scalable storage for multiple client advantage scenarios\n3. Implement data processing logic within Data Extraction Agent to:\n   - Analyze opportunity requirements against preferred client capabilities\n   - Generate specific use case examples for client advantages\n   - Extract competitive positioning insights from opportunity data\n4. Create client advantage scoring algorithm that evaluates opportunities based on:\n   - Client's historical success rates with similar opportunities\n   - Geographic proximity and regional presence\n   - Industry expertise alignment\n   - Resource capacity and capability matching\n   - Competitive landscape analysis\n5. Implement opportunity scoring service (use TypeScript for type safety)\n6. Build client profile management system to store:\n   - Company capabilities and expertise areas\n   - Geographic coverage and regional strengths\n   - Past performance metrics and success rates\n   - Resource availability and capacity\n7. Create competitive analysis module that:\n   - Identifies potential competitors for each opportunity\n   - Analyzes market positioning and competitive advantages\n   - Calculates win probability based on competitive factors\n8. Implement machine learning model for predictive scoring (use TensorFlow.js v4.8.0)\n9. Build advantage visualization dashboard (use D3.js v7.8.0 for data visualization)\n10. Create recommendation engine that suggests:\n    - High-advantage opportunities for specific clients\n    - Strategic partnerships or teaming arrangements\n    - Capability gaps to address for better positioning\n11. Implement real-time scoring updates when opportunity details change\n12. Use Redis v6.0.0 for caching calculated scores and analysis results\n13. Create API endpoints for advantage analysis integration with existing dashboard\n14. Implement batch processing for large-scale opportunity analysis\n15. Add configurable scoring weights and criteria customization\n16. Create detailed reporting and analytics for advantage trends\n17. Implement data export capabilities for client presentations\n18. Optimize database queries for opportunity detail page performance\n19. Create specific use case generation logic for client advantage examples",
        "testStrategy": "1. Test integration with Data Extraction Agent for 'client_advantage_analysis' field population\n2. Validate database structure performance for opportunity detail page queries\n3. Test advantage scoring algorithm accuracy with historical opportunity data\n4. Validate client profile management CRUD operations\n5. Test competitive analysis accuracy against known market data\n6. Verify machine learning model predictions with test datasets\n7. Test real-time scoring updates when opportunity data changes\n8. Validate API endpoint functionality and response formats\n9. Perform load testing on batch processing capabilities\n10. Test dashboard visualization accuracy and performance\n11. Verify recommendation engine suggestions against business logic\n12. Test data export functionality and format accuracy\n13. Conduct user acceptance testing with sample client profiles\n14. Validate caching performance and data consistency\n15. Test scoring customization and weight adjustment features\n16. Perform integration testing with existing opportunity management system\n17. Test opportunity detail page load times with client advantage data\n18. Validate specific use case generation accuracy and relevance",
        "subtasks": []
      },
      {
        "id": 21,
        "title": "Update Filter Function to use New Gating System Scoring",
        "description": "Refactor the filter function to use the new gating system scoring structure from the Analysis Agent, replacing the old scoring system with clientProjectRelevance, fundingAttractiveness, and fundingType metrics.",
        "details": "1. Update filter function to use new scoring structure:\n   - Replace projectTypeMatch/clientTypeMatch with clientProjectRelevance (0-6 scale)\n   - Replace overallScore with fundingAttractiveness (0-3 scale)\n   - Update fundingType to use 0-1 binary scoring instead of string matching\n   - Remove fundingThreshold parameter as it's replaced by gating logic\n\n2. Implement new gating logic:\n   - Primary gate: clientProjectRelevance ≥ 2 (minimum to pass filter)\n   - Auto-qualification gate: clientProjectRelevance ≥ 5 (bypasses other checks)\n   - Secondary filtering based on fundingAttractiveness and fundingType scores\n\n3. Update filter function signature and parameters:\n   - Modify function to accept new scoring object structure\n   - Update TypeScript interfaces for new scoring system\n   - Ensure backward compatibility during transition period\n\n4. Implement scoring validation:\n   - Add input validation for score ranges (0-6, 0-3, 0-1)\n   - Handle edge cases where scores are missing or invalid\n   - Add logging for filtering decisions and score thresholds\n\n5. Update filtering logic flow:\n   - First check: clientProjectRelevance ≥ 2 (fail if below)\n   - Auto-qualify if clientProjectRelevance ≥ 5\n   - For scores 2-4, apply additional filtering based on fundingAttractiveness and fundingType\n   - Implement weighted scoring for edge cases\n\n6. Performance considerations:\n   - Optimize filtering algorithm for large datasets\n   - Implement early exit conditions for failed gates\n   - Add caching for frequently used filter configurations\n\n7. Integration updates:\n   - Update all calling functions to pass new scoring structure\n   - Modify database queries to work with new scoring fields\n   - Update API responses to reflect new filtering results\n<info added on 2025-06-22T05:20:49.589Z>\n8. COMPLETED IMPLEMENTATION:\n   - Filter function successfully updated to use new gating system scoring structure\n   - Simplified logic removes unnecessary complexity (strict/lenient filtering, grant preference, quality requirements)\n   - Implements clean 3-stage filtering process: Primary Gate (clientProjectRelevance ≥ 2) → Auto-Qualification (clientProjectRelevance ≥ 5) → Secondary Filtering (status check + fundingAttractiveness ≥ 1)\n   - Enhanced logging and metrics tracking added for filtering decisions\n\n9. Test suite updates completed:\n   - Updated existing test suite to align with new scoring system\n   - Removed tests for deprecated functionality\n   - Added comprehensive validation for new gating logic\n   - Created new real data integration test script (scripts/test/04-test-filter-function.js) that uses actual Analysis Agent output from Stage 3\n   - Integration test validates filtering with multiple configurations using real analyzed opportunities\n\n10. Ready for validation:\n    - All changes implemented and tested\n    - Filter function now fully integrated with new gating system\n    - Can be tested using: node scripts/test/04-test-filter-function.js\n</info added on 2025-06-22T05:20:49.589Z>",
        "testStrategy": "1. Unit Testing:\n   - Test filter function with various score combinations across all ranges\n   - Verify gating logic: scores below 2 are rejected, scores ≥5 auto-qualify\n   - Test edge cases: missing scores, invalid ranges, null values\n   - Validate backward compatibility with old scoring system during transition\n\n2. Integration Testing:\n   - Test filter function integration with Analysis Agent output\n   - Verify correct filtering results with real opportunity data\n   - Test performance with large datasets (1000+ opportunities)\n   - Validate filtering accuracy against expected business rules\n\n3. Scoring Validation Tests:\n   - Test clientProjectRelevance scoring: 0-6 range validation\n   - Test fundingAttractiveness scoring: 0-3 range validation  \n   - Test fundingType binary scoring: 0-1 validation\n   - Verify proper error handling for out-of-range scores\n\n4. Gating Logic Tests:\n   - Test primary gate: opportunities with clientProjectRelevance < 2 are filtered out\n   - Test auto-qualification: opportunities with clientProjectRelevance ≥ 5 pass regardless of other scores\n   - Test intermediate scoring: opportunities with scores 2-4 are evaluated based on additional criteria\n   - Verify weighted scoring calculations for edge cases\n\n5. Performance Testing:\n   - Benchmark filtering speed with new vs old scoring system\n   - Test memory usage with large opportunity datasets\n   - Validate early exit conditions reduce processing time\n   - Test caching effectiveness for repeated filter operations\n\n6. End-to-End Testing:\n   - Test complete pipeline: Analysis Agent → Filter Function → Results\n   - Verify filtered results match expected business outcomes\n   - Test with various opportunity types and funding sources\n   - Validate logging and metrics collection for filtering decisions",
        "status": "done",
        "dependencies": [
          3
        ],
        "priority": "high",
        "subtasks": []
      },
      {
        "id": 22,
        "title": "Create Comprehensive Test Script for Updated Filter Function with New Scoring System",
        "description": "Develop a comprehensive test script that validates the updated Filter Function using real data and the new dual scoring system (clientRelevance + projectRelevance), building on the existing test script in scripts/test/03-test-analysis-agent.js to ensure proper filtering based on the new scoring thresholds, updated funding attractiveness criteria, and grant preferences.",
        "status": "deferred",
        "dependencies": [
          19,
          21
        ],
        "priority": "high",
        "details": "1. Extend existing test script at scripts/test/03-test-analysis-agent.js to include comprehensive filter function testing with new dual scoring system\n2. Create test data sets covering all new scoring scenarios:\n   - clientRelevance scores: 0-3 range based on eligible applicants\n   - projectRelevance scores: 0-3 range based on eligible activities\n   - Combined scoring scenarios with maximum 10 points total (6 from relevance + up to 4 from other factors)\n   - Updated fundingAttractiveness thresholds: 50M+/5M+ exceptional, 25M+/2M+ strong, 10M+/1M+ moderate\n   - fundingType binary scores: 0 and 1 values\n   - Grant preference matching scenarios with preferred activities weighting\n\n3. Implement new gating system validation tests:\n   - Test filtering logic with new scoring ranges (0-10 total vs previous system)\n   - Validate research opportunity filtering with low projectRelevance scores\n   - Test preferred activities weighting in filtering decisions\n   - Verify proper handling of clientRelevance vs projectRelevance balance\n\n4. Create edge case test scenarios:\n   - Missing or null scoring values for new dual scoring fields\n   - Invalid score ranges for clientRelevance and projectRelevance (negative numbers, values exceeding 3)\n   - Transition scenarios from old clientProjectRelevance to new dual scoring\n   - Malformed opportunity data structures with new scoring fields\n   - Empty result sets and boundary conditions with new thresholds\n\n5. Implement performance validation:\n   - Test with large datasets (1000+ opportunities) to validate filtering performance with new scoring calculations\n   - Memory usage monitoring during dual scoring filtering operations\n   - Execution time benchmarking comparing old vs new scoring system performance\n\n6. Build real data integration:\n   - Use anonymized real funding opportunity data from Task 19's data collection system\n   - Validate filtering accuracy against known opportunities using new dual scoring criteria\n   - Test research opportunity filtering effectiveness with projectRelevance scores\n   - Test with diverse funding types, agencies, and opportunity structures under new scoring\n\n7. Create comprehensive assertion framework:\n   - Validate filtered result counts match expected outcomes with new scoring thresholds\n   - Verify dual score-based sorting and ranking (clientRelevance + projectRelevance)\n   - Confirm research opportunities are properly filtered with low projectRelevance\n   - Test preferred activities weighting impact on filtering results\n   - Validate new funding attractiveness threshold application\n\n8. Implement regression testing:\n   - Compare results between old clientProjectRelevance and new dual scoring system\n   - Ensure smooth transition from previous filter function behavior\n   - Validate that new scoring improvements maintain filtering quality\n   - Test backward compatibility during scoring system transition\n\n9. Use Jest v29.5.0 testing framework with custom matchers for dual scoring validation\n10. Implement test data factories using faker.js v8.0.2 for generating varied dual scoring test scenarios",
        "testStrategy": "1. Execute comprehensive test suite covering all dual scoring combinations (clientRelevance 0-3, projectRelevance 0-3) and new gating scenarios\n2. Validate new filtering logic with updated scoring ranges (maximum 10 points total)\n3. Test research opportunity filtering: confirm opportunities with low projectRelevance scores are appropriately filtered\n4. Verify preferred activities weighting functionality in filtering decisions\n5. Test new funding attractiveness thresholds (50M+/5M+ exceptional, 25M+/2M+ strong, 10M+/1M+ moderate)\n6. Run edge case tests with malformed dual scoring data, missing clientRelevance/projectRelevance values, and boundary conditions\n7. Execute performance tests comparing old vs new scoring system with datasets of varying sizes (100, 500, 1000+ opportunities)\n8. Validate filtering accuracy using real anonymized funding data from Task 19 with new dual scoring criteria\n9. Conduct regression testing to ensure smooth transition from clientProjectRelevance to dual scoring system\n10. Verify test coverage reaches 95%+ for all new dual scoring filter function code paths\n11. Run load testing to ensure filter performance under concurrent usage with new scoring calculations\n12. Validate memory usage remains within acceptable limits during dual scoring processing of large datasets\n13. Execute integration tests with the complete Analysis Agent pipeline using new dual scoring system\n14. Test filtering consistency between old and new scoring systems during transition period",
        "subtasks": []
      },
      {
        "id": 23,
        "title": "Review Analysis Metrics Implementation",
        "description": "Examine the analysis metrics calculation in the Analysis Agent to determine if it's an alternate implementation of what the run manager was doing in v1 and should be doing in v2. Compare functionality and determine if consolidation or coordination is needed.",
        "details": "1. Conduct comprehensive code review of Analysis Agent metrics implementation (examine current 435-line implementation)\n2. Analyze v1 run manager metrics functionality and identify overlapping responsibilities\n3. Document current metrics being calculated by Analysis Agent:\n   - Opportunity scoring metrics\n   - Content enhancement statistics\n   - Processing performance metrics\n   - Token usage tracking\n   - API call frequency and success rates\n4. Compare with v1 run manager metrics to identify:\n   - Duplicate functionality\n   - Missing capabilities\n   - Inconsistent calculations\n   - Performance implications\n5. Create detailed comparison matrix showing:\n   - Metric types calculated by each system\n   - Data sources used\n   - Calculation methodologies\n   - Storage and retrieval patterns\n6. Evaluate architectural implications:\n   - Single responsibility principle adherence\n   - Data flow efficiency\n   - Maintenance complexity\n   - Testing coverage gaps\n7. Develop recommendations for:\n   - Consolidation opportunities (merge duplicate functionality)\n   - Coordination mechanisms (if separate systems needed)\n   - Migration strategy for existing metrics\n   - Performance optimization opportunities\n8. Design unified metrics architecture if consolidation is recommended:\n   - Central metrics collection service\n   - Standardized metric definitions\n   - Consistent data formats and storage\n   - Real-time vs batch processing decisions\n9. Create implementation plan for chosen approach:\n   - Code refactoring requirements\n   - Database schema changes\n   - API modifications\n   - Migration scripts for historical data\n10. Document impact on dependent systems and update integration points",
        "testStrategy": "1. Create comprehensive test suite comparing metrics output between Analysis Agent and v1 run manager using identical input data\n2. Validate metric calculation accuracy by cross-referencing results with manual calculations\n3. Test performance impact of current vs proposed metrics implementation using benchmark scenarios\n4. Verify data consistency across all metric collection points and storage systems\n5. Conduct integration testing to ensure metrics consolidation doesn't break dependent dashboard and reporting systems\n6. Test migration scripts with historical data to ensure no data loss or corruption\n7. Validate real-time metrics updates and batch processing scenarios\n8. Perform load testing on unified metrics system to ensure scalability\n9. Test rollback procedures in case consolidation needs to be reverted\n10. Conduct user acceptance testing with stakeholders who rely on metrics data for decision making",
        "status": "pending",
        "dependencies": [
          3
        ],
        "priority": "medium",
        "subtasks": []
      },
      {
        "id": 24,
        "title": "Comprehensive V1 vs V2 Functionality Audit",
        "description": "Conduct a thorough comparison between v1 and v2 agent implementations to ensure no functionality has been lost in the transition, documenting any differences and ensuring feature parity where intended.",
        "details": "1. Create comprehensive functionality mapping between v1 and v2 systems:\n   - Map v1 apiHandlerAgent.js (2,249 lines) to Data Extraction Agent functionality\n   - Compare v1 detailProcessorAgent.js (815 lines) with Analysis Agent implementation (435 lines)\n   - Analyze v1 dataProcessorAgent.js (1,049 lines) against Storage Agent functionality\n   - Document v1 run manager metrics vs Analysis Agent metrics implementation\n\n2. Implement automated comparison framework:\n   - Create side-by-side execution environment for v1 and v2 agents\n   - Use identical input datasets for both versions\n   - Implement result comparison utilities with configurable tolerance levels\n   - Build automated reporting system for functionality gaps\n\n3. Conduct detailed feature analysis:\n   - API handling capabilities: pagination, retry logic, error handling patterns\n   - Data processing workflows: field mapping, sanitization, validation\n   - Scoring algorithms: compare old vs new scoring systems (projectTypeMatch/clientTypeMatch vs clientProjectRelevance)\n   - Content enhancement: analyze prompt effectiveness and output quality\n   - Database operations: duplicate detection, state eligibility, change tracking\n\n4. Performance and efficiency comparison:\n   - Token usage analysis between v1 and v2 implementations\n   - Processing time benchmarks for equivalent operations\n   - Memory usage patterns and optimization improvements\n   - API call frequency and batching efficiency\n\n5. Create comprehensive audit documentation:\n   - Functionality parity matrix with pass/fail status\n   - Performance improvement metrics and regression analysis\n   - Missing feature identification and impact assessment\n   - Recommendation report for addressing gaps or confirming intentional changes\n\n6. Implement gap resolution tracking:\n   - Prioritize missing functionality by business impact\n   - Create implementation roadmap for critical gaps\n   - Document intentional feature removals and their justifications\n   - Establish ongoing monitoring for feature parity maintenance",
        "testStrategy": "1. Execute parallel testing framework with identical datasets on both v1 and v2 systems, comparing outputs across all major functionality areas including API handling, data processing, and scoring algorithms\n\n2. Validate functionality parity by running comprehensive test suites covering edge cases, error scenarios, and performance benchmarks, ensuring v2 meets or exceeds v1 capabilities in all critical areas\n\n3. Conduct regression testing using historical production data to identify any functionality gaps or performance degradations, with automated reporting of discrepancies exceeding defined tolerance thresholds\n\n4. Perform user acceptance testing with stakeholders to validate that business requirements are met by v2 implementation and that no critical workflows have been disrupted\n\n5. Execute load testing scenarios comparing v1 and v2 performance under various conditions, validating that efficiency improvements don't come at the cost of functionality loss\n\n6. Review audit documentation with development team and stakeholders to confirm gap resolution plans and obtain sign-off on intentional feature changes or removals",
        "status": "pending",
        "dependencies": [
          1,
          2,
          3,
          4,
          7,
          23
        ],
        "priority": "medium",
        "subtasks": []
      },
      {
        "id": 25,
        "title": "Agent Performance Optimization and Caching",
        "description": "Implement comprehensive performance optimization across all agents including Redis caching, token usage optimization, and performance profiling to enhance system efficiency and reduce operational costs.",
        "details": "1. Implement Redis caching layer (use Redis v7.0.0) for:\n   - API response caching with configurable TTL\n   - Database query result caching\n   - Session and authentication token caching\n   - Frequently accessed opportunity data caching\n2. Optimize token usage across all agents:\n   - Implement token pooling and rate limiting\n   - Add intelligent prompt optimization to reduce token consumption\n   - Create token usage analytics and monitoring dashboard\n   - Implement context window optimization for large documents\n3. Performance profiling implementation:\n   - Integrate APM tools (use New Relic v9.0.0 or Datadog v5.0.0)\n   - Add custom performance metrics collection\n   - Implement memory leak detection and monitoring\n   - Create performance bottleneck identification system\n4. Agent-specific optimizations:\n   - Data Extraction Agent: Implement connection pooling and batch processing\n   - Storage Agent: Optimize database queries with proper indexing\n   - Analysis agents: Implement result caching and parallel processing\n5. Infrastructure optimizations:\n   - Implement CDN for static assets (use CloudFlare)\n   - Add database connection pooling (use PgBouncer for PostgreSQL)\n   - Implement horizontal scaling capabilities\n   - Add load balancing for agent distribution\n6. Monitoring and alerting:\n   - Set up performance threshold alerts\n   - Create performance degradation detection\n   - Implement automated scaling triggers\n   - Add comprehensive logging for performance events\n7. Code-level optimizations:\n   - Implement lazy loading patterns\n   - Add memoization for expensive computations\n   - Optimize async/await patterns\n   - Implement efficient data structures and algorithms\n8. Cache invalidation strategies:\n   - Implement smart cache invalidation based on data changes\n   - Add cache warming strategies for critical data\n   - Create cache hit/miss ratio monitoring\n   - Implement distributed cache consistency",
        "testStrategy": "1. Performance baseline establishment:\n   - Measure current system performance metrics before optimization\n   - Document processing times, memory usage, and token consumption\n   - Establish performance benchmarks for each agent\n2. Cache effectiveness testing:\n   - Validate cache hit/miss ratios meet target thresholds (>80% hit rate)\n   - Test cache invalidation accuracy and timing\n   - Verify data consistency between cached and source data\n   - Load test cache performance under high concurrent access\n3. Token optimization validation:\n   - Measure token usage reduction (target: 25-40% reduction)\n   - Validate prompt optimization maintains output quality\n   - Test token pooling and rate limiting functionality\n   - Monitor token cost reduction in production environment\n4. Performance profiling verification:\n   - Validate APM tool integration and data accuracy\n   - Test performance alert triggers and thresholds\n   - Verify memory leak detection capabilities\n   - Validate bottleneck identification accuracy\n5. Agent performance testing:\n   - Conduct load testing on each optimized agent\n   - Measure processing time improvements (target: 40-60% improvement)\n   - Test memory usage optimization (target: 30-50% reduction)\n   - Validate concurrent processing capabilities\n6. Infrastructure optimization testing:\n   - Test CDN performance improvements for static assets\n   - Validate database connection pooling effectiveness\n   - Test horizontal scaling triggers and performance\n   - Verify load balancing distribution accuracy\n7. End-to-end performance validation:\n   - Execute comprehensive system performance tests\n   - Compare optimized vs. baseline performance metrics\n   - Validate system stability under optimized configuration\n   - Test performance monitoring and alerting systems",
        "status": "pending",
        "dependencies": [
          1,
          2,
          6,
          7,
          8
        ],
        "priority": "low",
        "subtasks": []
      },
      {
        "id": 26,
        "title": "Review and Refine Analysis Agent Output Quality",
        "description": "Conduct comprehensive review of Analysis Agent test results to improve scoring accuracy and sales team relevance, focusing on complete sample testing, research opportunity filtering, and sales-oriented output refinement. Includes critical scoring precision improvements to decouple applicant and project scoring, and integrate eligible activities analysis for more granular relevance assessment.",
        "status": "pending",
        "dependencies": [
          3,
          19,
          "30"
        ],
        "priority": "high",
        "details": "1. Execute comprehensive testing with complete dataset:\n   - Run Analysis Agent tests with ALL opportunities (not limited samples) to get statistically significant results\n   - Update test scripts to process full batches without artificial limitations\n   - Document performance metrics and processing times for complete dataset analysis\n   - Identify any bottlenecks or timeout issues when processing large volumes\n\n2. Fix scoring logic for research-focused opportunities:\n   - Review current scoring criteria and identify research-heavy opportunity patterns\n   - Implement scoring penalties for opportunities primarily focused on research activities\n   - Add keyword detection for research indicators (grants, studies, academic partnerships)\n   - Create scoring rules that downgrade opportunities outside core service capabilities\n   - Test scoring adjustments with known research opportunities to validate filtering\n\n3. Implement scoring precision improvements:\n   - Decouple applicant scoring from project scoring in clientProjectRelevance component\n   - Create separate scoring components for eligible applicants vs project types for granular control\n   - Design weighted scoring system that prioritizes eligible activities over broad project categories\n   - Implement activity-specific scoring boosts for service-aligned activities (equipment purchase, installation, HVAC upgrades)\n   - Add scoring penalties for non-service activities (research, studies, planning-only)\n   - Test decoupled scoring system to validate improved precision\n\n4. Integrate eligible activities data into scoring logic:\n   - Analyze current eligible activities data structure and availability\n   - Design integration points between eligible activities and opportunity scoring\n   - Implement scoring boosts for opportunities matching high-value eligible activities\n   - Create weighted scoring system that considers activity alignment with company capabilities\n   - Weight eligible activities more heavily than broad project type categories in scoring calculations\n   - Validate that eligible activities integration improves scoring relevance and precision\n\n5. Refine use case examples for client execution context:\n   - Review all current use case examples in prompts and scoring logic\n   - Rewrite examples to focus on company executing projects FOR clients rather than client self-execution\n   - Update prompt templates to emphasize service delivery perspective\n   - Create context-appropriate examples that align with company's service model\n   - Ensure all generated descriptions reflect proper client-vendor relationship\n\n6. Enhance sales team output relevance:\n   - Review all enhanced descriptions and actionable summaries for sales context\n   - Refine output templates to include sales-relevant information (decision makers, timelines, budget indicators)\n   - Update summary generation to highlight key selling points and engagement strategies\n   - Ensure technical details are translated into business value propositions\n   - Add sales-specific metadata fields to opportunity records\n\n7. Validate service capability filtering:\n   - Test scoring system with opportunities clearly outside service scope\n   - Verify that low-relevance opportunities receive appropriately low scores\n   - Implement capability matching logic that aligns opportunities with service offerings\n   - Create negative scoring factors for incompatible opportunity types\n   - Test edge cases where project types seem irrelevant but activities are aligned (e.g., security projects with retrofit activities)\n   - Document and test edge cases where opportunities might be borderline relevant\n\n8. Update scoring criteria and prompts based on findings:\n   - Analyze test results to identify scoring inconsistencies or gaps\n   - Refine scoring weights and thresholds based on sales team feedback requirements\n   - Update prompt engineering to improve output quality and consistency\n   - Implement iterative improvements based on comprehensive testing results\n   - Document all changes and create validation tests for new criteria",
        "testStrategy": "1. Comprehensive dataset testing validation:\n   - Execute Analysis Agent with complete opportunity dataset and measure processing success rate\n   - Compare results between limited sample testing and full dataset testing to identify differences\n   - Validate that all opportunities are processed without data loss or timeout issues\n   - Measure and document performance metrics for full dataset processing\n\n2. Research opportunity filtering verification:\n   - Create test dataset with known research-focused opportunities\n   - Validate that research opportunities receive appropriately low scores (below defined threshold)\n   - Test edge cases where opportunities have mixed research and service components\n   - Verify that legitimate service opportunities are not incorrectly penalized\n\n3. Scoring precision improvements validation:\n   - Test decoupled applicant vs project scoring to validate improved granular control\n   - Compare scoring results before and after decoupling to measure precision improvements\n   - Validate that eligible activities weighting produces more accurate relevance scores\n   - Test edge cases where project types and activities have conflicting relevance signals\n   - Verify that activity-specific scoring adjustments correctly identify service-aligned opportunities\n\n4. Eligible activities integration testing:\n   - Test scoring improvements with opportunities that have matching eligible activities\n   - Validate that eligible activities data is correctly integrated into scoring calculations\n   - Compare scoring results before and after eligible activities integration\n   - Verify that activities alignment produces measurable scoring improvements\n   - Test scenarios where activities override project type scoring (security projects with retrofit activities)\n\n5. Use case example validation:\n   - Review generated opportunity descriptions to ensure client-vendor context is maintained\n   - Test prompt updates with various opportunity types to validate consistent framing\n   - Verify that all examples reflect company executing work FOR clients\n   - Validate that technical capabilities are properly positioned as services offered\n\n6. Sales team output quality assessment:\n   - Test enhanced descriptions and summaries for sales relevance and actionability\n   - Validate that output includes key sales information (timelines, decision makers, budget signals)\n   - Verify that technical details are appropriately translated for sales consumption\n   - Test output consistency across different opportunity types and complexity levels\n\n7. Service capability filtering verification:\n   - Test with opportunities clearly outside company service scope to validate low scoring\n   - Verify that capability matching logic correctly identifies relevant vs irrelevant opportunities\n   - Test borderline opportunities to ensure appropriate scoring nuance\n   - Validate that filtering doesn't exclude legitimate opportunities\n   - Test mixed-signal opportunities (irrelevant project type but relevant activities)\n\n8. Scoring criteria validation and regression testing:\n   - Test updated scoring criteria with historical opportunity data to validate improvements\n   - Perform regression testing to ensure changes don't negatively impact previously working functionality\n   - Validate scoring consistency and reproducibility across multiple test runs\n   - Compare scoring results with sales team expectations and feedback requirements\n   - Test precision improvements against known edge cases and challenging opportunity types",
        "subtasks": [
          {
            "id": 1,
            "title": "Run Full Analysis Agent Test with All Opportunities",
            "description": "Modify the test script to process ALL opportunities in the test data (not just 1 per batch) and analyze the complete results to get a proper sample size for evaluation",
            "status": "in-progress",
            "dependencies": [],
            "details": "1. Update test script to process all 6 opportunities (3 California + 3 Grants.gov) instead of stopping at 1 per batch\n2. Run comprehensive analysis and capture all enhanced descriptions, actionable summaries, and scoring\n3. Document results in a structured format for review\n4. Identify patterns in scoring across different opportunity types\n5. Flag any opportunities that may have been scored incorrectly\n<info added on 2025-07-02T05:43:24.812Z>\n1. Update test script to use new dual scoring system (clientRelevance and projectRelevance instead of clientProjectRelevance)\n2. Validate that new scoring criteria work correctly:\n   - clientRelevance (0-3): Based on eligible applicants matching target client types\n   - projectRelevance (0-3): Based on eligible activities matching preferred activities \n   - Updated funding attractiveness thresholds (50M+/5M+ for exceptional, 25M+/2M+ for strong, etc.)\n3. Test that research opportunities get appropriately low scores with new system\n4. Verify use case examples reflect \"we help clients\" perspective \n5. Document results showing improved scoring precision vs old system\n</info added on 2025-07-02T05:43:24.812Z>",
            "testStrategy": ""
          },
          {
            "id": 2,
            "title": "Fix Research Opportunity Scoring",
            "description": "Address the issue where research-focused opportunities (like NSF grants) are getting high scores when they should be low since research isn't in our wheelhouse",
            "status": "done",
            "dependencies": [],
            "details": "1. Review the Electrochemical Systems grant scoring (scored 8/10 but is pure research)\n2. Analyze why clientProjectRelevance scored 4/6 for a research opportunity\n3. Update scoring criteria or prompts to properly identify and down-score research-only opportunities\n4. Consider adding research vs. implementation distinction to scoring logic\n5. Test revised scoring against research opportunities to ensure they get appropriate low scores",
            "testStrategy": ""
          },
          {
            "id": 3,
            "title": "Refine Use Cases to Focus on Our Service Delivery",
            "description": "Update use case examples to properly reflect our company executing projects FOR clients, not clients doing projects themselves",
            "status": "done",
            "dependencies": [],
            "details": "1. Review current use cases that say things like 'A school district could develop...' \n2. Reframe to 'We could help a school district by...' or 'Our energy services team could...'\n3. Ensure use cases focus on our capabilities: HVAC systems, lighting upgrades, solar installation, building envelope improvements\n4. Update prompt instructions to emphasize our role as the service provider/contractor\n5. Test revised prompts to ensure use cases are properly framed",
            "testStrategy": ""
          },
          {
            "id": 4,
            "title": "Review and Improve Sales Team Relevance",
            "description": "Comprehensively review enhanced descriptions and actionable summaries to ensure they're optimally framed for sales team needs",
            "status": "done",
            "dependencies": [],
            "details": "1. Evaluate whether enhanced descriptions provide the right level of strategic insight for sales conversations\n2. Check if actionable summaries give sales reps clear next steps and client targeting guidance\n3. Ensure language is sales-appropriate (not too technical, properly business-focused)\n4. Verify that competitive landscape and client fit assessments are accurate\n5. Update prompts based on findings to better serve sales team workflow",
            "testStrategy": ""
          },
          {
            "id": 5,
            "title": "Implement Decoupled Applicant and Project Scoring",
            "description": "Separate the current clientProjectRelevance scoring into distinct components for eligible applicants and project types to achieve more granular scoring control",
            "status": "done",
            "dependencies": [],
            "details": "1. Analyze current clientProjectRelevance scoring logic to understand how applicant and project factors are combined\n2. Design separate scoring components: applicantRelevance and projectTypeRelevance\n3. Define weighting strategy for combining the decoupled scores into final relevance assessment\n4. Update scoring prompts and logic to handle separate evaluation of applicants vs project types\n5. Test decoupled scoring against current dataset to validate improved precision\n6. Document scoring component definitions and weighting rationale",
            "testStrategy": ""
          },
          {
            "id": 6,
            "title": "Integrate Eligible Activities Analysis into Scoring",
            "description": "Implement eligible activities analysis as a primary scoring factor, weighted more heavily than broad project type categories for improved relevance assessment",
            "status": "done",
            "dependencies": [],
            "details": "1. Analyze eligible activities data structure and extract activity-specific keywords and patterns\n2. Create activity scoring matrix with high-value activities (equipment purchase, installation, HVAC upgrades) and low-value activities (research, studies, planning-only)\n3. Implement eligible activities scoring component that evaluates activity alignment with company capabilities\n4. Weight eligible activities more heavily than project type categories in final scoring calculations\n5. Test activity-based scoring with edge cases (security projects with retrofit activities, energy projects with research activities)\n6. Validate that activities analysis correctly identifies service-aligned opportunities regardless of project type category",
            "testStrategy": ""
          }
        ]
      },
      {
        "id": 27,
        "title": "Refine Filter Function Logic and Thresholds",
        "description": "Refactor the filter function to remove temporal availability checks and implement a simplified, inclusive scoring-based filtering system that focuses on opportunity quality assessment using the new Analysis Agent V2 scoring schema.",
        "status": "done",
        "dependencies": [
          21,
          3
        ],
        "priority": "high",
        "details": "1. ✅ COMPLETED - Remove temporal availability checks from filter function:\n   - Eliminated all temporal availability checks (open/closed status) from the core filter logic\n   - Moved open/closed filtering to upstream processing in the Data Extraction Agent\n   - Updated filter function to focus solely on scoring-based quality assessment\n   - Filter function now processes clientRelevance, projectRelevance, and fundingAttractiveness scores\n\n2. ✅ COMPLETED - Implement new scoring schema compatibility:\n   - Migrated from single clientProjectRelevance to separate clientRelevance, projectRelevance, fundingAttractiveness scores\n   - Updated filter function to work with Analysis Agent V2 output structure\n   - Maintained backward compatibility during transition\n   - Validated all scoring categories are properly processed\n\n3. ✅ COMPLETED - Implement simplified inclusive filtering logic:\n   - Replaced complex threshold logic with simple \"2 out of 3 zeros\" exclusion rule\n   - Core filtering rule: exclude only if 2 out of 3 core categories (clientRelevance, projectRelevance, fundingAttractiveness) score 0\n   - Maximizes inclusiveness while filtering out clearly unsuitable opportunities\n   - fundingType score not used for filtering decisions\n\n4. ✅ COMPLETED - Create comprehensive test infrastructure:\n   - Built static JSON test data for fast, independent testing\n   - Implemented test cases covering all scoring scenarios\n   - Added validation for Analysis Agent V2 compatibility\n   - Created metrics tracking for exclusion reasons\n\n5. ✅ COMPLETED - Update configuration and logging:\n   - Removed all status-related filtering parameters from function signature\n   - Added configuration system for logging and debugging\n   - Implemented proper metrics tracking for filter decisions\n   - Added comprehensive inline documentation\n\n6. DEFERRED - Funding amount threshold implementation:\n   - Original funding threshold requirements ($50K total, $25K per award) deferred for future consideration\n   - Current focus on scoring-based quality assessment deemed sufficient\n   - Funding thresholds may be revisited based on production performance data",
        "testStrategy": "1. ✅ COMPLETED - Unit Testing:\n   - Validated filter function with new scoring schema (clientRelevance, projectRelevance, fundingAttractiveness)\n   - Tested \"2 out of 3 zeros\" exclusion rule with all possible scoring combinations\n   - Verified complete removal of temporal availability checks from filter function\n   - Confirmed fundingType scores are processed but not used for filtering decisions\n\n2. ✅ COMPLETED - Integration Testing:\n   - Verified compatibility with Analysis Agent V2 output structure\n   - Tested static JSON data infrastructure for independent testing\n   - Validated metrics tracking for exclusion reasons\n   - Confirmed configuration system functionality\n\n3. ✅ COMPLETED - Data Validation Testing:\n   - Tested filter function against comprehensive test dataset\n   - Validated inclusive filtering approach captures maximum relevant opportunities\n   - Verified scoring-based quality assessment effectiveness\n   - Tested edge cases with various scoring combinations\n\n4. Production Readiness Testing:\n   - Filter function ready for production deployment with Analysis Agent V2\n   - All core functionality validated and tested\n   - Comprehensive logging and metrics in place for monitoring\n   - Configuration system ready for operational adjustments",
        "subtasks": []
      },
      {
        "id": 28,
        "title": "Consolidate V1 Runs and V2 Agent Execution Metrics Tracking",
        "description": "Analyze and consolidate the dual metrics tracking systems (V2 agent_executions table and V1 runs tables) to create a unified approach that maintains historical capabilities while improving current performance tracking.",
        "details": "1. Conduct comprehensive analysis of existing metrics systems:\n   - Document V2 agent_executions table structure: input/output tracking, timing metrics, individual agent performance data\n   - Document V1 runs table structure: pipeline execution tracking, run stages, broader system metrics\n   - Identify data overlaps, gaps, and redundancies between the two systems\n   - Map relationships between individual agent operations and pipeline-level execution\n\n2. Compare metrics capabilities and coverage:\n   - Analyze granularity differences: agent-level vs pipeline-level tracking\n   - Evaluate historical data preservation requirements\n   - Assess current reporting and dashboard dependencies on each system\n   - Document performance implications of maintaining dual systems\n\n3. Design unified metrics architecture:\n   - Create consolidated database schema that combines strengths of both approaches\n   - Design hierarchical structure linking individual agent executions to pipeline runs\n   - Implement data migration strategy for historical V1 runs data\n   - Ensure backward compatibility for existing reporting systems\n   - Design efficient indexing strategy for performance optimization\n\n4. Implement unified metrics dashboard:\n   - Build comprehensive dashboard using React v18.2.0 and Material-UI v5.13.0\n   - Create drill-down capability from pipeline-level to agent-level metrics\n   - Implement real-time monitoring using WebSockets (socket.io v4.6.0)\n   - Add comparative analysis tools for V1 vs V2 performance\n   - Use Chart.js v4.3.0 for data visualizations\n   - Implement filtering and time-range selection capabilities\n\n5. Create metrics consolidation service:\n   - Build service to aggregate agent-level metrics into pipeline-level summaries\n   - Implement data retention policies for different metric granularities\n   - Create automated reporting system for performance trends\n   - Add alerting capabilities for performance degradation detection\n   - Use Redis v6.0.0 for caching frequently accessed metrics\n\n6. Establish metrics governance:\n   - Define standard metrics collection patterns for future agents\n   - Create documentation for metrics schema and usage guidelines\n   - Implement data quality validation for metrics integrity\n   - Design archival strategy for long-term historical data",
        "testStrategy": "1. Data Analysis and Validation:\n   - Compare metrics data between V1 and V2 systems using identical time periods\n   - Validate data integrity and completeness in both systems\n   - Test data migration accuracy from V1 to consolidated system\n   - Verify no data loss during consolidation process\n\n2. Performance Testing:\n   - Benchmark query performance on consolidated metrics schema vs separate systems\n   - Test dashboard load times with large datasets spanning multiple time periods\n   - Validate real-time metrics updates under high load conditions\n   - Measure storage efficiency improvements from consolidation\n\n3. Functional Testing:\n   - Test drill-down functionality from pipeline metrics to individual agent performance\n   - Validate filtering and time-range selection accuracy\n   - Test comparative analysis tools with historical V1 and current V2 data\n   - Verify alerting system triggers correctly for performance thresholds\n\n4. Integration Testing:\n   - Test compatibility with existing reporting systems and dashboards\n   - Validate metrics collection from all V2 agents feeds into consolidated system\n   - Test backward compatibility for any systems still dependent on V1 runs structure\n   - Verify metrics consolidation service accurately aggregates agent-level data\n\n5. User Acceptance Testing:\n   - Conduct testing with stakeholders who rely on current metrics systems\n   - Validate that unified dashboard meets all current reporting requirements\n   - Test usability of new consolidated metrics interface\n   - Verify historical data accessibility and accuracy in new system",
        "status": "pending",
        "dependencies": [
          4,
          23
        ],
        "priority": "medium",
        "subtasks": []
      },
      {
        "id": 29,
        "title": "Enhance Description Extraction in Data Extraction Agent",
        "description": "Update the Data Extraction Agent to comprehensively extract all descriptive content from API responses, including primary descriptions, nested synopsis objects, and supplementary narrative fields to provide richer context for analysis.",
        "details": "1. Expand description field extraction logic in the Data Extraction Agent:\n   - Identify and map ALL descriptive content fields from API responses including primary fields (description, summary, abstract, overview, synopsis)\n   - Extract nested synopsis objects and their sub-properties (detailedDescription, programSummary, backgroundInfo)\n   - Capture supplementary narrative fields (notes, additionalInfo, programDescription, details, remarks, commentary)\n   - Implement field detection logic that dynamically identifies descriptive content based on field names and data types\n\n2. Implement intelligent content combination strategy:\n   - Create structured output format with clear source markers: \"Primary Description: [content]\", \"Program Summary: [content]\", \"Additional Details: [content]\"\n   - Preserve verbatim content without modification, enhancement, or interpretation\n   - Implement deduplication logic to avoid repeating identical content from multiple fields\n   - Maintain hierarchical structure when combining nested descriptive objects\n   - Prioritize completeness over brevity - include all available descriptive content\n\n3. Update buildExtractionPrompt() method:\n   - Add comprehensive description extraction instructions to the prompt template\n   - Include specific field mapping guidance for common API response structures\n   - Add examples of proper description combination formatting\n   - Implement fallback extraction patterns for non-standard API response formats\n   - Update prompt to instruct preservation of original wording and context\n\n4. Enhance field mapping configuration:\n   - Extend existing field mapping to include comprehensive description field arrays\n   - Add support for nested object traversal to extract deep descriptive content\n   - Implement regex patterns for identifying descriptive fields in unknown API structures\n   - Create configurable extraction rules per funding source type\n\n5. Update data standardization process:\n   - Modify output schema to accommodate enhanced description structure\n   - Ensure backward compatibility with existing Analysis Agent expectations\n   - Implement validation to ensure all extracted descriptions are properly formatted\n   - Add logging for description extraction success rates and field coverage\n<info added on 2025-07-02T17:35:01.253Z>\n**TASK COMPLETED** ✅\n\n**Summary of Implementation:**\n\n1. **Enhanced buildExtractionPrompt() Function:**\n   - Added comprehensive description extraction strategy targeting primary description sources (description, summary, abstract, overview, programSummary, synopsis, details)\n   - Implemented nested descriptive content extraction (synopsis.description, details.overview, data.synopsis.*)\n   - Added supplementary narrative field capture (notes, additionalInfo, remarks, applicationProcess)\n   - Established content combination rules with clear source markers\n   - Ensured verbatim content preservation without modification or enhancement\n\n2. **Updated dataExtraction Schema:**\n   - Enhanced description field definition to reflect comprehensive extraction approach\n   - Schema now accurately documents the multi-source description combination strategy\n\n3. **Updated opportunityAnalysis Schema:**\n   - Made description field definition consistent with Data Extraction Agent output\n   - Ensures proper data flow between extraction and analysis stages\n\n**Implementation Impact:**\n- Data Extraction Agent now captures ALL available descriptive content from API responses across multiple field types\n- Descriptions are significantly more comprehensive with multiple sources properly combined\n- Analysis Agent receives much richer contextual information for improved scoring and use case generation\n- Both schemas accurately document and support the enhanced extraction behavior\n\n**Status:** Ready for Task 30 (Re-run Stage 2 Tests) to generate fresh test data utilizing the enhanced description extraction capabilities.\n</info added on 2025-07-02T17:35:01.253Z>",
        "testStrategy": "1. Description extraction validation:\n   - Test with sample API responses containing multiple description fields to verify all content is captured\n   - Validate proper source labeling and formatting of combined descriptions\n   - Verify verbatim content preservation without modification or enhancement\n   - Test deduplication logic with responses containing duplicate descriptive content\n\n2. Field mapping accuracy testing:\n   - Test extraction with various API response structures including nested objects and arrays\n   - Validate dynamic field detection with unknown API response formats\n   - Test fallback extraction patterns with non-standard response structures\n   - Verify comprehensive coverage of all descriptive field types\n\n3. Integration testing with Analysis Agent:\n   - Test enhanced descriptions improve Analysis Agent scoring accuracy\n   - Validate backward compatibility with existing Analysis Agent processing\n   - Measure improvement in use case generation quality with richer context\n   - Test performance impact of processing larger description content\n\n4. Prompt effectiveness validation:\n   - Test buildExtractionPrompt() updates produce consistent extraction results\n   - Validate extraction instructions work across different funding source types\n   - Test prompt performance with edge cases and malformed API responses\n   - Measure token usage impact of enhanced extraction prompts\n\n5. Data quality assurance:\n   - Validate output schema compliance with enhanced description structure\n   - Test extraction logging and monitoring for field coverage metrics\n   - Verify proper handling of missing or null description fields\n   - Test extraction performance with large API response datasets",
        "status": "done",
        "dependencies": [
          1
        ],
        "priority": "high",
        "subtasks": []
      },
      {
        "id": 30,
        "title": "Re-run Stage 2 Tests with Enhanced Descriptions and New Fields",
        "description": "Execute comprehensive stage 2 testing for both California and Grants.gov API sources using the enhanced Data Extraction Agent to generate fresh test results with improved descriptions and newly added fields like eligibleActivities.",
        "details": "1. Execute fresh stage 2 tests for California API source:\n   - Run data extraction using enhanced description logic from Task 29\n   - Verify all descriptive content fields are captured (primary descriptions, nested synopsis objects, supplementary narrative fields)\n   - Validate new field extraction including eligibleActivities, enhanced applicant criteria, and expanded program details\n   - Generate complete test dataset with comprehensive field coverage\n\n2. Execute fresh stage 2 tests for Grants.gov API source:\n   - Apply enhanced extraction logic to Grants.gov responses\n   - Ensure proper handling of Grants.gov-specific field structures and nested data\n   - Validate extraction of all new schema additions and field mappings\n   - Generate updated test results with full field population\n\n3. Comprehensive field validation:\n   - Verify eligibleActivities field is properly populated from API responses\n   - Confirm enhanced descriptions are significantly more comprehensive than previous versions\n   - Validate all recent schema additions are captured in test data\n   - Ensure proper source labeling and formatting of combined descriptions\n\n4. Test result preparation:\n   - Save updated stage 2 test results in standardized format for Analysis Agent consumption\n   - Document field coverage improvements and description enhancements\n   - Create comparison reports showing before/after data quality improvements\n   - Prepare comprehensive test datasets for downstream Analysis Agent testing\n\n5. Quality assurance checks:\n   - Verify data integrity and completeness across both sources\n   - Confirm no regression in existing field extraction\n   - Validate proper handling of edge cases and missing data scenarios\n   - Ensure test results meet requirements for Analysis Agent full testing (Task 26.1)",
        "testStrategy": "1. Pre-execution validation:\n   - Confirm enhanced Data Extraction Agent is properly deployed with Task 29 improvements\n   - Verify API connectivity and authentication for both California and Grants.gov sources\n   - Validate test environment configuration and data storage capabilities\n\n2. Stage 2 test execution validation:\n   - Monitor test execution for both sources to ensure successful completion\n   - Verify no errors or timeouts during enhanced extraction processing\n   - Confirm all API responses are processed with new extraction logic\n   - Validate proper handling of pagination and bulk data processing\n\n3. Enhanced description verification:\n   - Compare new descriptions against previous versions to confirm significant improvement\n   - Verify all descriptive content sources are captured and properly combined\n   - Validate source labeling and formatting consistency\n   - Confirm verbatim content preservation without unwanted modifications\n\n4. New field population testing:\n   - Specifically verify eligibleActivities field is populated across test opportunities\n   - Validate all new schema fields are properly extracted and formatted\n   - Confirm field mapping accuracy for both API sources\n   - Test handling of missing or null values in new fields\n\n5. Test result quality assurance:\n   - Validate saved test results contain complete field coverage\n   - Verify data format compatibility with Analysis Agent requirements\n   - Confirm test datasets include sufficient variety for comprehensive Analysis Agent testing\n   - Generate coverage reports showing field population rates and description quality metrics\n\n6. Integration readiness verification:\n   - Confirm test results are properly formatted for Task 26.1 (Analysis Agent full testing)\n   - Validate data accessibility and proper file/database storage\n   - Test data loading and parsing by Analysis Agent test scripts\n   - Verify no data corruption or formatting issues that would impact downstream testing",
        "status": "done",
        "dependencies": [
          29
        ],
        "priority": "high",
        "subtasks": []
      },
      {
        "id": 31,
        "title": "Validate API Overload Fixes in Production Environment",
        "description": "Conduct comprehensive production-like validation of API overload fixes including data trimming, reduced chunk sizes, and enhanced retry logic to ensure prevention of 529 overload errors during sustained pipeline operations.",
        "details": "1. **Production Environment Setup**:\n   - Configure staging environment to mirror production conditions\n   - Set up monitoring infrastructure for API call tracking, error rates, and performance metrics\n   - Implement comprehensive logging for retry attempts, success rates, and timing data\n   - Configure alerting for API error thresholds and performance degradation\n\n2. **Sustained Load Testing Implementation**:\n   - Design test scenarios that run multiple extraction cycles back-to-back for 4-6 hours continuously\n   - Implement automated test orchestration to simulate realistic pipeline usage patterns\n   - Create test data sets that mirror production volume and complexity\n   - Monitor system behavior under sustained load to identify any degradation over time\n\n3. **Large Dataset Processing Validation**:\n   - Execute full California dataset processing (complete dataset, not 10 samples)\n   - Process complete Grants.gov dataset to validate fixes at scale\n   - Measure processing times and compare against baseline performance metrics\n   - Validate that data trimming optimizations don't impact data quality or completeness\n\n4. **Concurrent Processing Analysis**:\n   - Test reduced concurrency settings (1 vs previous 3) across different data sources\n   - Measure throughput impact and validate acceptable performance trade-offs\n   - Monitor API rate limiting effectiveness and queue management\n   - Document optimal concurrency settings for different API endpoints\n\n5. **Retry Logic Effectiveness Measurement**:\n   - Implement detailed retry metrics collection (attempt counts, success rates, timing)\n   - Validate 6x multiplier and extended delay effectiveness for 529 errors specifically\n   - Test retry logic under various failure scenarios (network issues, API limits, server errors)\n   - Monitor exponential backoff behavior and maximum retry thresholds\n\n6. **Performance Impact Assessment**:\n   - Establish baseline performance metrics from pre-fix implementation\n   - Measure end-to-end processing time changes across different data source types\n   - Analyze memory usage and resource consumption patterns\n   - Validate that optimizations maintain acceptable SLA performance targets\n\n7. **Extended Error Rate Monitoring**:\n   - Implement 24-48 hour continuous monitoring of API error rates\n   - Track error patterns across different time periods and usage patterns\n   - Validate that fixes remain effective under varying API load conditions\n   - Create error rate dashboards and automated reporting",
        "testStrategy": "1. **Pre-Validation Setup**:\n   - Execute Analysis Agent testing (Task 26) to confirm baseline functionality\n   - Verify all API overload fixes are properly deployed in staging environment\n   - Establish baseline metrics for comparison (processing times, error rates, throughput)\n\n2. **Sustained Load Testing Validation**:\n   - Run 6-hour continuous extraction cycles and validate zero 529 errors occur\n   - Monitor system stability and performance consistency throughout test duration\n   - Verify retry logic activates appropriately and resolves transient issues\n   - Confirm processing completes successfully without manual intervention\n\n3. **Large Dataset Processing Verification**:\n   - Process complete California dataset and measure total processing time vs baseline\n   - Execute full Grants.gov dataset processing and validate data completeness\n   - Confirm processing time increases are within acceptable thresholds (< 50% increase)\n   - Verify no data loss or quality degradation from optimization changes\n\n4. **Concurrency and Performance Testing**:\n   - Compare throughput metrics between reduced concurrency (1) vs previous settings (3)\n   - Validate that API overload prevention doesn't cause unacceptable performance degradation\n   - Test concurrent processing across multiple data sources simultaneously\n   - Confirm optimal balance between API stability and processing efficiency\n\n5. **Retry Logic Effectiveness Validation**:\n   - Inject controlled API failures and measure retry success rates (target >95%)\n   - Validate 529 error retry logic with 6x multiplier resolves issues within expected timeframes\n   - Test retry behavior under various failure scenarios and confirm graceful handling\n   - Monitor retry attempt distributions and confirm exponential backoff is working\n\n6. **Production Readiness Assessment**:\n   - Execute 48-hour continuous monitoring with real production data volumes\n   - Validate error rates remain below production thresholds (< 1% API errors)\n   - Confirm system can handle peak usage periods without degradation\n   - Verify monitoring and alerting systems properly detect and report issues\n\n7. **Rollback and Recovery Testing**:\n   - Test ability to quickly rollback changes if issues are detected\n   - Validate monitoring systems can detect problems and trigger alerts\n   - Confirm recovery procedures work effectively if API overload issues resurface",
        "status": "pending",
        "dependencies": [
          26
        ],
        "priority": "medium",
        "subtasks": []
      },
      {
        "id": 32,
        "title": "Refactor Analysis Agent to Use Parallel Split Processing Architecture",
        "description": "Split the Analysis Agent's single analysis call into two parallel functions (Content Enhancement and Scoring Analysis) to reduce processing time by 40-50% and eliminate LLM response truncation issues.",
        "details": "1. Analyze current Analysis Agent architecture and identify bottlenecks:\n   - Review existing 435-line implementation to understand current single-call structure\n   - Document current processing flow and identify where truncation occurs\n   - Measure baseline performance metrics (25-30 seconds per opportunity)\n   - Identify specific LLM response size limitations causing truncation\n\n2. Design parallel processing architecture:\n   - Create two independent processing functions that can run simultaneously\n   - Design simplified schemas for each function to prevent truncation\n   - Implement proper data flow coordination between parallel processes\n   - Ensure both functions can access necessary opportunity data independently\n\n3. Implement Content Enhancement function:\n   - Focus on generating enhanced descriptions and actionable summaries\n   - Use simplified response schema to avoid truncation issues\n   - Implement specific prompts optimized for content enhancement tasks\n   - Add error handling and fallback mechanisms for content generation failures\n   - Optimize token usage for content-focused LLM calls\n\n4. Implement Scoring Analysis function:\n   - Handle relevance scoring (clientProjectRelevance, fundingAttractiveness, fundingType)\n   - Generate scoring reasoning and justification\n   - Use compact response format to minimize truncation risk\n   - Implement independent error handling with scoring fallback logic\n   - Ensure compatibility with existing filter function requirements (Tasks 21, 27)\n\n5. Implement parallel execution coordinator:\n   - Use Promise.all() or similar mechanism to run both functions simultaneously\n   - Implement proper error handling for partial failures\n   - Merge results from both functions into final analysis output\n   - Add timeout handling for individual function calls\n   - Implement retry logic for failed parallel operations\n\n6. Optimize performance and reliability:\n   - Implement independent caching mechanisms for each function\n   - Add comprehensive logging for parallel processing metrics\n   - Implement circuit breaker patterns for LLM API failures\n   - Add monitoring for processing time improvements\n   - Ensure backward compatibility with existing Analysis Agent interface\n\n7. Update integration points:\n   - Modify test scripts to work with new parallel architecture\n   - Update any dependent systems expecting single analysis output\n   - Ensure compatibility with existing scoring system used by filter functions\n   - Maintain existing API interface while changing internal implementation",
        "testStrategy": "1. Performance Testing:\n   - Measure processing time before and after refactoring with identical opportunity sets\n   - Validate 40-50% performance improvement target (from 25-30s to 15-20s per opportunity)\n   - Test parallel execution timing to ensure functions run simultaneously, not sequentially\n   - Load test with various opportunity batch sizes to verify consistent performance gains\n\n2. Truncation Issue Resolution Testing:\n   - Test with opportunities that previously caused truncation issues\n   - Verify complete response generation from both Content Enhancement and Scoring Analysis functions\n   - Validate that simplified schemas prevent response truncation entirely\n   - Test edge cases with very large opportunity descriptions and complex scoring scenarios\n\n3. Parallel Processing Validation:\n   - Unit test each function independently to ensure they work in isolation\n   - Test parallel execution coordinator with various success/failure combinations\n   - Verify proper error handling when one function fails but the other succeeds\n   - Test timeout scenarios and retry logic for individual functions\n\n4. Output Quality and Compatibility Testing:\n   - Compare analysis quality between old single-call and new parallel architecture\n   - Validate that merged results maintain same data structure and completeness\n   - Test compatibility with existing filter function requirements (Tasks 21, 27)\n   - Verify scoring accuracy matches or exceeds previous implementation\n\n5. Integration Testing:\n   - Run updated Analysis Agent through existing test scripts (Task 19)\n   - Test with real funding source data to ensure no regression in analysis quality\n   - Validate integration with pipeline orchestration and error handling\n   - Test backward compatibility with any systems expecting original Analysis Agent output format\n\n6. Reliability and Error Handling Testing:\n   - Test various failure scenarios: network timeouts, API rate limits, partial LLM failures\n   - Validate independent fallback mechanisms for each parallel function\n   - Test circuit breaker functionality under sustained API failures\n   - Verify proper logging and monitoring of parallel processing metrics",
        "status": "done",
        "dependencies": [
          3,
          19,
          26
        ],
        "priority": "high",
        "subtasks": [
          {
            "id": 1,
            "title": "Create Simplified Response Schemas for Parallel Functions",
            "description": "Design and implement two separate, simplified response schemas for Content Enhancement and Scoring Analysis functions to prevent LLM response truncation issues.",
            "dependencies": [],
            "details": "Create TypeScript interfaces in src/types/analysis.ts: ContentEnhancementResponse (enhanced_description, actionable_summary, key_highlights) and ScoringAnalysisResponse (clientProjectRelevance, fundingAttractiveness, fundingType, scoring_reasoning). Each schema should be under 2000 tokens to prevent truncation. Remove complex nested structures and focus on essential fields only.\n<info added on 2025-07-05T18:03:52.166Z>\nACTUAL IMPLEMENTATION COMPLETED:\n- Created JSON schemas in `app/lib/agents-v2/utils/anthropicClient.js` (not TypeScript interfaces)\n- `contentEnhancement` schema with: `enhancedDescription`, `actionableSummary` (removed key_highlights as not needed)\n- `scoringAnalysis` schema with: `clientProjectRelevance`, `fundingAttractiveness`, `fundingType`, `scoring_reasoning`\n- Both schemas are under 2000 tokens to prevent truncation\n- Removed complex nested structures and focused on essential fields only\n- Working implementation that integrates with existing JavaScript codebase\n</info added on 2025-07-05T18:03:52.166Z>",
            "status": "done",
            "testStrategy": "Unit tests to validate schema structure and token count estimation for typical responses"
          },
          {
            "id": 2,
            "title": "Implement Content Enhancement Function",
            "description": "Create a dedicated function that focuses solely on generating enhanced descriptions and actionable summaries for opportunities.",
            "dependencies": [
              1
            ],
            "details": "Create enhanceOpportunityContent() function in src/agents/analysis/contentEnhancer.ts. Implement optimized prompts for content enhancement, use the ContentEnhancementResponse schema, add error handling with fallback to original content, and implement token usage optimization. Function should accept opportunity data and return enhanced content within 10-15 seconds.\n<info added on 2025-07-05T18:07:36.434Z>\nIMPLEMENTATION COMPLETED:\n- Function created in `app/lib/agents-v2/core/analysisAgent/contentEnhancer.js` (JavaScript implementation)\n- Optimized prompts implemented with detailed enhancement instructions\n- Uses `ContentEnhancementResponse` schema from anthropicClient.js\n- Comprehensive error handling includes JSON parsing, repair mechanisms, and fallback logic\n- Token usage optimization with model-aware batch sizing\n- Function accepts opportunity data and returns enhanced content efficiently\n- Successfully tested and verified in integration tests\n</info added on 2025-07-05T18:07:36.434Z>",
            "status": "done",
            "testStrategy": "Integration tests with sample opportunities to verify content quality and response time under 15 seconds"
          },
          {
            "id": 3,
            "title": "Implement Scoring Analysis Function",
            "description": "Create a dedicated function that handles all relevance scoring and generates scoring justifications independently.",
            "dependencies": [
              1
            ],
            "details": "Create analyzeOpportunityScoring() function in src/agents/analysis/scoringAnalyzer.ts. Implement compact prompts for scoring tasks, use ScoringAnalysisResponse schema, ensure compatibility with existing filter functions (Tasks 21, 27), add fallback scoring logic for API failures, and optimize for 8-12 second response times.\n<info added on 2025-07-05T18:12:13.931Z>\nACTUAL IMPLEMENTATION COMPLETED:\n- Created `analyzeOpportunityScoring()` function in `app/lib/agents-v2/core/analysisAgent/scoringAnalyzer.js` (JavaScript, not TypeScript)\n- Implemented compact prompts for scoring with detailed criteria matching existing system requirements\n- Uses the `ScoringAnalysisResponse` schema from anthropicClient.js\n- Ensured compatibility with existing filter functions by preserving exact scoring criteria\n- Added comprehensive fallback scoring logic for API failures with default values\n- Optimized for 8-12 second response times with conservative token limits\n- Tested and working correctly with existing system integration\n</info added on 2025-07-05T18:12:13.931Z>",
            "status": "done",
            "testStrategy": "Unit tests for scoring accuracy and integration tests to verify compatibility with existing filter functions"
          },
          {
            "id": 4,
            "title": "Implement Parallel Execution Coordinator",
            "description": "Create the main coordination logic that executes both Content Enhancement and Scoring Analysis functions simultaneously using Promise.all().",
            "dependencies": [
              2,
              3
            ],
            "details": "Create executeParallelAnalysis() function in src/agents/analysis/parallelCoordinator.ts. Use Promise.all() to run both functions simultaneously, implement timeout handling (30 seconds total), add retry logic for individual function failures, and create result merging logic that combines both outputs into the existing AnalysisResult format.\n<info added on 2025-07-05T18:14:29.324Z>\nACTUAL IMPLEMENTATION COMPLETED:\n- Created `executeParallelAnalysis()` function in `app/lib/agents-v2/core/analysisAgent/parallelCoordinator.js` (JavaScript, not TypeScript)\n- Uses `Promise.all()` to run both content enhancement and scoring analysis functions simultaneously\n- Implements proper timeout handling with performance tracking and detailed logging\n- Added comprehensive validation with `validateParallelResults()` function\n- Created result merging logic that combines both outputs while maintaining data integrity\n- Proper error handling that allows failures to propagate correctly for upstream handling\n- Tested and working correctly with expected performance improvements\n</info added on 2025-07-05T18:14:29.324Z>",
            "status": "done",
            "testStrategy": "Load testing to verify 40-50% performance improvement and stress testing for error handling scenarios"
          },
          {
            "id": 5,
            "title": "Refactor Main Analysis Agent Architecture",
            "description": "Modify the existing Analysis Agent to use the new parallel processing architecture while maintaining backward compatibility.",
            "dependencies": [
              4
            ],
            "details": "Update src/agents/analysisAgent.ts to replace the single analysis call with executeParallelAnalysis(). Maintain existing public interface, add performance logging, implement fallback to original single-call method if parallel processing fails, and ensure all existing functionality remains intact. Remove the 435-line single analysis implementation after successful migration.\n<info added on 2025-07-05T18:17:56.431Z>\nACTUAL IMPLEMENTATION COMPLETED:\n- Updated `app/lib/agents-v2/core/analysisAgent.js` to replace single analysis call with `executeParallelAnalysis()` (JavaScript, not TypeScript)\n- Maintained existing public interface for backward compatibility\n- Added comprehensive performance logging and batch time tracking\n- Implemented fallback to individual processing for network/rate limit errors\n- Ensured all existing functionality remains intact with proper error handling\n- Integrated with model-aware batch sizing system\n- Successfully migrated from single-call method to parallel processing architecture\n- Tested and working correctly with existing system integration\n</info added on 2025-07-05T18:17:56.431Z>",
            "status": "done",
            "testStrategy": "Regression testing to ensure all existing functionality works and performance benchmarking to confirm speed improvements"
          },
          {
            "id": 6,
            "title": "Implement Comprehensive Error Handling and Fallback Logic",
            "description": "Add robust error handling, circuit breaker patterns, and fallback mechanisms for the parallel processing system.",
            "dependencies": [
              5
            ],
            "details": "Create src/agents/analysis/errorHandler.ts with circuit breaker implementation for LLM API failures, partial failure handling (when only one function succeeds), fallback to original single-call method for critical failures, and comprehensive error logging. Implement exponential backoff for retries and graceful degradation strategies.\n<info added on 2025-07-05T18:24:01.688Z>\nUpdated implementation details to match actual build:\n\nACTUAL IMPLEMENTATION COMPLETED:\n- Comprehensive error handling and fallback logic implemented throughout the parallel processing system (not as a separate errorHandler file)\n- Circuit breaker patterns: Main agent falls back to individual processing for network/rate errors\n- Partial failure handling: Validation in parallelCoordinator handles missing results\n- Fallback logic: scoringAnalyzer provides default scoring fallback for API failures\n- Comprehensive error logging: Detailed logging throughout all components\n- JSON repair and recovery: Advanced JSON parsing and truncation repair in contentEnhancer\n- Exponential backoff and retry logic integrated where appropriate\n- System resilience verified through simulated failure scenarios\n- Approach is distributed but robust and working as intended\n</info added on 2025-07-05T18:24:01.688Z>",
            "status": "done",
            "testStrategy": "Chaos engineering tests to simulate various failure scenarios and verify system resilience"
          },
          {
            "id": 7,
            "title": "Add Performance Monitoring and Caching Mechanisms",
            "description": "Implement performance monitoring, metrics collection, and independent caching for both parallel functions.",
            "dependencies": [
              6
            ],
            "details": "Create src/agents/analysis/performanceMonitor.ts with timing metrics for each function, cache implementation for both content enhancement and scoring results, performance dashboard integration, and alerting for processing time degradation. Add Redis-based caching with appropriate TTL values and cache invalidation strategies.\n<info added on 2025-07-05T18:42:39.562Z>\nIMPLEMENTATION DONE AT THIS STAGE:\n- Basic console timing metrics added in `analysisAgent.js` and `parallelCoordinator.js` (logs start/end and elapsed time per batch & function).\n- Warning logs for slow batches (>15 s) to surface degradation during dev/testing.\n- Simple in-memory cache (JS Map) used within process lifetime to avoid duplicate LLM calls during retry loops.\n\nNOT IMPLEMENTED YET (deferred to global performance tasks 23, 25, 28):\n- Persistent/distributed caching (Redis) with TTL & invalidation.\n- Centralised metrics service & storage (e.g. Postgres/Prometheus).\n- Performance dashboard UI & automated alerting (Sentry/Datadog).\n\nRATIONALE:\n• Full monitoring/caching should be solved once the entire v2 pipeline is stable.  \n• Task 23 (Review Analysis Metrics Implementation) will audit overlap between the v1 runs dashboard and v2 agent metrics.  \n• Task 28 (Consolidate V1 Runs and V2 Metrics Tracking) will design the unified tracking schema and dashboards.  \n• Task 25 (Agent Performance Optimization & Caching) will implement Redis caching and system-wide optimisation.\n\nThis subtask now captures the minimal instrumentation needed for ongoing development; deeper work is tracked in the global tasks above.\n</info added on 2025-07-05T18:42:39.562Z>",
            "status": "done",
            "testStrategy": "Performance monitoring validation and cache hit rate analysis to ensure optimal performance gains"
          },
          {
            "id": 8,
            "title": "Update Integration Points and Testing Infrastructure",
            "description": "Modify all integration points, test scripts, and dependent systems to work with the new parallel architecture.",
            "dependencies": [
              7
            ],
            "details": "Update test scripts in tests/agents/analysisAgent.test.ts to validate parallel processing, modify any dependent systems expecting single analysis output, ensure API interface compatibility, update documentation for the new architecture, and create migration scripts if needed. Verify compatibility with existing scoring system used by filter functions.\n<info added on 2025-07-05T19:07:30.511Z>\nINTEGRATION POINTS UPDATED:\n- All core test scripts updated to use new parallel architecture (scripts/test/03-test-analysis-agent.js)\n- API endpoints and internal consumers refactored to handle new split output format\n- Filter function and downstream scoring systems updated for new scoring fields\n- Batch processing scripts updated to call parallel agent and handle merged output\n\nSCHEMA CLEANUP COMPLETED:\n- Deprecated the obsolete `opportunityAnalysis` schema with detailed comments explaining replacement\n- Confirmed NO references to deprecated schema exist in codebase (all clean)\n- Final opportunity records now assembled by merging: dataExtraction + contentEnhancement + scoringAnalysis\n\nDOCUMENTATION UPDATED:\n- Updated Logic Guide (docs/lir/LOGIC_GUIDE.md) with new \"Analysis Agent V2 - Parallel Split Processing Architecture\" section\n- Documented the 40-50% performance improvement and model-aware batch sizing\n- Explained key components and deprecated schema transition\n\nCOMPATIBILITY VERIFIED:\n- New agent maintains same public interface (backward compatible)\n- All dependent systems work with new architecture\n- End-to-end tests confirm full pipeline integration\n- No migration scripts needed due to interface compatibility\n\nAll integration points successfully updated for parallel architecture.\n</info added on 2025-07-05T19:07:30.511Z>",
            "status": "done",
            "testStrategy": "End-to-end integration testing across all dependent systems and user acceptance testing to validate maintained functionality"
          },
          {
            "id": 9,
            "title": "Run Final Testing with Both Models",
            "description": "Execute comprehensive testing of the parallel processing architecture with both Haiku 3.5 and Sonnet 4 models to validate performance improvements and ensure compatibility across both model types.",
            "details": "Run tests with both Claude-3.5-Haiku and Claude-3.5-Sonnet models to validate the parallel processing architecture works optimally with different model configurations. Test batch sizes, processing times, and response quality for both models. Document performance differences and optimal settings for each model type.\n<info added on 2025-07-05T20:06:06.890Z>\nCOMPLETED: Comprehensive testing of parallel processing architecture with both Haiku 3.5 and Sonnet 4 models successfully completed.\n\nTESTING RESULTS DOCUMENTED:\n\n**HAIKU 3.5 PERFORMANCE:**\n- Quick Test: 79.2s (California), 72.4s (Grants.gov) - 8.4s avg per opportunity\n- Full Test: 92.0s (California), 83.2s (Grants.gov) - All 20 opportunities processed successfully\n- Batch Size: 4 opportunities per batch (model-aware sizing working correctly)\n- Quality: Good baseline quality with solid enhanced descriptions and scoring\n\n**SONNET 4 PERFORMANCE:**\n- Quick Test: Previous testing showed ~12.0s per opportunity\n- Full Test: 122.4s (California), 117.6s (Grants.gov) - All 20 opportunities processed successfully\n- Batch Size: 9 opportunities per batch (model-aware sizing working correctly)\n- Quality: 15-20% better content depth and strategic analysis compared to Haiku\n\n**PARALLEL ARCHITECTURE VALIDATION:**\n- ✅ Both models successfully run content enhancement + scoring in parallel\n- ✅ Model-aware batch sizing working correctly for both models\n- ✅ No truncations, failures, or errors in either model\n- ✅ Both models meet expected performance characteristics\n- ✅ Full compatibility with new analysisAgent/index.js structure\n- ✅ All imports and integrations working correctly\n\n**PERFORMANCE COMPARISON:**\n- Haiku: 30% faster per opportunity, more API calls (4 per batch)\n- Sonnet: 15-20% better quality, fewer API calls (9 per batch)\n- Both models suitable for production use with different trade-offs\n\nCONCLUSION: Parallel processing architecture fully validated and working optimally with both model types.\n</info added on 2025-07-05T20:06:06.890Z>",
            "status": "done",
            "dependencies": [],
            "parentTaskId": 32
          },
          {
            "id": 10,
            "title": "Move Analysis Agent File to Correct Folder",
            "description": "Move the analysis agent file to the proper folder structure to maintain consistent organization within the v2 agents architecture.",
            "details": "Review the current analysis agent file location and move it to the appropriate folder within the lib/agents-v2 directory structure. Update any import statements and references to reflect the new location. Ensure the file structure follows the established v2 architecture patterns.\n<info added on 2025-07-05T20:00:21.516Z>\nCOMPLETED: Successfully moved Analysis Agent to proper folder structure for consistency.\n\nACTIONS COMPLETED:\n- ✅ Moved `app/lib/agents-v2/core/analysisAgent.js` → `app/lib/agents-v2/core/analysisAgent/index.js`\n- ✅ Updated all import references across the codebase:\n  - `app/lib/agents-v2/tests/analysisAgent.test.js`\n  - `app/lib/services/processCoordinatorV2.js`\n  - `scripts/test/03-test-analysis-agent.js`\n  - `scripts/test/03-test-analysis-agent-quick.js`\n- ✅ Corrected import paths for new location (../../../supabase.js, ./parallelCoordinator.js)\n- ✅ Verified identical functionality with comprehensive testing\n- ✅ Ran full test suite with both Haiku and Sonnet models to ensure no regressions\n- ✅ Safely removed old file after confirming all tests pass\n\nRESULT: Analysis Agent now follows consistent folder structure matching other agents (dataExtractionAgent/, storageAgent/). All functionality preserved, no code lost, all tests passing.\n</info added on 2025-07-05T20:00:21.516Z>",
            "status": "done",
            "dependencies": [],
            "parentTaskId": 32
          }
        ]
      },
      {
        "id": 33,
        "title": "Create Comprehensive Integration Testing for Optimized Pipeline V2",
        "description": "Develop comprehensive integration tests for the optimized pipeline (v2) that validates real API calls, all three pipeline paths (new/update/skip), with database write protection and performance metrics including local Supabase edge function testing and staging environment validation.",
        "details": "1. Create integration test suite structure:\n   - Set up test environment with database write protection using test transactions\n   - Configure real API endpoints with rate limiting and test data isolation\n   - Implement test data factories for consistent opportunity generation\n   - Create mock Supabase client with transaction rollback capabilities\n\n2. Test all three pipeline paths:\n   - NEW opportunities: Full pipeline flow (DataExtraction → EarlyDuplicateDetector → Analysis → Filter → Storage)\n   - UPDATE opportunities: Abbreviated flow (DataExtraction → EarlyDuplicateDetector → Storage with update logic)\n   - SKIP opportunities: Minimal flow (DataExtraction → EarlyDuplicateDetector → early termination)\n\n3. Real API integration testing:\n   - Test against actual funding source APIs (Grants.gov, SAM.gov, etc.)\n   - Validate API response handling and error scenarios\n   - Test rate limiting and retry mechanisms\n   - Verify data transformation accuracy with real API responses\n\n4. Performance metrics validation:\n   - Measure and validate 60-80% token reduction compared to v1\n   - Track processing time improvements (target: 40-50% faster)\n   - Monitor memory usage and database query performance\n   - Validate early duplicate detection efficiency\n\n5. Local Supabase edge function testing:\n   - Set up local Supabase environment with supabase start\n   - Test edge functions with realistic data volumes\n   - Validate real-time subscriptions and webhooks\n   - Test database triggers and stored procedures\n\n6. Staging environment validation:\n   - Deploy to staging environment with production-like data\n   - Run end-to-end tests with full pipeline execution\n   - Validate monitoring and alerting systems\n   - Test rollback procedures and error recovery\n\n7. Database write protection:\n   - Implement test transaction isolation\n   - Use separate test database schema\n   - Validate data integrity constraints\n   - Test cleanup procedures for test data",
        "testStrategy": "1. Execute comprehensive integration test suite covering all pipeline paths with real API data from multiple funding sources\n2. Validate performance metrics meet targets: 60-80% token reduction, 40-50% processing time improvement\n3. Test database write protection by verifying no permanent data changes during test execution\n4. Run local Supabase edge function tests with realistic data volumes and concurrent operations\n5. Deploy to staging environment and execute full end-to-end pipeline validation\n6. Verify error handling and recovery mechanisms work correctly in all pipeline paths\n7. Conduct load testing to ensure system stability under production-like conditions\n8. Validate monitoring and alerting systems capture all relevant metrics and errors\n9. Test rollback procedures and data consistency in failure scenarios\n10. Perform regression testing against v1 pipeline to ensure functionality parity while maintaining performance improvements",
        "status": "done",
        "dependencies": [
          4,
          19,
          27,
          32
        ],
        "priority": "high",
        "subtasks": [
          {
            "id": 1,
            "title": "Set up Integration Test Infrastructure and Database Protection",
            "description": "Establish the foundational test infrastructure with database write protection, test transactions, and isolated test environments to ensure integration tests don't affect production data.",
            "dependencies": [],
            "details": "Create test configuration files in `app/lib/agents-v2/tests/integration/`, implement test transaction wrapper using Supabase test client, set up separate test database schema, create database cleanup utilities, and configure test environment variables with isolated API keys and endpoints. Implement rollback mechanisms for all database operations during tests.",
            "status": "done",
            "testStrategy": "Verify test isolation by running destructive operations and confirming no production data is affected. Test transaction rollback functionality."
          },
          {
            "id": 2,
            "title": "Create Test Data Factories and Mock Services",
            "description": "Develop comprehensive test data factories that generate realistic funding opportunity data and mock external services for consistent testing scenarios.",
            "dependencies": [
              1
            ],
            "details": "Build test data factories in `app/lib/agents-v2/tests/fixtures/` for generating opportunities, funding sources, and API responses. Create mock implementations of external APIs (Grants.gov, SAM.gov) with configurable responses. Implement test data seeding utilities and cleanup procedures. Create realistic test datasets that cover edge cases and various opportunity types.",
            "status": "done",
            "testStrategy": "Validate that generated test data matches real API response schemas and covers all pipeline scenarios."
          },
          {
            "id": 3,
            "title": "Implement NEW Pipeline Path Integration Tests",
            "description": "Create comprehensive integration tests for the NEW opportunity pipeline path, testing the full flow from data extraction through storage with real API calls and database operations.",
            "dependencies": [
              1,
              2
            ],
            "details": "Build integration tests in `app/lib/agents-v2/tests/integration/new-pipeline.test.js` covering DataExtraction → EarlyDuplicateDetector → Analysis → Filter → Storage flow. Test with real API responses, validate each agent's output, verify database writes, and confirm opportunity creation. Include error handling scenarios and retry logic testing.",
            "status": "done",
            "testStrategy": "Assert that new opportunities are properly created in database with correct analysis scores and filtering results. Verify all agent schemas are followed."
          },
          {
            "id": 4,
            "title": "Implement UPDATE and SKIP Pipeline Path Integration Tests",
            "description": "Create integration tests for UPDATE and SKIP pipeline paths, validating the abbreviated flows and early termination scenarios with proper database handling.",
            "dependencies": [
              1,
              2
            ],
            "details": "Build integration tests in `app/lib/agents-v2/tests/integration/update-skip-pipeline.test.js` for UPDATE path (DataExtraction → EarlyDuplicateDetector → Storage with update logic) and SKIP path (DataExtraction → EarlyDuplicateDetector → early termination). Validate duplicate detection accuracy, update logic correctness, and proper early termination without unnecessary processing.",
            "status": "done",
            "testStrategy": "Verify that UPDATE operations correctly modify existing opportunities and SKIP operations terminate early without database writes or unnecessary API calls."
          },
          {
            "id": 5,
            "title": "Implement Real API Integration and Error Handling Tests",
            "description": "Create comprehensive tests against actual funding source APIs with proper rate limiting, error scenarios, and data transformation validation.",
            "dependencies": [
              2
            ],
            "details": "Build real API integration tests in `app/lib/agents-v2/tests/integration/api-integration.test.js` testing against live Grants.gov, SAM.gov, and other configured APIs. Implement rate limiting respect, timeout handling, retry mechanism validation, and API error scenario testing. Validate data transformation accuracy between API responses and internal opportunity format.",
            "status": "done",
            "testStrategy": "Test with actual API endpoints using test credentials. Verify rate limiting doesn't cause failures and error scenarios are handled gracefully."
          },
          {
            "id": 6,
            "title": "Implement Performance Metrics Validation and Benchmarking",
            "description": "Create performance testing suite that validates the 60-80% token reduction and 40-50% processing time improvements compared to v1 architecture.",
            "dependencies": [
              3,
              4
            ],
            "details": "Build performance test suite in `app/lib/agents-v2/tests/integration/performance.test.js` measuring token usage, processing time, memory consumption, and database query performance. Create benchmarking utilities comparing v1 vs v2 performance. Implement metrics collection and validation for early duplicate detection efficiency. Include load testing scenarios.",
            "status": "done",
            "testStrategy": "Run comparative benchmarks against v1 architecture baseline. Validate performance improvements meet specified targets through automated assertions."
          },
          {
            "id": 7,
            "title": "Implement Local Supabase Edge Function Testing",
            "description": "Set up and test local Supabase environment with edge functions, real-time subscriptions, and database triggers using realistic data volumes.",
            "dependencies": [
              1,
              2
            ],
            "details": "Create local Supabase test setup in `app/lib/agents-v2/tests/integration/supabase-local.test.js` using `supabase start`. Test edge functions with realistic data volumes, validate real-time subscriptions during pipeline execution, test database triggers and stored procedures. Implement webhook testing and event handling validation.",
            "status": "done",
            "testStrategy": "Verify edge functions execute correctly with test data. Test real-time subscriptions receive proper events during pipeline execution."
          },
          {
            "id": 8,
            "title": "Implement Staging Environment End-to-End Validation",
            "description": "Create comprehensive end-to-end test suite for staging environment with production-like data, monitoring validation, and rollback procedures.",
            "dependencies": [
              3,
              4,
              5,
              6,
              7
            ],
            "details": "Build staging environment test suite in `app/lib/agents-v2/tests/integration/staging-e2e.test.js` deploying to staging with production-like data volumes. Test full pipeline execution, validate monitoring and alerting systems, test rollback procedures and error recovery. Include load testing with concurrent pipeline executions and database stress testing.",
            "status": "done",
            "testStrategy": "Execute full pipeline runs in staging environment. Validate monitoring dashboards show correct metrics and alerts trigger appropriately during error scenarios."
          },
          {
            "id": 9,
            "title": "Vet and validate NEW pipeline analysis data quality",
            "description": "Compare actual database entries from 33.3 tests against expected analysis outputs to verify data accuracy, field completeness, and content quality",
            "details": "",
            "status": "done",
            "dependencies": [
              "33.3"
            ],
            "parentTaskId": 33
          }
        ]
      },
      {
        "id": 34,
        "title": "Build Admin Panel V2 for Pipeline Version Switching and Run Monitoring",
        "description": "Create a comprehensive admin interface at /admin/funding-v2 that enables switching between v1 and v2 pipelines, triggering runs, and providing real-time monitoring of all pipeline stages with performance metrics.",
        "details": "1. Create /admin/funding-v2 page structure:\n   - Build React component with Next.js 15 app router pattern\n   - Use shadcn/ui components for consistent design with existing admin interface\n   - Implement role-based access control to restrict admin functionality\n   - Add navigation breadcrumbs and admin-specific layout\n\n2. Implement pipeline version switching:\n   - Create toggle interface for v1/v2 pipeline selection\n   - Add configuration persistence using Supabase for pipeline version state\n   - Implement API route to handle pipeline version changes\n   - Add validation to prevent switching during active runs\n\n3. Build run triggering interface:\n   - Create run configuration form with source selection\n   - Add run type selection (full/incremental/test)\n   - Implement run scheduling with immediate and scheduled execution\n   - Add run parameter configuration (concurrency, batch size, etc.)\n\n4. Develop real-time monitoring dashboard:\n   - Create live status display for SourceOrchestrator coordination\n   - Add DataExtraction agent progress tracking with API call metrics\n   - Implement EarlyDuplicateDetector monitoring with duplicate detection rates\n   - Build Analysis agent tracking with token usage and processing times\n   - Add Filter agent monitoring with filtering statistics\n   - Create Storage agent tracking with database write metrics\n   - Implement DirectUpdate monitoring for duplicate handling\n\n5. Add performance metrics visualization:\n   - Create charts for token usage comparison (v1 vs v2)\n   - Add processing time metrics with stage-by-stage breakdown\n   - Implement throughput monitoring (opportunities/minute)\n   - Add error rate tracking and failure analysis\n   - Create cost analysis dashboard with API usage costs\n\n6. Implement real-time updates:\n   - Use Supabase real-time subscriptions for run status updates\n   - Add WebSocket connections for live progress tracking\n   - Implement progress bars and stage completion indicators\n   - Add notification system for run completion/errors\n\n7. Create run history and logging:\n   - Build run history table with filtering and search\n   - Add detailed run logs with expandable stage details\n   - Implement error investigation tools with stack traces\n   - Create export functionality for run reports\n\n8. Add pipeline comparison tools:\n   - Create side-by-side performance comparison interface\n   - Add metrics comparison charts (v1 vs v2)\n   - Implement A/B testing configuration for pipeline versions\n   - Create recommendation engine for optimal pipeline selection",
        "testStrategy": "1. Test pipeline version switching:\n   - Verify toggle functionality correctly updates pipeline configuration\n   - Test prevention of version switching during active runs\n   - Validate configuration persistence across browser sessions\n   - Test role-based access control for admin functions\n\n2. Validate run triggering:\n   - Test run configuration form with various parameter combinations\n   - Verify run scheduling functionality with immediate and delayed execution\n   - Test source selection and validation\n   - Validate run parameter constraints and error handling\n\n3. Test real-time monitoring:\n   - Verify live updates for all seven pipeline stages\n   - Test WebSocket connections and reconnection logic\n   - Validate progress tracking accuracy against actual pipeline execution\n   - Test notification system for run events\n\n4. Validate performance metrics:\n   - Test metrics calculation accuracy by comparing with actual run data\n   - Verify chart rendering with various data ranges\n   - Test export functionality for performance reports\n   - Validate cost calculation accuracy\n\n5. Test integration with existing systems:\n   - Verify compatibility with existing admin interface (Task 12)\n   - Test integration with optimized pipeline v2 (Task 2)\n   - Validate real-time updates using Supabase subscriptions\n   - Test performance impact on overall system during monitoring\n\n6. Conduct user acceptance testing:\n   - Test admin workflow efficiency for pipeline management\n   - Verify monitoring dashboard provides actionable insights\n   - Test error investigation workflow with real failure scenarios\n   - Validate overall user experience for admin tasks",
        "status": "pending",
        "dependencies": [
          2,
          12,
          27,
          33
        ],
        "priority": "high",
        "subtasks": [
          {
            "id": 1,
            "title": "Create Admin Panel V2 Page Structure and Access Control",
            "description": "Build the foundational /admin/funding-v2 page with proper routing, layout, and role-based access control using Next.js 15 app router pattern.",
            "dependencies": [],
            "details": "Create app/(routes)/admin/funding-v2/page.tsx using Next.js 15 app router. Implement admin layout with shadcn/ui components matching existing admin interface. Add role-based access control middleware to restrict access to admin users only. Include navigation breadcrumbs and admin-specific sidebar navigation. Set up proper TypeScript types for admin components.",
            "status": "pending",
            "testStrategy": "Test page routing, access control restrictions, and UI component rendering"
          },
          {
            "id": 2,
            "title": "Implement Pipeline Version Switching Interface",
            "description": "Create toggle interface for switching between v1 and v2 pipelines with persistent configuration storage and validation against active runs.",
            "dependencies": [
              1
            ],
            "details": "Build pipeline version toggle component using shadcn/ui Switch component. Create API route /api/admin/pipeline-version for handling version changes. Implement Supabase configuration table for persisting pipeline version state. Add validation to prevent switching during active runs by checking run status. Include visual indicators for current pipeline version and switching confirmation dialogs.",
            "status": "pending",
            "testStrategy": "Test pipeline version persistence, active run validation, and UI state management"
          },
          {
            "id": 3,
            "title": "Build Run Triggering Interface and Configuration",
            "description": "Create comprehensive run configuration form with source selection, run types, scheduling, and parameter configuration capabilities.",
            "dependencies": [
              2
            ],
            "details": "Build run configuration form using shadcn/ui Form components. Add dropdown for funding source selection from database. Implement run type selection (full/incremental/test) with radio buttons. Create scheduling interface with immediate and future execution options. Add parameter configuration for concurrency limits and batch sizes. Include run description and tagging fields for organization.",
            "status": "pending",
            "testStrategy": "Test form validation, scheduling logic, and parameter range validation"
          },
          {
            "id": 4,
            "title": "Develop Real-time Monitoring Dashboard Core",
            "description": "Create the main monitoring dashboard with live status displays for all pipeline stages including SourceOrchestrator, DataExtraction, EarlyDuplicateDetector, Analysis, Filter, Storage, and DirectUpdate agents.",
            "dependencies": [
              3
            ],
            "details": "Build monitoring dashboard layout with grid system for different agent status cards. Create individual status components for each agent type with progress indicators. Implement real-time status updates using Supabase subscriptions. Add stage completion indicators and current processing counts. Include agent-specific metrics like API call counts, duplicate detection rates, and processing times.",
            "status": "pending",
            "testStrategy": "Test real-time updates, status accuracy, and dashboard responsiveness"
          },
          {
            "id": 5,
            "title": "Add Performance Metrics Visualization",
            "description": "Implement comprehensive performance metrics with charts for token usage, processing times, throughput, error rates, and cost analysis comparing v1 vs v2 pipelines.",
            "dependencies": [
              4
            ],
            "details": "Create performance metrics dashboard using chart library (recharts or similar). Build token usage comparison charts between v1 and v2 pipelines. Add processing time breakdowns by stage with bar and line charts. Implement throughput monitoring with opportunities per minute tracking. Create error rate visualization with trend analysis. Build cost analysis dashboard calculating API usage costs and efficiency metrics.",
            "status": "pending",
            "testStrategy": "Test chart data accuracy, performance metric calculations, and comparison logic"
          },
          {
            "id": 6,
            "title": "Implement Real-time Updates System",
            "description": "Set up comprehensive real-time update system using Supabase subscriptions and WebSocket connections for live progress tracking and notifications.",
            "dependencies": [
              5
            ],
            "details": "Implement Supabase real-time subscriptions for run status table updates. Create WebSocket connection management for live progress tracking. Build progress bar components with percentage completion and ETA calculations. Add notification system using toast notifications for run completion and error alerts. Implement automatic dashboard refresh and state synchronization.",
            "status": "pending",
            "testStrategy": "Test real-time connection stability, progress accuracy, and notification delivery"
          },
          {
            "id": 7,
            "title": "Create Run History and Logging System",
            "description": "Build comprehensive run history interface with detailed logging, error investigation tools, and export functionality for administrative analysis.",
            "dependencies": [
              6
            ],
            "details": "Create run history table component with pagination, filtering, and search capabilities. Build expandable run detail views with stage-by-stage logs. Implement error investigation tools with stack trace display and error categorization. Add export functionality for run reports in JSON/CSV formats. Include run comparison features for analyzing performance differences.",
            "status": "pending",
            "testStrategy": "Test data filtering, log detail accuracy, and export functionality"
          },
          {
            "id": 8,
            "title": "Add Pipeline Comparison and A/B Testing Tools",
            "description": "Implement advanced pipeline comparison interface with side-by-side metrics, A/B testing configuration, and recommendation engine for optimal pipeline selection.",
            "dependencies": [
              7
            ],
            "details": "Build side-by-side comparison interface for v1 vs v2 performance metrics. Create comparison charts with overlays and difference highlighting. Implement A/B testing configuration with percentage traffic splitting. Build recommendation engine that analyzes historical performance data to suggest optimal pipeline version. Add configuration persistence for A/B test settings and automatic switching based on performance thresholds.",
            "status": "pending",
            "testStrategy": "Test comparison accuracy, A/B testing logic, and recommendation algorithm effectiveness"
          },
          {
            "id": 9,
            "title": "Implement API Response Tracking Utilities",
            "description": "Build lightweight utilities for response inspection, basic metrics for response frequency and patterns, and simple monitoring for response capture success rates within the admin dashboard. This includes API raw response monitoring, response tracking visualizations showing API call patterns and deduplication statistics, and debugging interface for inspecting raw API responses and their processing status.",
            "details": "",
            "status": "pending",
            "dependencies": [
              "34.4"
            ],
            "parentTaskId": 34
          },
          {
            "id": 10,
            "title": "Create Dashboard-Ready Query APIs",
            "description": "Implement API endpoints and sample queries for dashboard consumption: real-time progress tracking, optimization impact visualization, stage performance analytics, historical trending, and pipeline health monitoring. Design for Task 34 admin panel integration.",
            "details": "",
            "status": "pending",
            "dependencies": [
              "34.4"
            ],
            "parentTaskId": 34
          }
        ]
      },
      {
        "id": 35,
        "title": "Implement Enhanced API Raw Response Capture System",
        "description": "Develop a streamlined API raw response capture system that provides perfect opportunity-to-API traceability while maintaining storage efficiency through intelligent response updating and tracking.",
        "status": "done",
        "dependencies": [
          1
        ],
        "priority": "high",
        "details": "1. Add tracking fields to existing api_raw_responses table:\n   - Add first_seen_at, last_seen_at, and call_count fields\n   - Track when responses were first captured and last updated\n   - Count frequency of identical responses across runs\n\n2. Modify storeRawResponse to UPDATE existing records:\n   - Instead of skipping duplicate responses, update existing records with fresh metadata\n   - Update last_seen_at timestamp and increment call_count\n   - Preserve original response data while tracking recurrence patterns\n\n3. Enhance EarlyDuplicateDetector for perfect traceability:\n   - Include raw_response_id when updating funding opportunities\n   - Create direct linkage between opportunities and their source API responses\n   - Maintain audit trail without complex schema changes\n\n4. Implement response tracking utilities:\n   - Create simple debugging tools for response inspection\n   - Add basic metrics for response frequency and patterns\n   - Build lightweight monitoring for response capture success rates",
        "testStrategy": "1. Test response tracking field functionality:\n   - Verify first_seen_at, last_seen_at, and call_count updates\n   - Test timestamp accuracy and call_count incrementation\n   - Validate tracking across multiple runs and sources\n\n2. Validate storeRawResponse update behavior:\n   - Test UPDATE operations instead of INSERT for duplicates\n   - Verify metadata updates without data loss\n   - Test performance impact of UPDATE vs INSERT operations\n\n3. Test enhanced EarlyDuplicateDetector integration:\n   - Verify raw_response_id inclusion in opportunity updates\n   - Test traceability from opportunity back to source response\n   - Validate audit trail completeness and accuracy\n\n4. Integration testing with existing pipeline:\n   - Test seamless integration with current Data Extraction Agent\n   - Verify no disruption to existing deduplication benefits\n   - Test response tracking during error scenarios and retries\n\n5. Performance and efficiency testing:\n   - Measure storage efficiency improvements\n   - Test response capture success rates\n   - Validate minimal performance impact on existing pipeline",
        "subtasks": [
          {
            "id": 1,
            "title": "Add Tracking Fields to api_raw_responses Table",
            "description": "Extend existing api_raw_responses table with first_seen_at, last_seen_at, and call_count fields",
            "status": "done",
            "dependencies": [],
            "details": "Add three new fields to the existing api_raw_responses table: first_seen_at (timestamp for initial capture), last_seen_at (timestamp for most recent update), and call_count (integer tracking frequency of identical responses). Create database migration to add these fields with appropriate defaults.",
            "testStrategy": "Create migration tests, verify field additions, test default values, and validate data types and constraints"
          },
          {
            "id": 2,
            "title": "Modify storeRawResponse to UPDATE Existing Records",
            "description": "Change storeRawResponse logic to update existing records instead of skipping duplicates",
            "status": "done",
            "dependencies": [
              1
            ],
            "details": "Modify the storeRawResponse function to perform UPDATE operations on existing records instead of skipping duplicates. Update last_seen_at timestamp and increment call_count while preserving original response data and first_seen_at timestamp.\n<info added on 2025-07-09T09:07:46.707Z>\nSuccessfully implemented the UPDATE functionality for existing records in storeRawResponse function. The system now updates last_seen_at timestamp and increments call_count while preserving original response data and first_seen_at timestamp. Enhanced to capture all detail API calls individually for two-step APIs like Grants.gov, with each detail call stored using call_type='detail' and comprehensive metadata including execution_time_ms, api_endpoint, and opportunity_count. Testing confirmed proper tracking of 5 detail responses plus 1 list response.\n</info added on 2025-07-09T09:07:46.707Z>",
            "testStrategy": "Unit tests for UPDATE logic, verify metadata updates, test call_count incrementation, and validate timestamp handling"
          },
          {
            "id": 3,
            "title": "Enhance EarlyDuplicateDetector with raw_response_id",
            "description": "Modify EarlyDuplicateDetector to include raw_response_id when updating opportunities",
            "status": "done",
            "dependencies": [
              2
            ],
            "details": "Update the EarlyDuplicateDetector to include raw_response_id field when updating funding opportunities, creating direct linkage between opportunities and their source API responses for perfect traceability.",
            "testStrategy": "Test raw_response_id inclusion in opportunity updates, verify traceability links, and validate audit trail completeness"
          },
          {
            "id": 5,
            "title": "Clean up redundant timestamp fields in api_raw_responses",
            "description": "Remove duplicate timestamp fields from api_raw_responses table. Currently has 'timestamp', 'created_at', and 'first_seen_at' which are essentially the same for new records. Keep 'first_seen_at' as the creation timestamp and 'last_seen_at' for updates. Remove 'timestamp' and 'created_at' fields to eliminate redundancy.",
            "details": "",
            "status": "done",
            "dependencies": [],
            "parentTaskId": 35
          }
        ]
      },
      {
        "id": 36,
        "title": "Implement Clean V2 Pipeline Metrics System",
        "description": "Develop a comprehensive metrics system for the V2 pipeline with semantic database tables, enhanced RunManagerV2, optimized ProcessCoordinatorV2, improved EarlyDuplicateDetector metrics, and updated integration tests for dashboard analytics.",
        "details": "1. Create semantic database tables for V2 pipeline metrics:\n   - pipeline_metrics table with columns: id, run_id, pipeline_stage, start_time, end_time, duration_ms, success_count, error_count, token_usage, memory_usage\n   - stage_performance table with columns: id, stage_name, avg_duration_ms, success_rate, error_rate, last_updated\n   - duplicate_detection_metrics table with columns: id, run_id, total_opportunities, duplicates_found, duplicates_skipped, detection_accuracy, processing_time_saved_ms\n\n2. Enhance RunManagerV2 with comprehensive metrics collection:\n   - Add performance tracking for each pipeline stage (DataExtraction, EarlyDuplicateDetector, Analysis, Filter, Storage)\n   - Implement token usage tracking with detailed breakdown by agent\n   - Add memory usage monitoring and garbage collection metrics\n   - Create real-time metrics streaming to database\n   - Implement metrics aggregation and rollup functionality\n\n3. Optimize ProcessCoordinatorV2 with metrics-driven orchestration:\n   - Add pipeline performance monitoring with stage-by-stage timing\n   - Implement adaptive batch sizing based on performance metrics\n   - Add circuit breaker patterns for failing stages\n   - Create metrics-based auto-scaling for concurrent processing\n   - Implement performance threshold alerts and notifications\n\n4. Improve EarlyDuplicateDetector with detailed metrics:\n   - Track duplicate detection accuracy and false positive rates\n   - Monitor processing time savings from early duplicate detection\n   - Add metrics for ID + Title validation logic performance\n   - Implement duplicate pattern analysis for optimization\n   - Create metrics for database update efficiency on duplicates\n\n5. Update integration tests for comprehensive metrics validation:\n   - Create test scenarios for all metrics collection points\n   - Validate metrics accuracy across different pipeline configurations\n   - Test performance impact of metrics collection overhead\n   - Implement metrics regression testing for performance degradation detection\n   - Create dashboard analytics integration tests\n\n6. Implement dashboard analytics integration:\n   - Create real-time metrics API endpoints for dashboard consumption\n   - Implement metrics visualization components using Chart.js v4.3.0\n   - Add historical performance trending and analysis\n   - Create alerting system for performance anomalies\n   - Implement metrics export functionality for reporting",
        "testStrategy": "1. Test semantic database table functionality:\n   - Verify correct metrics data insertion and retrieval\n   - Test table performance with large datasets\n   - Validate data integrity and foreign key relationships\n   - Test metrics aggregation queries performance\n\n2. Validate RunManagerV2 metrics collection:\n   - Test metrics accuracy across all pipeline stages\n   - Verify token usage tracking precision\n   - Test memory usage monitoring accuracy\n   - Validate real-time metrics streaming functionality\n\n3. Test ProcessCoordinatorV2 optimization features:\n   - Verify adaptive batch sizing based on performance metrics\n   - Test circuit breaker functionality under failure conditions\n   - Validate auto-scaling behavior with varying loads\n   - Test performance threshold alerting system\n\n4. Validate EarlyDuplicateDetector metrics improvements:\n   - Test duplicate detection accuracy measurement\n   - Verify processing time savings calculations\n   - Test duplicate pattern analysis functionality\n   - Validate database update efficiency metrics\n\n5. Execute comprehensive integration tests:\n   - Test end-to-end metrics collection through complete pipeline runs\n   - Validate metrics data consistency across different scenarios\n   - Test performance impact of metrics collection (should be <5% overhead)\n   - Verify dashboard analytics integration accuracy\n\n6. Conduct performance regression testing:\n   - Compare V2 pipeline performance with and without metrics\n   - Validate that metrics collection doesn't degrade core functionality\n   - Test metrics system scalability with high-volume processing\n   - Verify dashboard real-time updates performance",
        "status": "in-progress",
        "dependencies": [
          1,
          6
        ],
        "priority": "high",
        "subtasks": [
          {
            "id": 1,
            "title": "Create Clean V2 Database Schema",
            "description": "Create semantic database tables: pipeline_runs, pipeline_stages, opportunity_processing_paths, duplicate_detection_sessions, and pipeline_performance_baselines with proper indexes and foreign keys. Design for dashboard analytics and future-proof extensibility.",
            "details": "",
            "status": "done",
            "dependencies": [],
            "parentTaskId": 36
          },
          {
            "id": 2,
            "title": "Enhance RunManagerV2 with Clean Metrics",
            "description": "Modify existing RunManagerV2 to use new semantic tables instead of V1-mapped fields. Add comprehensive stage tracking, optimization impact measurement, token usage analytics, and real-time metrics capture without creating new manager classes.",
            "details": "",
            "status": "done",
            "dependencies": [],
            "parentTaskId": 36
          },
          {
            "id": 3,
            "title": "Update ProcessCoordinatorV2 for Enhanced Metrics Capture",
            "description": "Modify ProcessCoordinatorV2 to capture optimization insights, pipeline branching analytics, token savings calculations, and performance improvements. Record opportunity flow paths (NEW/UPDATE/SKIP) and feed data into new metrics system.",
            "details": "",
            "status": "done",
            "dependencies": [],
            "parentTaskId": 36
          },
          {
            "id": 4,
            "title": "Enhance EarlyDuplicateDetector Metrics Collection",
            "description": "Extend EarlyDuplicateDetector to capture detailed analytics: detection accuracy, false positive rates, processing time savings, ID+Title validation performance, duplicate pattern analysis, and database update efficiency. Store results in duplicate_detection_sessions table.",
            "details": "",
            "status": "done",
            "dependencies": [],
            "parentTaskId": 36
          },
          {
            "id": 5,
            "title": "Update Integration Tests for New Metrics System",
            "description": "Modify existing test-new-pipeline-path.js to validate new metrics capture, test optimization analytics, verify opportunity path tracking, validate duplicate detection metrics, and ensure dashboard query compatibility. No new test files - enhance existing integration tests.",
            "details": "",
            "status": "done",
            "dependencies": [],
            "parentTaskId": 36
          }
        ]
      },
      {
        "id": 37,
        "title": "Database Pruning Initiative - Remove unused tables and optimize schema",
        "description": "Comprehensive V2 Pipeline Optimization Initiative that combines database schema cleanup with critical performance enhancements to achieve 60-80% performance improvements in the V2 architecture. This multi-phase project encompasses database pruning, query optimization, architecture improvements, and advanced database features implementation.",
        "status": "pending",
        "dependencies": [
          1,
          2
        ],
        "priority": "high",
        "details": "PHASE 1 - Database Schema Analysis and Cleanup:\n   - Generate complete table usage report using pg_stat_user_tables\n   - Identify tables with zero or minimal activity over past 6 months\n   - Document foreign key relationships and dependencies\n   - Map table usage across application codebase using grep/ripgrep searches\n   - Remove legacy tables, temporary tables, and redundant structures\n   - Consolidate overlapping functionality where appropriate\n\nPHASE 2 - V2 Pipeline Performance Optimization:\n   - Implement strategic database indexes for frequently queried columns\n   - Identify and fix N+1 query patterns in funding opportunities processing\n   - Eliminate dual write operations causing performance bottlenecks\n   - Optimize agent-based processing workflows for better throughput\n   - Implement query batching and connection pooling improvements\n\nPHASE 3 - Advanced Database Features:\n   - Design and implement table partitioning for large datasets (funding_opportunities, api_raw_responses)\n   - Create materialized views for complex aggregations (dashboard metrics, geographic data)\n   - Develop optimized database functions for common operations\n   - Implement proper caching strategies at database level\n\nPHASE 4 - Migration and Testing Strategy:\n   - Create safe migration procedures with rollback capabilities\n   - Implement performance monitoring and benchmarking\n   - Validate 60-80% improvement targets through comprehensive testing\n   - Update application code to leverage optimized database features",
        "testStrategy": "1. Baseline Performance Measurement:\n   - Document current V2 pipeline execution times and resource usage\n   - Record database query performance metrics and connection statistics\n   - Establish benchmarks for agent processing throughput and latency\n   - Measure current database size, table counts, and index usage\n\n2. Phase-by-Phase Validation:\n   - Test application functionality after each schema change\n   - Validate API endpoints and agent workflows after optimization\n   - Measure incremental performance improvements throughout implementation\n   - Verify data integrity and consistency across all optimizations\n\n3. Performance Target Validation:\n   - Confirm 60-80% improvement in V2 pipeline execution times\n   - Validate reduced memory usage and improved connection efficiency\n   - Test scalability under increased load with optimized architecture\n   - Verify materialized views and partitioning provide expected benefits\n\n4. Rollback and Recovery Testing:\n   - Test migration rollback procedures for each optimization phase\n   - Validate backup and restore processes with new schema features\n   - Ensure rolled-back state maintains full V2 pipeline functionality\n   - Test disaster recovery scenarios with optimized database structure",
        "subtasks": [
          {
            "id": 1,
            "title": "Remove unused funding_programs table from database schema",
            "description": "Analyze the funding_programs table usage across the codebase and remove it from the database schema if it's confirmed to be unused. This includes dropping the table, removing any references in migrations, and updating any related documentation.",
            "status": "pending",
            "dependencies": [],
            "details": "",
            "testStrategy": ""
          },
          {
            "id": 2,
            "title": "Rename agency_type field to source_type in funding_sources table",
            "description": "Rename the agency_type column to source_type in the funding_sources table to better reflect its purpose of categorizing funding source types (Federal, State, Local, etc.) rather than specifically agency types. This includes creating a database migration, updating all code references, and ensuring all functionality continues to work.",
            "status": "pending",
            "dependencies": [],
            "details": "<info added on 2025-08-02T04:25:37.817Z>\n1. Create a Supabase migration file to rename the column from agency_type to source_type. 2. Search the entire codebase for references to agency_type and update them to source_type, including API routes, components, types, and database queries. 3. Update any TypeScript interfaces or types that reference agency_type. 4. Test all funding source-related functionality to ensure the rename doesn't break anything. 5. Update any documentation or comments that reference the old field name. 6. Run the migration in development and test environments before production deployment.\n</info added on 2025-08-02T04:25:37.817Z>",
            "testStrategy": ""
          },
          {
            "id": 3,
            "title": "Fix agency_type enum case and values",
            "description": "Change the PostgreSQL enum from title case (Federal, State) to lowercase (federal, state). Add local as a new enum value option. Update enum values to be: federal, state, utility, foundation, local, other. Create database migration to alter the enum type. Update any existing data to use lowercase values.",
            "status": "pending",
            "dependencies": [],
            "details": "",
            "testStrategy": ""
          },
          {
            "id": 4,
            "title": "Fix categorizeAgencyType function",
            "description": "Change default fallback from government to other. Ensure function returns only valid enum values that match the database. Test that the function properly categorizes agencies and no longer causes enum constraint violations. Verify that NULL values in agency_type get resolved.",
            "status": "pending",
            "dependencies": [],
            "details": "",
            "testStrategy": ""
          },
          {
            "id": 5,
            "title": "Test API raw responses deduplication feature",
            "description": "Verify that the deduplication logic is working correctly for raw API responses before they're processed into opportunities. This should test the early duplicate detection mechanism to ensure redundant processing is prevented and data consistency is maintained.",
            "status": "pending",
            "dependencies": [],
            "details": "",
            "testStrategy": ""
          },
          {
            "id": 6,
            "title": "Analyze api_extracted_opportunities table for potential removal",
            "description": "Analyze the usage and necessity of the api_extracted_opportunities intermediate table in the data processing pipeline. Determine if this table can be removed to simplify the architecture by examining current usage patterns, dependencies, performance impact, and migration strategy if removal is beneficial.",
            "status": "pending",
            "dependencies": [],
            "details": "",
            "testStrategy": ""
          },
          {
            "id": 7,
            "title": "Implement strategic database indexes for V2 pipeline optimization",
            "description": "Analyze query patterns in the V2 pipeline and implement strategic indexes on frequently queried columns in funding_opportunities, funding_sources, and runs tables. Focus on indexes that will provide the most significant performance improvements for agent processing workflows.",
            "status": "pending",
            "dependencies": [],
            "details": "",
            "testStrategy": ""
          },
          {
            "id": 8,
            "title": "Identify and fix N+1 query patterns in funding opportunities processing",
            "description": "Audit the V2 pipeline codebase to identify N+1 query patterns that are causing performance bottlenecks. Implement solutions such as eager loading, query batching, or JOIN optimizations to eliminate these inefficient query patterns.",
            "status": "pending",
            "dependencies": [],
            "details": "",
            "testStrategy": ""
          },
          {
            "id": 9,
            "title": "Eliminate dual write operations causing performance bottlenecks",
            "description": "Identify and eliminate dual write operations in the V2 pipeline that are causing unnecessary database load and performance degradation. Consolidate write operations and implement more efficient data persistence patterns.",
            "status": "pending",
            "dependencies": [],
            "details": "",
            "testStrategy": ""
          },
          {
            "id": 10,
            "title": "Implement table partitioning for large datasets",
            "description": "Design and implement table partitioning strategies for large tables like funding_opportunities and api_raw_responses to improve query performance and maintenance operations. Focus on time-based or geographic partitioning strategies that align with common query patterns.",
            "status": "pending",
            "dependencies": [],
            "details": "",
            "testStrategy": ""
          },
          {
            "id": 11,
            "title": "Create materialized views for complex aggregations",
            "description": "Implement materialized views for complex aggregations used in dashboard metrics and geographic data visualization. This should significantly improve query performance for frequently accessed aggregated data while maintaining data freshness through appropriate refresh strategies.",
            "status": "pending",
            "dependencies": [],
            "details": "",
            "testStrategy": ""
          },
          {
            "id": 12,
            "title": "Develop optimized database functions for common operations",
            "description": "Create optimized PostgreSQL functions for frequently used operations in the V2 pipeline, such as duplicate detection, data aggregation, and complex filtering logic. This will reduce application-level processing and improve overall pipeline performance.",
            "status": "pending",
            "dependencies": [],
            "details": "",
            "testStrategy": ""
          },
          {
            "id": 13,
            "title": "Add Missing Composite Indexes for V2 Pipeline",
            "description": "Create critical composite indexes for V2 pipeline performance: idx_pipeline_stages_run_order_status, idx_opportunity_paths_duplicate_analysis, idx_pipeline_runs_analytics. Expected impact: 3-5x query speedup for pipeline operations. Priority: HIGH (Week 1-2)",
            "details": "",
            "status": "done",
            "dependencies": [],
            "parentTaskId": 37
          },
          {
            "id": 14,
            "title": "Fix N+1 Query Patterns in StorageAgent",
            "description": "Implement batch operations for opportunity storage in StorageAgent. Replace individual inserts with bulk operations. Target locations: app/lib/agents-v2/storageAgent.js. Expected impact: Reduce database round trips by 80%. Priority: HIGH (Week 1-2)",
            "details": "",
            "status": "done",
            "dependencies": [],
            "parentTaskId": 37
          },
          {
            "id": 15,
            "title": "Eliminate Dual V1/V2 Writes in RunManagerV2",
            "description": "Remove V1 table writes from V2 pipeline in RunManagerV2. Keep only V2 persistence for new runs. Target locations: app/lib/agents-v2/services/runManagerV2.js. Expected impact: 20-30% performance improvement. Priority: HIGH (Week 1-2)",
            "details": "",
            "status": "done",
            "dependencies": [],
            "parentTaskId": 37
          },
          {
            "id": 16,
            "title": "Implement Table Partitioning Strategy",
            "description": "Partition pipeline_runs by month and opportunity_processing_paths by hash. Add archival strategy for old data. Design automated partition management. Expected impact: Improved query performance on large datasets. Priority: MEDIUM (Month 2-3)",
            "details": "",
            "status": "pending",
            "dependencies": [],
            "parentTaskId": 37
          },
          {
            "id": 17,
            "title": "Create Materialized Views for Dashboard Performance",
            "description": "Build mv_pipeline_efficiency and mv_duplicate_detection_effectiveness materialized views. Add refresh scheduling and integrate with dashboard queries. Expected impact: Faster dashboard load times. Priority: MEDIUM (Month 2-3)",
            "details": "",
            "status": "pending",
            "dependencies": [],
            "parentTaskId": 37
          },
          {
            "id": 18,
            "title": "Move Duplicate Detection to Database Functions",
            "description": "Create optimized SQL function for duplicate checking. Replace application-level duplicate logic with database functions. Implement similarity threshold configuration. Expected impact: Faster duplicate detection. Priority: MEDIUM (Month 2-3)",
            "details": "",
            "status": "pending",
            "dependencies": [],
            "parentTaskId": 37
          },
          {
            "id": 19,
            "title": "Add Data Integrity Constraints",
            "description": "Add cross-table consistency checks, standardize foreign key cascade rules, and add enum validation constraints. Improve database data quality and referential integrity. Priority: LOW (Month 4+)",
            "details": "",
            "status": "pending",
            "dependencies": [],
            "parentTaskId": 37
          },
          {
            "id": 20,
            "title": "Implement Comprehensive Analytics Dashboard",
            "description": "Build dashboards consuming V2 metrics, add real-time efficiency monitoring, and create cost savings calculations. Showcase V2 optimization results and provide ongoing performance insights. Priority: LOW (Month 4+)",
            "details": "",
            "status": "pending",
            "dependencies": [],
            "parentTaskId": 37
          }
        ]
      },
      {
        "id": 38,
        "title": "V2 Pipeline Status Handling Improvements",
        "description": "Implement comprehensive improvements to the V2 process runs system with a primary focus on removing V1 vs V2 comparison metrics and replacing them with meaningful absolute performance metrics. This architecture change eliminates over-engineered comparison tracking in favor of direct throughput, efficiency, cost, and quality measurements that provide actionable insights for system optimization. Additionally, enhance overall system reliability, error handling, and monitoring capabilities.",
        "status": "in-progress",
        "dependencies": [
          36,
          2,
          6
        ],
        "priority": "high",
        "details": "1. **Primary Focus: Eliminate V1 vs V2 Comparison Architecture**\n   - Remove all percentage-based comparison metrics (token_savings_percentage, time_savings_percentage, efficiency_score)\n   - Replace with absolute performance metrics: opportunities_per_minute, tokens_per_opportunity, cost_per_opportunity_usd\n   - Add quality metrics: success_rate_percentage, sla_compliance_percentage\n   - Remove comparison baseline tracking and pipeline_performance_baselines table\n   - Update all UI components to display absolute values instead of V1 vs V2 improvements\n\n2. **Database Architecture Transformation**\n   - Migrate database schema to support absolute performance tracking\n   - Remove comparison-focused tables and fields\n   - Add new absolute performance fields with proper constraints and indexes\n   - Create automated calculation functions and triggers for real-time metrics\n\n3. **Frontend and API Updates**\n   - Update admin dashboard pages to show absolute performance metrics\n   - Replace V2 Optimization Impact displays with meaningful throughput dashboards\n   - Modify API endpoints to serve absolute performance data\n   - Update table components to display actionable performance columns\n\n4. **Critical System Reliability Fixes**\n   - Fix pipeline runs stuck in 'started/processing' status\n   - Implement comprehensive error handling and timeout mechanisms\n   - Add proper status transition logging and recovery strategies\n   - Ensure all processing paths update run status appropriately\n\n5. **Enhanced Monitoring and Performance Tracking**\n   - Implement performance metrics collection focused on absolute values\n   - Add execution time tracking and resource utilization monitoring\n   - Create performance profiling capabilities for bottleneck identification\n   - Add system health checks and diagnostic tools\n\n6. **Improved User Experience**\n   - Provide meaningful performance insights instead of abstract comparisons\n   - Enhance real-time updates and progress reporting\n   - Improve error messaging and system feedback\n   - Create actionable performance dashboards for optimization decisions",
        "testStrategy": "1. **Validate Architecture Migration**\n   - Verify all V1 vs V2 comparison metrics are completely removed\n   - Test absolute performance metric accuracy and calculation correctness\n   - Validate database migration integrity and data preservation\n   - Test automated performance calculation functions and triggers\n\n2. **Test Frontend and API Changes**\n   - Verify admin dashboard displays absolute performance metrics correctly\n   - Test API endpoints serve proper absolute performance data\n   - Validate table components show actionable performance information\n   - Test UI responsiveness and data visualization accuracy\n\n3. **Validate System Reliability Improvements**\n   - Test pipeline runs complete with proper status updates\n   - Verify timeout mechanisms prevent indefinite hanging\n   - Test error handling prevents system crashes\n   - Validate status recovery mechanisms for orphaned runs\n\n4. **Test Performance Monitoring**\n   - Verify performance metrics collection accuracy\n   - Test execution time tracking and resource monitoring\n   - Validate performance profiling effectiveness\n   - Test system health checks and diagnostic capabilities\n\n5. **Integration and End-to-End Testing**\n   - Test complete pipeline execution with absolute performance tracking\n   - Verify user experience improvements and dashboard functionality\n   - Test system performance under various load conditions\n   - Validate overall system stability and reliability",
        "subtasks": [
          {
            "id": 1,
            "title": "Fix Status Stuck in 'Started/Processing' Issue",
            "description": "Implement immediate fixes for pipeline runs that get stuck in 'started' or 'processing' status and never update. This is the critical first step to prevent indefinite hanging and improve system reliability.",
            "status": "done",
            "dependencies": [
              "38.4"
            ],
            "details": "<info added on 2025-08-02T17:30:07.109Z>\nPROBLEM ANALYSIS:\n- Pipeline runs remain indefinitely in 'started' or 'processing' status\n- Indicates system crashes, timeouts, or unhandled exceptions before status updates\n- Affects monitoring, debugging, and user experience\n- Critical reliability issue for V2 agent architecture\n\nIMMEDIATE FIXES NEEDED:\n1. Add comprehensive try-catch blocks around all status transition points\n2. Implement timeout mechanisms for long-running processes (e.g., 30-minute default timeout)\n3. Add proper error handling in RunManagerV2 status update methods\n4. Ensure all code paths (success, error, timeout) update run status appropriately\n5. Add status transition logging for debugging purposes\n6. Implement automatic status recovery for orphaned runs on system restart\n\nSPECIFIC CODE AREAS TO REVIEW:\n- app/lib/agents-v2/services/RunManagerV2.js (status update methods)\n- app/lib/agents-v2/orchestrators/SourceOrchestrator.js (processing loops)\n- app/api/admin/run-source/route.js (API endpoint error handling)\n- Any agent processing methods that don't properly handle exceptions\n\nSUCCESS CRITERIA:\n- No runs remain stuck in 'started'/'processing' for more than timeout period\n- All processing paths update status on completion or failure\n- System handles crashes gracefully with proper status cleanup\n- Comprehensive logging enables effective debugging of status issues\n</info added on 2025-08-02T17:30:07.109Z>",
            "testStrategy": ""
          },
          {
            "id": 2,
            "title": "Enhance V2 Pipeline Error Handling",
            "description": "Implement comprehensive error handling throughout the V2 pipeline to improve system reliability and provide better error recovery mechanisms.",
            "status": "pending",
            "dependencies": [
              1,
              "38.1"
            ],
            "details": "- Add structured error handling in all V2 agents and orchestrators\n- Implement proper exception handling with contextual error information\n- Create error classification and categorization system\n- Add retry mechanisms for transient failures\n- Implement graceful degradation for non-critical failures\n- Add error logging and reporting for debugging and monitoring",
            "testStrategy": ""
          },
          {
            "id": 3,
            "title": "Improve V2 Performance Monitoring",
            "description": "Implement comprehensive performance monitoring and metrics collection for the V2 pipeline focused on absolute performance values rather than V1 comparisons.",
            "status": "pending",
            "dependencies": [
              2
            ],
            "details": "- Add absolute performance metrics collection: opportunities_per_minute, tokens_per_opportunity, cost_per_opportunity_usd\n- Implement quality metrics tracking: success_rate_percentage, sla_compliance_percentage\n- Create execution time tracking and resource utilization monitoring\n- Add performance profiling capabilities for bottleneck identification\n- Implement performance alerting based on absolute thresholds\n- Remove all V1 vs V2 comparison metric calculations",
            "testStrategy": ""
          },
          {
            "id": 4,
            "title": "Database Migration Phase 1: Add Absolute Performance Fields",
            "description": "Add new absolute performance tracking fields to pipeline_runs and duplicate_detection_sessions tables. Fields to add: opportunities_per_minute (throughput), tokens_per_opportunity (efficiency), cost_per_opportunity_usd (cost efficiency), success_rate_percentage (quality), sla_compliance_percentage (target tracking). Create migration file in supabase/migrations/ with proper column definitions and constraints.",
            "status": "done",
            "dependencies": [],
            "details": "<info added on 2025-08-03T20:44:43.193Z>\nRemove superfluous analytics fields from database tables to simplify tracking and focus on measurable business value. Remove from duplicate_detection_sessions table: estimated_tokens_saved, estimated_cost_saved_usd, efficiency_improvement_percentage, detection_accuracy_score, false_positive_rate, false_negative_rate, detection_config. Remove from pipeline_runs table: token_savings_percentage, time_savings_percentage, efficiency_score, opportunities_bypassed_llm. Keep only fields that provide real, measurable business insights rather than theoretical metrics that add complexity without clear value.\n</info added on 2025-08-03T20:44:43.193Z>",
            "testStrategy": ""
          },
          {
            "id": 5,
            "title": "Database Migration Phase 2: Remove Comparison Fields",
            "description": "Remove V1 vs V2 comparison fields from database tables. Remove from pipeline_runs: token_savings_percentage, time_savings_percentage, efficiency_score, opportunities_bypassed_llm. Remove from duplicate_detection_sessions: estimated_tokens_saved, estimated_cost_saved_usd, efficiency_improvement_percentage. Archive existing comparison data before removal. Create migration file in supabase/migrations/.",
            "status": "pending",
            "dependencies": [
              4,
              "38.4"
            ],
            "details": "",
            "testStrategy": ""
          },
          {
            "id": 6,
            "title": "Database Migration Phase 3: Remove Baseline Comparison Table",
            "description": "Remove the entire pipeline_performance_baselines table as it exists only for V1 comparison tracking. Archive table data first, then drop the table completely. Update any foreign key references or dependent views. Create migration file in supabase/migrations/. This table serves no purpose in the new absolute performance tracking approach.",
            "status": "pending",
            "dependencies": [
              5,
              "38.5"
            ],
            "details": "",
            "testStrategy": ""
          },
          {
            "id": 7,
            "title": "Update Admin Runs Detail Page - Remove V2 Optimization Card",
            "description": "Update /app/admin/funding-sources/runs/[id]/pageV2.jsx to remove the V2 Optimization Impact card that displays comparison metrics. Replace with absolute performance metrics display showing opportunities_per_minute, tokens_per_opportunity, cost_per_opportunity_usd, success_rate_percentage, and sla_compliance_percentage. Update the page layout and styling accordingly.",
            "status": "pending",
            "dependencies": [
              "38.4",
              "38.10",
              "38.12"
            ],
            "details": "",
            "testStrategy": ""
          },
          {
            "id": 8,
            "title": "Update Admin Funding Sources Page - Remove V2 Pipeline Performance",
            "description": "Update /app/admin/funding-sources/[id]/pageV2.jsx to remove V2 Pipeline Performance comparison metrics section. Replace with absolute performance dashboard showing real throughput metrics, cost efficiency, success rates, and SLA compliance. Update charts and visualizations to display absolute values instead of percentage improvements over V1.",
            "status": "pending",
            "dependencies": [
              "38.4",
              "38.10",
              "38.12"
            ],
            "details": "",
            "testStrategy": ""
          },
          {
            "id": 9,
            "title": "Update RunsTableV2 Component - Replace Optimization Column",
            "description": "Update /components/admin/RunsTable/RunsTableV2.jsx to replace the Optimization column with absolute performance metrics columns. Add columns for throughput (opportunities/min), efficiency (tokens/opportunity), cost efficiency ($/opportunity), success rate %, and SLA compliance %. Update table sorting, filtering, and display logic to work with absolute values instead of comparison percentages.",
            "status": "pending",
            "dependencies": [
              "38.4",
              "38.10",
              "38.12"
            ],
            "details": "",
            "testStrategy": ""
          },
          {
            "id": 10,
            "title": "Update RunManagerV2 Service - Remove Comparison Calculations",
            "description": "Update /lib/services/runManagerV2.js to remove all V1 vs V2 comparison metric calculations. Replace with absolute performance metric calculations for opportunities_per_minute, tokens_per_opportunity, cost_per_opportunity_usd, success_rate_percentage, and sla_compliance_percentage. Update calculateMetrics functions and remove comparison baseline lookups.",
            "status": "pending",
            "dependencies": [
              4,
              "38.4"
            ],
            "details": "<info added on 2025-08-03T18:47:09.915Z>\nThis subtask has been consolidated with Task 37.15 (Eliminate Dual V1/V2 Writes in RunManagerV2) which addressed identical scope. Both tasks target removing V1 table writes from V2 pipeline in RunManagerV2, keeping only V2 persistence for new runs. The consolidated implementation will deliver combined benefits: 20-30% performance improvement from eliminating dual writes plus complete removal of V1 vs V2 comparison metric calculations. Task 37.15 should be marked completed/consolidated to eliminate duplication. Target locations: app/lib/agents-v2/services/runManagerV2.js.\n</info added on 2025-08-03T18:47:09.915Z>",
            "testStrategy": ""
          },
          {
            "id": 11,
            "title": "Update API Routes - Remove Comparison Data Endpoints",
            "description": "Review and update all API routes that serve comparison data. Remove endpoints or response fields related to V1 vs V2 comparisons (token_savings_percentage, time_savings_percentage, efficiency_score, etc.). Update API responses to include new absolute performance fields. Focus on routes under /api/admin/ that serve dashboard and runs data.",
            "status": "pending",
            "dependencies": [
              10,
              "38.10"
            ],
            "details": "",
            "testStrategy": ""
          },
          {
            "id": 12,
            "title": "Create Performance Calculation Functions and Triggers",
            "description": "Create database functions and triggers to automatically calculate absolute performance metrics. Implement functions for: calculating opportunities_per_minute from run duration, computing tokens_per_opportunity averages, determining cost_per_opportunity_usd, calculating success_rate_percentage from processed vs failed opportunities, and computing sla_compliance_percentage against target benchmarks. Add triggers to update metrics on data changes.",
            "status": "pending",
            "dependencies": [
              4,
              "38.4"
            ],
            "details": "",
            "testStrategy": ""
          },
          {
            "id": 13,
            "title": "Build Advanced Analytics Dashboard from final_results Data",
            "description": "Create sophisticated data visualizations and monitoring capabilities from the rich analytics data stored in pipeline_runs.final_results JSONB field.",
            "details": "<info added on 2025-08-02T19:23:28.306Z>\nLooking at the user request and the context, I'll generate the new content that should be added to subtask 38.13's details.**Problem Statement:**\nWe have Netflix-level analytics stored in the final_results JSONB field of the pipeline_runs table, but we're only displaying McDonald's-level dashboards (basic summary numbers and raw JSON dumps). We're sitting on a goldmine of detailed performance data that could power sophisticated monitoring and optimization dashboards.\n\n**Rich Data Currently Unused:**\n\nStage Performance Analytics:\n- Individual stage execution times, token usage, API calls\n- Stage-by-stage success/failure rates  \n- Resource utilization per stage (memory, processing time)\n\nOpportunity Path Analytics:\n- Individual opportunity journeys through the pipeline\n- Path type distribution (NEW vs UPDATE vs SKIP)\n- Token savings per opportunity, processing time per opportunity\n\nAdvanced Duplicate Detection Analytics:\n- Detection method breakdown (ID matches vs title matches)\n- Detection accuracy scores and confidence levels\n- False positive/negative rates\n\nQuality Metrics:\n- Data quality scores per stage\n- Processing accuracy rates\n- Error breakdown by type and stage\n\n**Technical Implementation:**\n- Parse and display data from final_results.enhancedMetrics.stageMetrics\n- Visualize final_results.enhancedMetrics.opportunityPaths data\n- Create charts from final_results.enhancedMetrics.duplicateDetectionMetrics\n- Add stage performance comparison widgets\n- Build pipeline efficiency trending over time\n\n**Files to Update:**\n- /app/admin/funding-sources/runs/[id]/pageV2.jsx - Replace raw JSON with rich visualizations\n- /app/admin/funding-sources/[id]/pageV2.jsx - Add advanced performance analytics\n- /components/admin/RunsTable/RunsTableV2.jsx - Enhance with detailed metrics\n- Create new components for stage analytics, opportunity path tracking\n\n**Priority**: MEDIUM-HIGH (significant value unlock from existing data)\n\nThis subtask leverages the comprehensive final_results data documented in our PRD to create actionable insights and monitoring capabilities.\n</info added on 2025-08-02T19:23:28.306Z>",
            "status": "pending",
            "dependencies": [
              "38.4",
              "38.7",
              "38.10",
              "38.12"
            ],
            "parentTaskId": 38
          },
          {
            "id": 14,
            "title": "Implement User-Friendly Error Details Display",
            "description": "Address the poor error presentation problem by implementing comprehensive error details display in the admin UI. Transform excellent error data capture in error_details JSONB fields into meaningful user-facing error messages with actionable guidance.",
            "details": "<info added on 2025-08-02T19:37:32.330Z>\n**Problem Statement:**\nCurrently users only see generic 'failed' indicators without understanding what went wrong, why it failed, or how to fix it, despite having robust error capture in pipeline_runs.error_details and pipeline_stages.error_details JSONB fields.\n\n**Technical Implementation:**\n- Parse and display pipeline_runs.error_details.message in run detail pages\n- Show pipeline_stages.error_details for individual stage failures  \n- Create error severity classification (warning, error, critical)\n- Add 'Retry Run' functionality for transient errors\n- Implement error correlation between stage and pipeline failures\n\n**Files to Update:**\n- /app/admin/funding-sources/runs/[id]/pageV2.jsx - Add error details display\n- /app/admin/funding-sources/[id]/pageV2.jsx - Show failed run details\n- /components/admin/RunsTable/RunsTableV2.jsx - Add error message column\n- Create new error display components and error categorization logic\n\n**User Experience Goals:**\n- Replace '❌ Failed' with '❌ Anthropic API rate limit exceeded - retry in 5 minutes'\n- Show which specific stage failed and why\n- Provide actionable guidance for common error types\n- Enable troubleshooting without raw JSON inspection\n\n**Priority:** MEDIUM (improves debugging and user experience significantly)\n\n**Dependencies:** Should come after 38.2 (Enhance V2 Pipeline Error Handling) as it builds on the error handling infrastructure\n\n**Scope:**\n- Display meaningful error messages from error_details.message in admin UI\n- Add stage-specific error indicators in pipeline visualization\n- Create expandable error details modal with technical information\n- Add error categorization (user-actionable vs system issues)\n- Provide retry suggestions for transient failures\n</info added on 2025-08-02T19:37:32.330Z>",
            "status": "pending",
            "dependencies": [
              "38.2"
            ],
            "parentTaskId": 38
          },
          {
            "id": 15,
            "title": "Fix Pipeline Stages Field Usage and Performance Calculations",
            "description": "Critical fixes for pipeline_stages table: Currently input_count and output_count fields are completely unused (always 0), preventing accurate performance calculations. Execution time is duplicated between execution_time_ms field and performance_metrics.executionTime. Performance calculation functions fail because they expect real counts but receive 0s.\n\nImplementation Phases:\nPhase 1 - Update Count Tracking:\n- Update RunManagerV2 to accept and store actual opportunity counts for each stage\n- Modify ProcessCoordinatorV2 to pass real input/output counts to RunManager\n- Ensure counts are properly tracked for: storage, analysis, filtering stages\n\nPhase 2 - Eliminate Duplication:\n- Standardize on execution_time_ms field (remove performance_metrics.executionTime)\n- Update all references to use single source of truth\n- Ensure consistent time tracking across all stages\n\nPhase 3 - Add Meaningful Performance Fields:\n- Add opportunities_processed field for actual work done\n- Add processing_rate_per_second for throughput metrics\n- Update performance calculation functions to use real data\n- Add proper constraints and indexes for new fields\n\nThis directly impacts dashboard analytics accuracy as current calculations show incorrect or meaningless values (e.g., infinite processing rates due to 0 denominators).\n\nDependencies: Should be completed before or alongside 38.4 (Database Migration Phase 1) as both involve schema changes.",
            "details": "<info added on 2025-08-02T20:11:17.692Z>\nCRITICAL FINDING: The pipeline_stages table has proper structure but is NOT being populated correctly, making our performance metrics unreliable.\n\nKey Issues Identified:\n1. input_count and output_count fields are ALWAYS 0 (never populated by RunManagerV2 or ProcessCoordinatorV2)\n2. Execution time is duplicated in both execution_time_ms field AND performance_metrics.executionTime\n3. Performance calculation functions use GREATEST(input_count, output_count, 1) but always get 0s, defaulting to 1, making operations per second calculations meaningless\n4. Missing critical performance fields: opportunities_processed, processing_rate_per_second, token_efficiency\n5. Inconsistent data placement between stage_results and performance_metrics JSONB fields\n\nImplementation Steps Required:\n\n1. Fix RunManagerV2 stage tracking to properly populate input_count and output_count fields\n2. Update ProcessCoordinatorV2 to consistently track and record stage metrics\n3. Resolve execution time duplication by standardizing on single field location\n4. Add missing performance fields to pipeline_stages table schema\n5. Implement proper data flow between stage_results and performance_metrics\n6. Update performance calculation functions to handle actual count data instead of defaulting to 1\n7. Add validation to ensure metrics are being recorded correctly during pipeline execution\n8. Create migration script to backfill any missing performance data where possible\n</info added on 2025-08-02T20:11:17.692Z>",
            "status": "pending",
            "dependencies": [
              "38.16"
            ],
            "parentTaskId": 38
          },
          {
            "id": 16,
            "title": "Fix pipeline_stages input/output count tracking",
            "description": "Priority: HIGH\nProblem: input_count and output_count fields are always 0 (never populated)\nImpact: Performance calculation functions fail, dashboard metrics inaccurate\nSolution: \n- Update RunManagerV2.updateV2Stage() to accept inputCount and outputCount parameters\n- Update ProcessCoordinatorV2 to pass actual opportunity counts to each stage\n- Example: data_extraction stage should set output_count to extractionResult.opportunities.length\n- Example: early_duplicate_detector should set input_count to opportunities received, output_count to new+update opportunities",
            "details": "<info added on 2025-08-03T18:46:45.847Z>\nThis task consolidates with Task 37.14 (Fix N+1 Query Patterns in StorageAgent - Implement batch operations for opportunity storage) which addressed identical scope. Both tasks target the core issue where pipeline_stages input/output count tracking remains unused (always 0), preventing accurate performance calculations. The consolidated implementation will deliver combined benefits: 80% reduction in database round trips from batch operations plus enabled accurate performance metrics through proper stage tracking. Task 37.14 should be marked completed/consolidated to eliminate duplication.\n</info added on 2025-08-03T18:46:45.847Z>",
            "status": "done",
            "dependencies": [],
            "parentTaskId": 38
          },
          {
            "id": 17,
            "title": "Eliminate execution time duplication in pipeline_stages",
            "description": "Priority: MEDIUM\nProblem: Execution time stored in both execution_time_ms field AND performance_metrics.executionTime\nImpact: Data inconsistency, confusion in reporting\nSolution:\n- Standardize on execution_time_ms field only\n- Remove executionTime from performance_metrics JSONB\n- Update all agents to stop adding executionTime to performance_metrics\n- Create migration to clean existing data",
            "details": "",
            "status": "pending",
            "dependencies": [
              "38.16"
            ],
            "parentTaskId": 38
          },
          {
            "id": 18,
            "title": "Fix performance calculation functions for pipeline_stages",
            "description": "Priority: HIGH\nProblem: calculate_pipeline_stage_metrics function uses GREATEST(input_count, output_count, 1) but always gets 0s, defaulting to 1\nImpact: Operations per second calculations are meaningless\nSolution:\n- First fix subtask 38.15.16 to populate real counts\n- Update performance calculation functions to handle actual opportunity counts\n- Add proper fallback logic for edge cases\n- Test with real processing data",
            "details": "",
            "status": "pending",
            "dependencies": [
              "38.16"
            ],
            "parentTaskId": 38
          },
          {
            "id": 19,
            "title": "Add missing performance tracking fields to pipeline_stages",
            "description": "Priority: MEDIUM\nProblem: Missing key performance fields for meaningful analytics\nSolution:\n- Add opportunities_processed INTEGER field\n- Add processing_rate_per_second DECIMAL field\n- Add token_efficiency DECIMAL field (tokens per opportunity)\n- Create migration to add these fields\n- Update RunManagerV2 to calculate and populate these fields",
            "details": "",
            "status": "pending",
            "dependencies": [
              "38.16"
            ],
            "parentTaskId": 38
          },
          {
            "id": 20,
            "title": "Clarify stage_results vs performance_metrics separation",
            "description": "Priority: LOW\nProblem: Inconsistent data placement between stage_results and performance_metrics\nSolution:\n- Document clear guidelines: stage_results for business outcomes, performance_metrics for resource usage\n- Audit all agents to ensure consistent data placement\n- Move misplaced data to correct JSONB field\n- Update agent templates with correct patterns",
            "details": "",
            "status": "in-progress",
            "dependencies": [],
            "parentTaskId": 38
          },
          {
            "id": 21,
            "title": "Fix api_raw_responses linkage to opportunities",
            "description": "Problem: 24 orphaned raw responses not linked to opportunities (92.59% opportunities have raw_response_id but only 7/31 responses are linked). Solution: Run cleanup query to link orphaned opportunities to their raw responses, fix dataExtractionAgent to ensure raw_response_id is always populated, add foreign key constraint to ensure referential integrity, add monitoring to track linkage success rate. Priority: HIGH",
            "details": "",
            "status": "pending",
            "dependencies": [],
            "parentTaskId": 38
          },
          {
            "id": 22,
            "title": "Fix two-step API workflow response aggregation",
            "description": "Problem: Detail API calls create individual raw responses that aren't aggregated properly. Impact: Storage inefficiency, broken traceability. Solution: Modify dataExtractionAgent to aggregate detail responses under parent list response, add parent_response_id field for detail responses, update storage logic to handle two-step workflows correctly, clean up existing orphaned detail responses. Priority: MEDIUM",
            "details": "",
            "status": "pending",
            "dependencies": [
              "38.21"
            ],
            "parentTaskId": 38
          },
          {
            "id": 23,
            "title": "Add missing indexes and constraints to api_raw_responses",
            "description": "Problem: Missing indexes for content_hash, call_type, created_at affecting query performance. Solution: Add index on content_hash for deduplication queries, add index on call_type for filtering, add index on created_at for time-based queries, add foreign key constraint from funding_opportunities.raw_response_id to api_raw_responses.id. Priority: LOW",
            "details": "<info added on 2025-08-03T18:47:34.108Z>\nThis subtask has been consolidated with Task 37.13 (Add Missing Composite Indexes for V2 Pipeline) due to identical scope. Both tasks target creating critical composite indexes for V2 pipeline performance including idx_pipeline_stages_run_order_status, idx_opportunity_paths_duplicate_analysis, idx_pipeline_runs_analytics, plus additional indexes for content_hash, call_type, and created_at on api_raw_responses. The consolidated implementation delivers combined benefits of 3-5x query speedup for pipeline operations and improved query performance for content_hash deduplication queries, call_type filtering, and time-based queries. Task 37.13 should be marked completed/consolidated to eliminate duplication.\n</info added on 2025-08-03T18:47:34.108Z>",
            "status": "pending",
            "dependencies": [
              "38.21"
            ],
            "parentTaskId": 38
          },
          {
            "id": 24,
            "title": "Fix Early Duplicate Detection metrics data flow",
            "description": "CRITICAL PRIORITY: Enhanced metrics from earlyDuplicateDetector not reaching database, making performance claims unsubstantiated. 60-80% efficiency claims are based on hardcoded estimates, not real measurements. Fix processCoordinatorV2.js line 201 to pass duplicateDetection.enhancedMetrics, update runManagerV2.recordDuplicateDetectionSession() to accept enhanced metrics, ensure duplicate_detection_method field gets populated (currently always null), and fix token tracking to show real values instead of 0.",
            "details": "",
            "status": "pending",
            "dependencies": [
              "38.27"
            ],
            "parentTaskId": 38
          },
          {
            "id": 25,
            "title": "Implement real token measurement for V2 optimization validation",
            "description": "HIGH PRIORITY: Token savings calculations use hardcoded 1500 tokens/opportunity estimate instead of real measurements. Cannot validate actual V2 efficiency gains, performance claims may be overstated. Remove hardcoded token estimates from earlyDuplicateDetector.js, implement baseline measurement by processing sample sets through full pipeline, track actual tokens used in analysis/filter stages for comparison, create A/B testing framework to compare V1 vs V2 processing, and calculate real savings by measuring bypassed processing costs.",
            "details": "",
            "status": "pending",
            "dependencies": [
              "38.27"
            ],
            "parentTaskId": 38
          },
          {
            "id": 26,
            "title": "Complete duplicate_detection_sessions schema migration",
            "description": "MEDIUM PRIORITY: Table still contains comparison metric fields that should have been removed in migration Phase 2. Data inconsistency, potential corruption in reporting. Complete migration to remove estimated_tokens_saved, estimated_cost_saved_usd, efficiency_improvement_percentage. Ensure data consistency between duplicate_detection_sessions and pipeline_runs. Update any dependent queries or reports.",
            "details": "",
            "status": "pending",
            "dependencies": [
              "38.5"
            ],
            "parentTaskId": 38
          },
          {
            "id": 27,
            "title": "Implement missing token usage tracking in Analysis Agent",
            "description": "CRITICAL PRIORITY - Fix broken token usage tracking in Analysis Agent that invalidates all V2 efficiency claims.\n\n**Problem**: Analysis Agent completely ignores token usage data from Anthropic API, making all V2 efficiency claims invalid\n**Evidence**: Line 153 analysisAgent/index.js has TODO comment for token tracking, all database records show 0 tokens despite real API calls\n**Impact**: Cannot validate 60-80% efficiency claims, cost calculations are fictional, performance metrics unreliable\n\n**Required Implementation**:\n- Extract token usage from anthropicClient responses in analysisAgent/index.js\n- Fix line 153 to capture and pass real token data to logAgentExecution\n- Update runManagerV2.updateV2Analysis() to receive and store actual token counts\n- Modify processCoordinatorV2.js lines 263-276 to pass real token data instead of hardcoded estimates\n- Replace all hardcoded token estimates with actual measurements\n- Validate token savings calculations with real data\n\n**Validation**: Verify real token usage is recorded in database for analysis operations and efficiency calculations are based on actual measurements rather than estimates.\n\nThis is the most critical finding - the entire V2 efficiency validation system is broken because token usage is never actually measured or stored, despite the infrastructure existing to do so.",
            "details": "",
            "status": "pending",
            "dependencies": [],
            "parentTaskId": 38
          },
          {
            "id": 28,
            "title": "Fix non-functional Filter Function stage",
            "description": "Filter stage has 100% pass rate and provides zero quality improvement",
            "details": "",
            "status": "pending",
            "dependencies": [],
            "parentTaskId": 38
          },
          {
            "id": 29,
            "title": "Add Storage Agent conflict resolution and transaction safety",
            "description": "Storage Agent performs direct INSERTs without conflict resolution, relying entirely on upstream duplicate detection. Missing ON CONFLICT handling, atomic transactions, and metrics accuracy validation. Add conflict resolution, transaction safety, and cross-validation with database state.",
            "details": "",
            "status": "pending",
            "dependencies": [
              "38.32"
            ],
            "parentTaskId": 38
          },
          {
            "id": 31,
            "title": "Fix false parallel processing documentation claims",
            "description": "Documentation claims Direct Update Handler runs in parallel with Storage Agent, but code shows sequential execution. Update architecture documentation to accurately describe intelligent branching vs false parallelism claims.",
            "details": "",
            "status": "pending",
            "dependencies": [],
            "parentTaskId": 38
          },
          {
            "id": 32,
            "title": "Fix changeDetector field name mismatch with directUpdateHandler",
            "description": "Critical field naming inconsistency: changeDetector.js uses camelCase (openDate, closeDate) while directUpdateHandler.js expects snake_case (open_date, close_date). This causes direct updates to fail silently. Solution: Standardize field names across both components, update field mapping in directUpdateHandler.js lines that reference these fields, ensure consistent naming convention throughout the pipeline, add tests to catch field name mismatches.",
            "details": "",
            "status": "pending",
            "dependencies": [],
            "parentTaskId": 38
          },
          {
            "id": 33,
            "title": "Fix processCoordinatorV2 configuration flag integration",
            "description": "High Priority: Configuration flags (optimization_enabled, early_duplicate_detection, metrics_collection) are hardcoded as true in processCoordinatorV2.js lines 112-114. Never reads from database configuration. Solution: Import configurationReader service, read flags from run_configuration table, apply flags throughout pipeline execution, ensure stages respect configuration settings, add configuration validation and error handling.",
            "details": "",
            "status": "pending",
            "dependencies": [
              "38.1"
            ],
            "parentTaskId": 38
          },
          {
            "id": 34,
            "title": "Fix V2 efficiency claims validation system",
            "description": "CRITICAL: Entire V2 efficiency validation system is fundamentally broken. All 60-80% efficiency claims are unsubstantiated due to three critical issues: 1) Token tracking completely unimplemented (analysisAgent/index.js line 153 TODO), 2) Hardcoded estimates instead of real measurements (1500 tokens/opportunity estimate), 3) Performance metrics based on fictional data. Solution: Implement real token measurement from Anthropic API, replace all hardcoded estimates with actual measurements, create A/B testing framework to validate claims, establish baseline measurements for comparison.",
            "details": "",
            "status": "pending",
            "dependencies": [
              "38.27"
            ],
            "parentTaskId": 38
          }
        ]
      },
      {
        "id": 39,
        "title": "Implement V2 Pipeline Configuration Flag Logic",
        "description": "Make the existing configuration flags (optimization_enabled, early_duplicate_detection, metrics_collection) actually functional in the V2 pipeline system by implementing the logic to read and apply these flags during pipeline execution. Additionally, create a comprehensive configuration management system with admin UI, API endpoints, database storage, validation, and audit capabilities.",
        "status": "pending",
        "dependencies": [
          1,
          36
        ],
        "priority": "medium",
        "details": "CORE CONFIGURATION LOGIC:\n1. Analyze current run_configuration table structure and existing flag definitions\n2. Implement configuration reader service in app/lib/services/ to fetch flags from run_configuration\n3. Modify ProcessCoordinatorV2 to read and apply optimization_enabled flag\n4. Update EarlyDuplicateDetector to respect early_duplicate_detection flag\n5. Integrate metrics_collection flag into RunManagerV2\n6. Update agent initialization in app/lib/agents-v2/ to pass configuration flags\n7. Modify existing V2 pipeline stages to check and apply relevant configuration flags\n\nCOMPREHENSIVE CONFIGURATION SYSTEM:\n8. Create pipeline_configurations database table with versioning and user/organization scoping\n9. Build admin UI for real-time configuration management with preview capabilities\n10. Implement API endpoints for configuration CRUD operations\n11. Add comprehensive validation and error handling with rollback capability\n12. Implement configuration audit logging for compliance and debugging\n13. Add real-time configuration validation during pipeline execution\n14. Create configuration export/import functionality for backup and migration\n15. Add configuration change notification system\n16. Update run creation API to use new configuration system\n17. Create comprehensive documentation for all configuration options",
        "testStrategy": "CORE FUNCTIONALITY TESTS:\n1. Test configuration reader service with various flag combinations\n2. Validate optimization_enabled flag behavior by comparing performance metrics\n3. Test early_duplicate_detection flag by running with known duplicate datasets\n4. Verify metrics_collection flag by checking database metrics table population\n5. Test flag validation logic with invalid configuration combinations\n6. Run end-to-end pipeline tests with different flag configurations\n\nCOMPREHENSIVE SYSTEM TESTS:\n7. Test admin UI configuration changes with real-time preview validation\n8. Validate API endpoints for configuration management (CRUD operations)\n9. Test database versioning and rollback functionality\n10. Verify user/organization scoping of configuration settings\n11. Test configuration audit logging and compliance tracking\n12. Validate export/import functionality with various configuration sets\n13. Test real-time configuration validation during active pipeline runs\n14. Verify configuration change notifications and alerting\n15. Test error handling and rollback scenarios for invalid configurations\n16. Performance test impact of different configuration combinations\n17. Integration test full configuration lifecycle from UI to pipeline execution",
        "subtasks": [
          {
            "id": 1,
            "title": "Create pipeline_configurations database table",
            "description": "Design and implement database table for storing user-configurable pipeline settings with versioning, user/organization scoping, and audit trail capabilities",
            "status": "pending",
            "dependencies": [],
            "details": "",
            "testStrategy": ""
          },
          {
            "id": 2,
            "title": "Implement configuration reader service",
            "description": "Create service in app/lib/services/ to fetch and validate configuration flags from pipeline_configurations table with caching and real-time updates",
            "status": "pending",
            "dependencies": [],
            "details": "",
            "testStrategy": ""
          },
          {
            "id": 3,
            "title": "Build admin UI for configuration management",
            "description": "Create comprehensive admin interface for toggling configuration options with real-time preview, validation feedback, and change confirmation dialogs",
            "status": "pending",
            "dependencies": [],
            "details": "",
            "testStrategy": ""
          },
          {
            "id": 4,
            "title": "Implement configuration management API endpoints",
            "description": "Create REST API endpoints for configuration CRUD operations with proper authentication, validation, and error handling",
            "status": "pending",
            "dependencies": [],
            "details": "",
            "testStrategy": ""
          },
          {
            "id": 5,
            "title": "Integrate configuration flags into V2 pipeline components",
            "description": "Update ProcessCoordinatorV2, EarlyDuplicateDetector, and RunManagerV2 to read and apply configuration flags during execution",
            "status": "pending",
            "dependencies": [],
            "details": "",
            "testStrategy": ""
          },
          {
            "id": 6,
            "title": "Add configuration validation and rollback capability",
            "description": "Implement comprehensive validation logic and rollback functionality for configuration changes with error recovery mechanisms",
            "status": "pending",
            "dependencies": [],
            "details": "",
            "testStrategy": ""
          },
          {
            "id": 7,
            "title": "Implement configuration audit logging",
            "description": "Create audit trail system for tracking all configuration changes with user attribution, timestamps, and change details for compliance purposes",
            "status": "pending",
            "dependencies": [],
            "details": "",
            "testStrategy": ""
          },
          {
            "id": 8,
            "title": "Add configuration export/import functionality",
            "description": "Create system for exporting and importing configuration sets for backup, migration, and template sharing capabilities",
            "status": "pending",
            "dependencies": [],
            "details": "",
            "testStrategy": ""
          },
          {
            "id": 9,
            "title": "Create configuration documentation",
            "description": "Write comprehensive documentation explaining all configuration options, their impacts, best practices, and troubleshooting guides",
            "status": "pending",
            "dependencies": [],
            "details": "",
            "testStrategy": ""
          },
          {
            "id": 10,
            "title": "Implement comprehensive test suite",
            "description": "Create unit tests, integration tests, and end-to-end tests covering all configuration functionality including UI, API, validation, and pipeline integration",
            "status": "pending",
            "dependencies": [],
            "details": "",
            "testStrategy": ""
          }
        ]
      }
    ],
    "metadata": {
      "created": "2025-06-17T23:54:58.582Z",
      "updated": "2025-08-04T02:18:44.070Z",
      "description": "Tasks for master context"
    }
  }
}